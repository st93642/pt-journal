What is AI security?|Artificial intelligence|Protecting AI/ML systems from attacks and ensuring safe, reliable operation|AI software|Intelligent security|1|AI security involves protecting artificial intelligence and machine learning systems from adversarial attacks, data poisoning, model theft, and ensuring models operate safely, fairly, and reliably without unintended harmful behaviors.|AI/ML Security|Fundamentals
What is machine learning?|Learning machines|Computer systems that learn from data to make predictions or decisions without explicit programming|Machine education|Learning algorithms|1|Machine learning is a subset of AI where systems learn patterns from data to make predictions or decisions. Types include supervised learning (labeled data), unsupervised learning (patterns in unlabeled data), and reinforcement learning (reward-based).|AI/ML Security|Fundamentals
What is an adversarial attack on AI?|Advanced attack|Crafting malicious inputs designed to cause ML models to make incorrect predictions|Adversary attack|AI virus|1|Adversarial attacks craft inputs with small, often imperceptible perturbations designed to fool ML models. Examples include adding noise to images to misclassify objects or manipulating text to evade spam filters.|AI/ML Security|Adversarial Attacks
What is an adversarial example?|Example attack|Input intentionally designed to cause machine learning model misclassification|Adversary example|Bad example|1|Adversarial examples are inputs crafted with carefully calculated perturbations that cause ML models to make mistakes. A classic example is adding imperceptible noise to an image of a panda to make it classify as a gibbon.|AI/ML Security|Adversarial Attacks
What is the Fast Gradient Sign Method (FGSM)?|Fast gradient|Simple one-step method for generating adversarial examples using gradient of loss function|Gradient method|Fast sign|1|FGSM is a white-box attack that generates adversarial examples by computing the gradient of the loss function and adding a small perturbation in the direction that maximizes loss, causing misclassification.|AI/ML Security|Attack Techniques
What is data poisoning in ML?|Data corruption|Injecting malicious data into training datasets to corrupt model behavior|Data pollution|Poison data|1|Data poisoning attacks inject malicious samples into training data to manipulate model behavior. Attackers can cause targeted misclassification, insert backdoors, or degrade overall model performance during training.|AI/ML Security|Data Attacks
What is a backdoor attack on ML models?|Back door|Embedding hidden triggers in models that cause specific misclassification when triggered|Model backdoor|Door attack|1|Backdoor attacks embed hidden behaviors in ML models during training. The model performs normally except when presented with a specific trigger pattern, causing targeted misclassification useful to the attacker.|AI/ML Security|Data Attacks
What is model inversion?|Model reversal|Reconstructing training data by exploiting model outputs and internal representations|Inversion attack|Reverse model|1|Model inversion attacks exploit ML model predictions to reconstruct sensitive training data. For example, reconstructing faces from facial recognition model outputs or extracting private information from language models.|AI/ML Security|Privacy Attacks
What is membership inference attack?|Member attack|Determining if specific data point was used in model training|Inference attack|Membership test|1|Membership inference determines whether specific data was in the training set by analyzing model behavior. This violates privacy by revealing if someone's data (medical records, personal info) was used for training.|AI/ML Security|Privacy Attacks
What is model theft (model extraction)?|Model stealing|Replicating ML model functionality through query access without direct access|Model copy|Theft attack|1|Model extraction attacks replicate proprietary ML models by querying them repeatedly and training a substitute model that mimics behavior. This steals intellectual property and enables further attacks.|AI/ML Security|Model Attacks
What is transfer learning attack?|Transfer attack|Exploiting vulnerabilities in pre-trained models used as starting points|Learning transfer|Transfer vulnerability|1|Transfer learning attacks exploit pre-trained models (BERT, GPT, ResNet) used as foundations. Hidden backdoors or vulnerabilities in base models transfer to all models fine-tuned from them.|AI/ML Security|Model Attacks
What is the OWASP Machine Learning Security Top 10?|OWASP list|Prioritized list of top 10 security risks specific to machine learning systems|OWASP ML|Top security|1|OWASP ML Security Top 10 identifies critical risks: Input Manipulation, Data Poisoning, Model Inversion, Membership Inference, Model Theft, AI Supply Chain, Transfer Learning, Model Skewing, Output Integrity, and Model Poisoning.|AI/ML Security|Frameworks
What is MITRE ATLAS?|MITRE framework|Adversarial Threat Landscape for AI Systems - knowledge base of adversary tactics against AI/ML|ATLAS protocol|AI threat list|1|MITRE ATLAS is a knowledge base documenting adversary tactics and techniques against AI/ML systems. It extends MITRE ATT&CK framework with AI-specific threats, providing 15 tactics and 130+ techniques.|AI/ML Security|Frameworks
What is evasion attack in ML?|Evasion technique|Modifying malicious input to avoid detection by ML-based security systems|Evade attack|Evasion method|1|Evasion attacks modify malicious inputs to bypass ML-based detection systems like spam filters, malware detectors, or intrusion detection. Attackers craft inputs that appear benign to the model but retain malicious functionality.|AI/ML Security|Adversarial Attacks
What is a white-box adversarial attack?|White box|Attack with full knowledge of model architecture, parameters, and training data|White attack|Box attack|1|White-box attacks assume complete knowledge of the target model including architecture, weights, and training procedure. This enables precise gradient-based attacks like FGSM, C&W, and PGD that are highly effective.|AI/ML Security|Attack Types
What is a black-box adversarial attack?|Black box|Attack with only query access to model, no internal knowledge|Black attack|Box query|1|Black-box attacks have only input-output access to models without internal knowledge. Attackers query the model repeatedly to infer behavior and craft adversarial examples or extract the model.|AI/ML Security|Attack Types
What is Foolbox?|Fool tool|Python library for creating adversarial examples to test ML model robustness|Fooling box|Box tool|1|Foolbox is a Python library providing implementations of various adversarial attack algorithms. It supports multiple frameworks (PyTorch, TensorFlow, JAX) and enables testing ML model robustness against adversarial examples.|AI/ML Security|Tools
What is CleverHans?|Clever tool|TensorFlow library for generating adversarial examples and testing model robustness|Clever hands|Hans tool|1|CleverHans is a Python library built on TensorFlow for benchmarking ML models against adversarial attacks. It implements various attack algorithms and defenses for adversarial training.|AI/ML Security|Tools
What is the Adversarial Robustness Toolbox (ART)?|Art tool|Library for defending and evaluating ML models against adversarial threats|Robustness art|Art attack|1|ART is a Python library providing tools for defending ML models against adversarial attacks. It supports evasion, poisoning, extraction, and inference attacks across TensorFlow, PyTorch, scikit-learn, and more.|AI/ML Security|Tools
What is adversarial training?|Training defense|Training models on adversarial examples to improve robustness|Adversary training|Advanced training|1|Adversarial training augments training data with adversarial examples, teaching models to correctly classify both normal and adversarial inputs. This is the most effective defense but computationally expensive.|AI/ML Security|Defenses
What is input sanitization for ML?|Input cleaning|Preprocessing inputs to remove adversarial perturbations before model inference|Input validation|Sanitize input|1|Input sanitization applies preprocessing (compression, denoising, quantization) to remove adversarial perturbations while preserving legitimate features. Methods include JPEG compression, bit-depth reduction, or spatial smoothing.|AI/ML Security|Defenses
What is model ensemble defense?|Ensemble model|Using multiple diverse models and aggregating predictions to improve robustness|Model group|Multiple models|1|Ensemble defenses use multiple models with different architectures or training data, making it harder for adversarial examples to fool all models simultaneously. Prediction aggregation (voting, averaging) improves robustness.|AI/ML Security|Defenses
What is gradient masking?|Gradient hiding|Obscuring gradient information to make gradient-based adversarial attacks less effective|Gradient mask|Masking defense|1|Gradient masking makes gradients non-informative or obfuscated to hinder gradient-based attacks. However, it provides false sense of security as attackers can use gradient-free attacks or approximate gradients.|AI/ML Security|Defenses
What is certified defense in ML?|Certification|Providing mathematical guarantees about model robustness within specified perturbation bounds|Certified model|Defense certificate|1|Certified defenses provide provable guarantees that models will maintain correct predictions within specified perturbation bounds. Techniques include randomized smoothing and interval bound propagation.|AI/ML Security|Defenses
What is prompt injection in LLMs?|Prompt attack|Crafting inputs to large language models that override instructions or extract sensitive information|Injection attack|Prompt hack|1|Prompt injection manipulates large language model instructions through user input, causing the model to ignore original prompts, reveal system prompts, or perform unintended actions like generating harmful content.|AI/ML Security|LLM Attacks
What is jailbreaking an LLM?|LLM breaking|Bypassing safety guardrails in language models to generate prohibited content|Jail attack|Breaking model|1|Jailbreaking circumvents large language model safety measures through creative prompting, role-playing scenarios, or encoded requests. Examples include DAN (Do Anything Now) prompts that override content policies.|AI/ML Security|LLM Attacks
What is data extraction from LLMs?|Data stealing|Recovering training data from language models through targeted prompts|Extraction attack|Data recovery|1|Data extraction attacks recover verbatim training data from LLMs by carefully crafted prompts. Language models can memorize and regurgitate sensitive training data including personal information, code, or copyrighted text.|AI/ML Security|LLM Attacks
What is model collapse in AI?|Model failure|Degradation of model quality when trained on AI-generated synthetic data over generations|Collapse attack|Model break|1|Model collapse occurs when models are recursively trained on AI-generated data, leading to quality degradation, reduced diversity, and amplified biases. Each generation produces progressively worse outputs.|AI/ML Security|AI Risks
What is AI bias amplification?|Bias increase|ML models learning and amplifying societal biases present in training data|Amplified bias|Bias growth|1|AI systems can amplify existing biases in training data, leading to discriminatory outcomes in hiring, lending, criminal justice, or facial recognition. Bias can enter through data collection, labeling, or model design.|AI/ML Security|AI Ethics
What is fairness in machine learning?|Fair AI|Ensuring ML models make equitable decisions across different demographic groups|ML fairness|Fairness measure|1|ML fairness aims to prevent discriminatory outcomes based on protected attributes (race, gender, age). Fairness metrics include demographic parity, equalized odds, and individual fairness. Trade-offs exist between fairness and accuracy.|AI/ML Security|AI Ethics
What is explainable AI (XAI)?|Explanation AI|Making AI decision-making processes interpretable and understandable to humans|AI explanation|Explainable model|1|Explainable AI techniques make black-box models interpretable. Methods include LIME (Local Interpretable Model-agnostic Explanations), SHAP (SHapley Additive exPlanations), attention mechanisms, and saliency maps.|AI/ML Security|Interpretability
What is model interpretability?|Model understanding|Degree to which humans can understand reasons behind model predictions|Interpretation|Understanding model|1|Model interpretability is the extent to which humans can understand and explain model behavior. Inherently interpretable models (decision trees, linear regression) are transparent, while deep learning requires post-hoc explanation techniques.|AI/ML Security|Interpretability
What is the AI supply chain attack surface?|AI supply|Vulnerabilities in ML pipelines, dependencies, pre-trained models, and datasets|Supply attack|AI chain|1|AI supply chain includes risks from compromised datasets (poisoned data), malicious pre-trained models (backdoored weights), vulnerable dependencies (TensorFlow packages), and cloud ML services. Each stage introduces attack vectors.|AI/ML Security|Supply Chain
What is MLOps security?|ML operations|Securing machine learning operations including model development, deployment, and monitoring|ML ops|Operations security|1|MLOps security protects ML pipelines from development to deployment: securing training data, model repositories, API endpoints, monitoring for drift and attacks, access control, and audit logging.|AI/ML Security|Operations
What is model drift?|Model change|Degradation of model performance over time as data distribution changes|Drift attack|Model shift|1|Model drift occurs when statistical properties of production data differ from training data, causing performance degradation. Types include concept drift (relationship changes) and data drift (distribution changes).|AI/ML Security|Monitoring
What is adversarial patch attack?|Patch attack|Physical stickers or patterns that cause misclassification when placed in real-world scenes|Adversarial sticker|Patch adversary|1|Adversarial patches are physical objects (stickers, patterns) that cause misclassification when placed in camera view. Examples include patches that make stop signs invisible to autonomous vehicles or fool facial recognition.|AI/ML Security|Physical Attacks
What is deepfake technology?|Deep fake|AI-generated synthetic media (video, audio, images) that realistically impersonate people|Fake technology|Deep video|1|Deepfakes use deep learning (GANs, autoencoders) to create fake but realistic images, videos, or audio. Applications include face swapping, voice cloning, and lip-syncing for impersonation, misinformation, or fraud.|AI/ML Security|Deepfakes
What is a Generative Adversarial Network (GAN)?|GAN network|Two neural networks (generator and discriminator) competing to create realistic synthetic data|Adversarial network|Generation network|1|GANs consist of a generator creating synthetic data and a discriminator distinguishing real from fake. Through adversarial training, generators learn to create increasingly realistic outputs used in deepfakes and art generation.|AI/ML Security|AI Models
What is deepfake detection?|Fake detection|Identifying AI-generated synthetic media through artifacts, inconsistencies, or forensic analysis|Detection method|Deepfake finder|1|Deepfake detection uses techniques like analyzing facial movements, eye blinking patterns, lighting inconsistencies, compression artifacts, and neural network-based detectors. However, detection is an arms race with improving generation.|AI/ML Security|Deepfakes
What is federated learning?|Distributed learning|Training ML models across decentralized devices without centralizing data|Federal learning|Federation training|1|Federated learning trains models on distributed devices (phones, hospitals) keeping data local. Central server aggregates model updates without accessing raw data, preserving privacy but vulnerable to poisoning attacks.|AI/ML Security|Privacy Techniques
What is federated learning poisoning?|Learning poison|Injecting malicious updates in federated learning to corrupt the global model|Federation attack|Poison learning|1|Federated learning poisoning involves compromised participants sending malicious model updates to corrupt the global model. Byzantine attacks, backdoor insertion, and model replacement can degrade or control model behavior.|AI/ML Security|Attacks
What is differential privacy in ML?|Private ML|Adding calibrated noise to data or queries to protect individual privacy while enabling analysis|Differential method|Privacy difference|1|Differential privacy provides mathematical privacy guarantees by adding controlled noise to query results or during training. It ensures individual data points cannot be identified while maintaining statistical utility.|AI/ML Security|Privacy Techniques
What is homomorphic encryption for ML?|Encrypted ML|Performing computations on encrypted data without decrypting it|Homomorphic security|Encrypted computation|1|Homomorphic encryption enables computations on encrypted data, producing encrypted results that decrypt to the same answer as operations on plaintext. This allows privacy-preserving ML inference on sensitive data.|AI/ML Security|Privacy Techniques
What is secure multi-party computation (SMPC)?|Secure computation|Multiple parties jointly computing function without revealing individual inputs|Multi-party security|Secure protocol|1|SMPC enables multiple parties to jointly compute functions on private inputs without revealing those inputs to each other. Applications include privacy-preserving ML training and collaborative analytics.|AI/ML Security|Privacy Techniques
What is model watermarking?|Model marking|Embedding identifiable signatures in ML models to prove ownership or detect theft|Watermark model|Model signature|1|Model watermarking embeds identifiable patterns in model behavior without affecting performance. This proves ownership, detects unauthorized copies, or traces leaked models back to sources.|AI/ML Security|Protection
What is neural network backdoor?|Network backdoor|Hidden behavior in neural networks triggered by specific inputs|Backdoor network|Network trigger|1|Neural network backdoors are hidden malicious behaviors triggered by specific input patterns. Models perform normally except when presented with attacker-defined triggers, causing targeted misclassification.|AI/ML Security|Attacks
What is AI red teaming?|AI testing|Adversarial testing of AI systems to identify vulnerabilities and failure modes|Red team AI|AI adversary|1|AI red teaming involves adversarial testing where security teams attempt to break, manipulate, or misuse AI systems. This identifies vulnerabilities, biases, safety issues, and harmful capabilities before deployment.|AI/ML Security|Testing
What is responsible AI?|Responsible use|Developing and deploying AI systems ethically with considerations for fairness, safety, and accountability|AI responsibility|Responsible development|1|Responsible AI encompasses principles including fairness, accountability, transparency, privacy, safety, and beneficial use. It requires considering societal impact, preventing bias, ensuring explainability, and human oversight.|AI/ML Security|Ethics
What is the AI alignment problem?|AI alignment|Ensuring AI systems pursue goals aligned with human values and intentions|Alignment issue|AI goals|1|AI alignment ensures advanced AI systems' goals and behaviors align with human values and intentions. Misaligned AI could pursue objectives harmful to humans, making alignment crucial for safe AI development.|AI/ML Security|Safety
What is model poisoning vs data poisoning?|Same attack|Model poisoning corrupts deployed models through updates; data poisoning corrupts training data|Different names|Model attack only|1|Data poisoning injects malicious samples during training to corrupt the model from the start. Model poisoning corrupts already-deployed models through malicious updates in online learning or federated learning scenarios.|AI/ML Security|Attacks
