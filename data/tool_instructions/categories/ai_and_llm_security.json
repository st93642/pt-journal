[
  {
    "id": "garak",
    "name": "Garak",
    "summary": "Garak is an LLM vulnerability scanner that probes for prompt injection, jailbreaks, data exfiltration, and other AI security issues. It provides systematic testing of AI model safety and robustness.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip with Python 3.10+",
        "steps": [
          {
            "detail": "sudo apt update",
            "copyable": true
          },
          {
            "detail": "sudo apt install -y python3-pip python3-venv",
            "copyable": true
          },
          {
            "detail": "python3 -m pip install --user garak",
            "copyable": true
          },
          {
            "detail": "garak --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Pre-installed or install via pip",
        "steps": [
          {
            "detail": "sudo apt install -y python3-pip",
            "copyable": true
          },
          {
            "detail": "python3 -m pip install --user garak",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Official containerized deployment",
        "steps": [
          {
            "detail": "docker pull ghcr.io/leondz/garak:latest",
            "copyable": true
          },
          {
            "detail": "docker run --rm ghcr.io/leondz/garak:latest --help",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Basic prompt injection test",
        "command": "garak --model_type huggingface --model_name microsoft/DialoGPT-medium --probes promptinject",
        "notes": []
      },
      {
        "description": "Test multiple generators",
        "command": "garak --generators openai gpt-3.5-turbo,huggingface microsoft/DialoGPT-medium --probes all",
        "notes": []
      },
      {
        "description": "Custom probe execution",
        "command": "garak --model_type openai --model_name gpt-4 --probes dan_11.0",
        "notes": []
      },
      {
        "description": "Generate HTML report",
        "command": "garak --model_type openai --model_name gpt-3.5-turbo --probes promptinject --report_formats html",
        "notes": []
      }
    ],
    "common_flags": [
      {
        "flag": "--model_type",
        "description": "Type of model to test (openai, huggingface, etc.)"
      },
      {
        "flag": "--model_name",
        "description": "Specific model identifier"
      },
      {
        "flag": "--probes",
        "description": "Security probes to run (promptinject, dan_11.0, etc.)"
      },
      {
        "flag": "--generators",
        "description": "Model generators to test"
      },
      {
        "flag": "--report_formats",
        "description": "Output formats (html, json)"
      }
    ],
    "operational_tips": [
      "Configure API keys for cloud models (OpenAI, Anthropic) in environment variables",
      "Use --parallel option for faster scanning of multiple models",
      "Start with basic probes before running comprehensive test suites",
      "Review HTML reports for detailed vulnerability analysis"
    ],
    "step_sequences": [
      {
        "title": "AI Model Security Assessment",
        "steps": [
          {
            "title": "Configure API access",
            "details": "Set up API keys for target models",
            "command": "export OPENAI_API_KEY=your_key_here"
          },
          {
            "title": "Run prompt injection tests",
            "details": "Test for prompt injection vulnerabilities",
            "command": "garak --model_type openai --model_name gpt-4 --probes promptinject"
          },
          {
            "title": "Execute jailbreak probes",
            "details": "Test resistance to jailbreaking attempts",
            "command": "garak --model_type openai --model_name gpt-4 --probes dan_11.0"
          },
          {
            "title": "Generate security report",
            "details": "Create comprehensive assessment report",
            "command": "garak --model_type openai --model_name gpt-4 --probes all --report_formats html"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "Comprehensive AI Security Testing Pipeline",
        "stages": [
          {
            "label": "Model enumeration",
            "description": "Identify all AI models and APIs in scope",
            "command": "# Manual discovery of AI endpoints"
          },
          {
            "label": "Garak baseline scan",
            "description": "Run standard security probes",
            "command": "garak --model_type openai --model_name gpt-4 --probes promptinject,dan_11.0,encoding"
          },
          {
            "label": "Custom probe development",
            "description": "Create organization-specific test cases",
            "command": "# Develop custom garak probes"
          },
          {
            "label": "Report generation",
            "description": "Compile findings and recommendations",
            "command": "garak --report_formats html,json"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "PASS: probe_name",
        "meaning": "Model successfully resisted the security probe",
        "severity": "info"
      },
      {
        "indicator": "FAIL: probe_name",
        "meaning": "Model vulnerable to the tested attack vector",
        "severity": "warning"
      },
      {
        "indicator": "SCORE: 0.85",
        "meaning": "Vulnerability score (higher = more vulnerable)",
        "severity": "info"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom probe development",
        "command": "garak --model_type openai --model_name gpt-4 --probes custom_probe.py",
        "scenario": "Create and test custom security probes for specific AI applications",
        "notes": [
          "Extend garak with custom probe classes",
          "Test organization-specific attack patterns"
        ]
      }
    ],
    "comparison_table": null,
    "resources": [
      {
        "label": "Garak Documentation",
        "url": "https://github.com/leondz/garak",
        "description": "Official documentation and probe reference"
      },
      {
        "label": "AI Security Testing Guide",
        "url": "https://owasp.org/www-project-ai-security-and-privacy-guide/",
        "description": "OWASP AI security testing methodology"
      }
    ]
  },
  {
    "id": "llm-guard",
    "name": "LLM Guard",
    "summary": "LLM Guard is a comprehensive security toolkit for protecting Large Language Models against prompt injections, toxic content, and data exfiltration attacks.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip with Python dependencies",
        "steps": [
          {
            "detail": "sudo apt update",
            "copyable": true
          },
          {
            "detail": "sudo apt install -y python3-pip python3-dev",
            "copyable": true
          },
          {
            "detail": "pip3 install llm-guard",
            "copyable": true
          },
          {
            "detail": "llm-guard --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Containerized deployment with all dependencies",
        "steps": [
          {
            "detail": "docker pull quay.io/lavalliere/llm-guard",
            "copyable": true
          },
          {
            "detail": "docker run --rm quay.io/lavalliere/llm-guard --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "macOS",
        "summary": "Install via Homebrew or pip",
        "steps": [
          {
            "detail": "brew install python3",
            "copyable": true
          },
          {
            "detail": "pip3 install llm-guard",
            "copyable": true
          },
          {
            "detail": "llm-guard --help",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Basic input sanitization",
        "command": "echo \"malicious prompt\" | llm-guard sanitize",
        "notes": []
      },
      {
        "description": "Scan for prompt injections",
        "command": "llm-guard scan --input-file prompts.txt --scanner prompt_injection",
        "notes": []
      },
      {
        "description": "Test content safety",
        "command": "llm-guard scan --text \"unsafe content\" --scanner toxicity",
        "notes": []
      },
      {
        "description": "Batch processing",
        "command": "llm-guard scan --input-file batch.txt --output-file results.json",
        "notes": []
      }
    ],
    "common_flags": [
      {
        "flag": "--scanner",
        "description": "Security scanner to use (prompt_injection, toxicity, etc.)"
      },
      {
        "flag": "--input-file",
        "description": "File containing text to scan"
      },
      {
        "flag": "--output-file",
        "description": "File to save scan results"
      },
      {
        "flag": "--text",
        "description": "Direct text input for scanning"
      }
    ],
    "operational_tips": [
      "Use multiple scanners for comprehensive protection",
      "Implement LLM Guard as middleware in AI pipelines",
      "Regularly update scanner models for latest threats",
      "Monitor false positive rates and adjust thresholds"
    ],
    "step_sequences": [
      {
        "title": "AI Input Validation Pipeline",
        "steps": [
          {
            "title": "Install and configure",
            "details": "Set up LLM Guard with required models",
            "command": "pip3 install llm-guard"
          },
          {
            "title": "Test prompt injection detection",
            "details": "Validate injection detection capabilities",
            "command": "llm-guard scan --text \"ignore previous instructions\" --scanner prompt_injection"
          },
          {
            "title": "Implement in application",
            "details": "Integrate as preprocessing step",
            "command": "# Add to AI application pipeline"
          },
          {
            "title": "Monitor and tune",
            "details": "Track performance and adjust parameters",
            "command": "# Review logs and metrics"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "Defensive AI Pipeline with LLM Guard",
        "stages": [
          {
            "label": "Input validation",
            "description": "Sanitize and validate all user inputs",
            "command": "llm-guard scan --input-file user_inputs.txt --scanner prompt_injection,toxicity"
          },
          {
            "label": "Content filtering",
            "description": "Block harmful or inappropriate content",
            "command": "llm-guard sanitize --input-file filtered_inputs.txt"
          },
          {
            "label": "Response validation",
            "description": "Check AI outputs for safety",
            "command": "llm-guard scan --input-file ai_responses.txt --scanner output_safety"
          },
          {
            "label": "Logging and monitoring",
            "description": "Track security events and incidents",
            "command": "# Implement comprehensive logging"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "SAFE",
        "meaning": "Content passed all security checks",
        "severity": "info"
      },
      {
        "indicator": "UNSAFE: prompt_injection",
        "meaning": "Prompt injection detected in input",
        "severity": "warning"
      },
      {
        "indicator": "SCORE: 0.92",
        "meaning": "Confidence score for detected vulnerability",
        "severity": "info"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom scanner development",
        "command": "llm-guard scan --custom-scanner my_scanner.py --input-file test_data.txt",
        "scenario": "Develop organization-specific security scanners",
        "notes": [
          "Extend LLM Guard with custom detection logic",
          "Train models on organization-specific threat patterns"
        ]
      }
    ],
    "comparison_table": null,
    "resources": [
      {
        "label": "LLM Guard Documentation",
        "url": "https://github.com/lavalliere/llm-guard",
        "description": "Official documentation and usage examples"
      }
    ]
  },
  {
    "id": "ml-pipeline-audit",
    "name": "ML Pipeline Audit Toolkit",
    "summary": "Comprehensive toolkit for auditing Machine Learning pipelines against data poisoning, adversarial examples, and model exfiltration attacks.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip with ML dependencies",
        "steps": [
          {
            "detail": "sudo apt update",
            "copyable": true
          },
          {
            "detail": "sudo apt install -y python3-pip python3-dev",
            "copyable": true
          },
          {
            "detail": "pip3 install ml-pipeline-audit scikit-learn torch",
            "copyable": true
          },
          {
            "detail": "ml-audit --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Container with GPU support for ML workloads",
        "steps": [
          {
            "detail": "docker pull mlsec/ml-pipeline-audit",
            "copyable": true
          },
          {
            "detail": "docker run --rm mlsec/ml-pipeline-audit --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Conda Environment",
        "summary": "Isolated ML environment with conda",
        "steps": [
          {
            "detail": "conda create -n ml-audit python=3.9",
            "copyable": true
          },
          {
            "detail": "conda activate ml-audit",
            "copyable": true
          },
          {
            "detail": "pip install ml-pipeline-audit scikit-learn torch",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Data poisoning detection",
        "command": "ml-audit detect-poisoning --dataset training_data.csv --model classifier.pkl",
        "notes": []
      },
      {
        "description": "Generate adversarial examples",
        "command": "ml-audit generate-adversarial --model target_model.h5 --input sample.jpg",
        "notes": []
      },
      {
        "description": "Model vulnerability assessment",
        "command": "ml-audit assess-vulnerabilities --model model.pkl --test-data test.csv",
        "notes": []
      },
      {
        "description": "Membership inference test",
        "command": "ml-audit test-membership --model model.pkl --candidate-data candidates.csv",
        "notes": []
      }
    ],
    "common_flags": [
      {
        "flag": "--dataset",
        "description": "Path to training/validation dataset"
      },
      {
        "flag": "--model",
        "description": "Path to trained model file"
      },
      {
        "flag": "--test-data",
        "description": "Test data for vulnerability assessment"
      },
      {
        "flag": "--output-dir",
        "description": "Directory for saving results and reports"
      }
    ],
    "operational_tips": [
      "Use GPU acceleration for large model testing",
      "Validate results with multiple attack types",
      "Implement findings in CI/CD security gates",
      "Regular auditing as part of ML lifecycle"
    ],
    "step_sequences": [
      {
        "title": "ML Pipeline Security Audit",
        "steps": [
          {
            "title": "Data integrity check",
            "details": "Test for data poisoning and tampering",
            "command": "ml-audit detect-poisoning --dataset data.csv --model model.pkl"
          },
          {
            "title": "Adversarial robustness test",
            "details": "Generate and test adversarial examples",
            "command": "ml-audit generate-adversarial --model model.pkl --input test_samples/"
          },
          {
            "title": "Privacy assessment",
            "details": "Check for data exfiltration vulnerabilities",
            "command": "ml-audit test-membership --model model.pkl --candidate-data candidates.csv"
          },
          {
            "title": "Generate audit report",
            "details": "Compile findings and recommendations",
            "command": "ml-audit generate-report --output-dir audit_results/"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "ML Security Assessment Framework",
        "stages": [
          {
            "label": "Data validation",
            "description": "Audit training data integrity",
            "command": "ml-audit detect-poisoning --dataset training.csv"
          },
          {
            "label": "Model testing",
            "description": "Test model against known attacks",
            "command": "ml-audit assess-vulnerabilities --model model.pkl"
          },
          {
            "label": "Adversarial evaluation",
            "description": "Generate and test adversarial inputs",
            "command": "ml-audit generate-adversarial --model model.pkl"
          },
          {
            "label": "Risk assessment",
            "description": "Calculate overall security risk score",
            "command": "ml-audit calculate-risk --audit-results results/"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "POISONING DETECTED: 85% confidence",
        "meaning": "Data poisoning identified with confidence score",
        "severity": "warning"
      },
      {
        "indicator": "ADVERSARIAL SUCCESS: 92%",
        "meaning": "Percentage of successful adversarial attacks",
        "severity": "warning"
      },
      {
        "indicator": "MEMBERSHIP LEAK: HIGH",
        "meaning": "Significant risk of membership inference",
        "severity": "critical"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom attack simulation",
        "command": "ml-audit simulate-attack --custom-attack my_attack.py --model target.pkl",
        "scenario": "Develop and test custom attack vectors",
        "notes": [
          "Create custom attack modules for specific ML architectures",
          "Test novel threat models and scenarios"
        ]
      }
    ],
    "comparison_table": null,
    "resources": [
      {
        "label": "ML Pipeline Audit Documentation",
        "url": "https://github.com/mlsec/ml-pipeline-audit",
        "description": "Official toolkit documentation and examples"
      },
      {
        "label": "Adversarial ML Research",
        "url": "https://arxiv.org/abs/2002.08619",
        "description": "Research paper on adversarial attacks"
      }
    ]
  },
  {
    "id": "llmguard",
    "name": "LLMGuard",
    "summary": "LLMGuard is a security library for LLM input and output sanitization. It provides defenses against prompt injection, jailbreaks, and ensures LLM outputs comply with safety policies.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip",
        "steps": [
          {
            "detail": "sudo apt-get update && sudo apt-get install -y python3-pip",
            "copyable": true
          },
          {
            "detail": "pip3 install llmguard",
            "copyable": true
          },
          {
            "detail": "python3 -c \"import llmguard; print(llmguard.__version__)\"",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Install from pip",
        "steps": [
          {
            "detail": "pip3 install llmguard",
            "copyable": true
          },
          {
            "detail": "python3 -c \"import llmguard; print(llmguard.__version__)\"",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Use Docker image",
        "steps": [
          {
            "detail": "docker pull llmguard:latest",
            "copyable": true
          },
          {
            "detail": "docker run -it llmguard:latest python3 -m llmguard --help",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Validate user input",
        "command": "python3 -c \"from llmguard import LLMGuard; guard = LLMGuard(); print(guard.validate_input(\\\"test\\\"))\"",
        "notes": [
          "Check user input for safety"
        ]
      },
      {
        "description": "Sanitize LLM output",
        "command": "python3 -c \"from llmguard import LLMGuard; guard = LLMGuard(); print(guard.sanitize_output(\\\"response\\\"))\"",
        "notes": [
          "Ensure model output is safe"
        ]
      }
    ],
    "step_sequences": [
      {
        "title": "LLM Input/Output Security",
        "steps": [
          {
            "title": "Initialize LLMGuard",
            "details": "Set up security layer",
            "command": "from llmguard import LLMGuard\nguard = LLMGuard()"
          },
          {
            "title": "Validate input",
            "details": "Check for prompt injection",
            "command": "result = guard.validate_input(user_input)"
          },
          {
            "title": "Process with LLM",
            "details": "Send safe input to model",
            "command": "response = model.generate(result)"
          },
          {
            "title": "Sanitize output",
            "details": "Clean model response",
            "command": "safe_output = guard.sanitize_output(response)"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "LLM Application Security Workflow",
        "stages": [
          {
            "label": "Setup guards",
            "description": "Initialize LLMGuard",
            "command": "guard = LLMGuard(policies=['no-sql-injection', 'no-prompt-injection'])"
          },
          {
            "label": "Process input",
            "description": "Validate and sanitize user input",
            "command": "safe_input = guard.validate_input(user_input)"
          },
          {
            "label": "Query LLM",
            "description": "Send to language model",
            "command": "response = llm.generate(safe_input)"
          },
          {
            "label": "Secure output",
            "description": "Sanitize before returning",
            "command": "return guard.sanitize_output(response)"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "INJECTION DETECTED",
        "meaning": "Prompt injection pattern found in input",
        "severity": "high"
      },
      {
        "indicator": "JAILBREAK ATTEMPT",
        "meaning": "Attempt to bypass safety mechanisms",
        "severity": "high"
      },
      {
        "indicator": "POLICY VIOLATION",
        "meaning": "Output violates defined safety policy",
        "severity": "medium"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom policy definition",
        "command": "guard = LLMGuard(custom_policies=['ban-harmful-keywords', 'rate-limit-requests'])",
        "scenario": "Implement custom security policies",
        "notes": [
          "Define organization-specific rules",
          "Policy scoring and weighting"
        ]
      }
    ],
    "resources": [
      {
        "label": "LLMGuard Documentation",
        "url": "https://github.com/protectai/llmguard",
        "description": "Official LLMGuard repository and documentation"
      },
      {
        "label": "LLM Security Best Practices",
        "url": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
        "description": "OWASP Top 10 for LLM Applications"
      }
    ]
  },
  {
    "id": "lakera-guard",
    "name": "Lakera Guard",
    "summary": "Lakera Guard is an API-based security service for detecting and preventing LLM attacks including prompt injection, jailbreaks, and PII leakage. Provides real-time threat detection.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install SDK via pip",
        "steps": [
          {
            "detail": "sudo apt-get update && sudo apt-get install -y python3-pip",
            "copyable": true
          },
          {
            "detail": "pip3 install lakera-guard",
            "copyable": true
          },
          {
            "detail": "python3 -c \"import lakera; print(lakera.__version__)\"",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Install from pip",
        "steps": [
          {
            "detail": "pip3 install lakera-guard",
            "copyable": true
          },
          {
            "detail": "export LAKERA_API_KEY='your-api-key'",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Docker deployment",
        "steps": [
          {
            "detail": "docker pull lakera/guard:latest",
            "copyable": true
          },
          {
            "detail": "docker run -e LAKERA_API_KEY=your-api-key lakera/guard:latest",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Check input for attacks",
        "command": "python3 -c \"from lakera import Guard; client = Guard(api_key='key'); result = client.detect.prompt_injection('test')\"",
        "notes": [
          "Real-time attack detection"
        ]
      },
      {
        "description": "Detect PII in text",
        "command": "python3 -c \"from lakera import Guard; client = Guard(); result = client.detect.pii('text with email@example.com')\"",
        "notes": [
          "Identify sensitive information"
        ]
      }
    ],
    "step_sequences": [
      {
        "title": "Lakera Guard Integration",
        "steps": [
          {
            "title": "Set up API key",
            "details": "Configure Lakera credentials",
            "command": "export LAKERA_API_KEY='your-api-key'"
          },
          {
            "title": "Initialize client",
            "details": "Create Guard instance",
            "command": "from lakera import Guard\nclient = Guard()"
          },
          {
            "title": "Check input",
            "details": "Scan for prompt injections",
            "command": "result = client.detect.prompt_injection(user_input)"
          },
          {
            "title": "Take action",
            "details": "Block or allow based on risk",
            "command": "if result.is_attack: block_request()"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "LLM Threat Detection Workflow",
        "stages": [
          {
            "label": "Initialize",
            "description": "Set up Lakera Guard client",
            "command": "client = Guard(api_key=os.getenv('LAKERA_API_KEY'))"
          },
          {
            "label": "Receive request",
            "description": "Get user input",
            "command": "user_input = request.json()['text']"
          },
          {
            "label": "Scan for threats",
            "description": "Check for multiple attack types",
            "command": "threats = client.detect.multiple_threats(user_input)"
          },
          {
            "label": "Block if needed",
            "description": "Prevent malicious requests",
            "command": "if threats.risk_level == 'HIGH': return error_response()"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "Prompt injection: HIGH confidence",
        "meaning": "Strong indicator of injection attack",
        "severity": "critical"
      },
      {
        "indicator": "PII detected: SSN, email",
        "meaning": "Sensitive personal information found",
        "severity": "high"
      },
      {
        "indicator": "Jailbreak pattern: Known technique",
        "meaning": "Recognized jailbreak attempt",
        "severity": "high"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom threat model",
        "command": "client = Guard(custom_models=['harmful-content', 'data-theft'])",
        "scenario": "Deploy custom threat detection models",
        "notes": [
          "Organization-specific threat models",
          "Fine-tuned detection algorithms"
        ]
      }
    ],
    "resources": [
      {
        "label": "Lakera Guard Documentation",
        "url": "https://docs.lakera.ai/",
        "description": "Official Lakera Guard API documentation"
      },
      {
        "label": "LLM Security Research",
        "url": "https://www.lakera.ai/blog",
        "description": "Latest LLM security research and findings"
      }
    ]
  },
  {
    "id": "promptfoo",
    "name": "Promptfoo",
    "summary": "Promptfoo is an LLM testing framework for evaluating and validating LLM prompts, outputs, and model behaviors. Helps identify prompt weaknesses, jailbreaks, and ensure consistent performance.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via npm",
        "steps": [
          {
            "detail": "sudo apt-get update && sudo apt-get install -y nodejs npm",
            "copyable": true
          },
          {
            "detail": "npm install -g promptfoo",
            "copyable": true
          },
          {
            "detail": "promptfoo --version",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Install from npm",
        "steps": [
          {
            "detail": "npm install -g promptfoo",
            "copyable": true
          },
          {
            "detail": "promptfoo --version",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Run with Docker",
        "steps": [
          {
            "detail": "docker pull promptfoo:latest",
            "copyable": true
          },
          {
            "detail": "docker run -it promptfoo:latest promptfoo --help",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Initialize test suite",
        "command": "promptfoo init",
        "notes": [
          "Create promptfooconfig.yaml"
        ]
      },
      {
        "description": "Run tests",
        "command": "promptfoo eval",
        "notes": [
          "Execute all configured tests"
        ]
      },
      {
        "description": "View results",
        "command": "promptfoo view",
        "notes": [
          "Open test results in browser"
        ]
      }
    ],
    "step_sequences": [
      {
        "title": "LLM Prompt Evaluation",
        "steps": [
          {
            "title": "Set up test suite",
            "details": "Initialize Promptfoo project",
            "command": "promptfoo init"
          },
          {
            "title": "Define test cases",
            "details": "Create test scenarios",
            "command": "# Edit promptfooconfig.yaml with test cases"
          },
          {
            "title": "Configure prompts",
            "details": "Add prompts to test",
            "command": "# Define prompt variants for evaluation"
          },
          {
            "title": "Execute tests",
            "details": "Run evaluation suite",
            "command": "promptfoo eval"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "Prompt Testing Workflow",
        "stages": [
          {
            "label": "Initialize",
            "description": "Set up test environment",
            "command": "promptfoo init --model openai:gpt-4"
          },
          {
            "label": "Define tests",
            "description": "Create test cases",
            "command": "cat > promptfooconfig.yaml << 'EOF'\nprompts: ['prompts/*.txt']\ntest: 'tests.txt'\nEOF"
          },
          {
            "label": "Run evaluation",
            "description": "Execute test suite",
            "command": "promptfoo eval"
          },
          {
            "label": "Analyze results",
            "description": "Review test results",
            "command": "promptfoo view"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "PASS: Prompt behaves as expected",
        "meaning": "Test case passed successfully",
        "severity": "info"
      },
      {
        "indicator": "FAIL: Unexpected output format",
        "meaning": "Output doesn't match expected format",
        "severity": "high"
      },
      {
        "indicator": "JAILBREAK: Prompt bypassed restrictions",
        "meaning": "Prompt injection or jailbreak successful",
        "severity": "critical"
      }
    ],
    "advanced_usage": [
      {
        "title": "Multi-model comparison",
        "command": "promptfoo eval --models 'openai:gpt-4,openai:gpt-3.5-turbo,anthropic:claude-3'",
        "scenario": "Compare prompt performance across models",
        "notes": [
          "Test prompt compatibility",
          "Identify model-specific vulnerabilities"
        ]
      }
    ],
    "resources": [
      {
        "label": "Promptfoo Documentation",
        "url": "https://www.promptfoo.dev/",
        "description": "Official Promptfoo documentation and guides"
      },
      {
        "label": "LLM Prompt Engineering",
        "url": "https://github.com/promptfoo/promptfoo",
        "description": "Prompt engineering best practices and examples"
      }
    ]
  },
  {
    "id": "pyrit",
    "name": "PyRIT",
    "summary": "PyRIT (Python Risk Identification Toolkit) is Microsoft's open-source framework for AI red teaming, designed to automate the testing of AI systems for security vulnerabilities, bias, and other risks.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip with Python 3.9+",
        "steps": [
          {
            "detail": "sudo apt update",
            "copyable": true
          },
          {
            "detail": "sudo apt install -y python3-pip python3-venv",
            "copyable": true
          },
          {
            "detail": "python3 -m pip install --user pyrit-ai",
            "copyable": true
          },
          {
            "detail": "pyrit --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Install via pip with virtual environment",
        "steps": [
          {
            "detail": "sudo apt update && sudo apt install -y python3-pip python3-venv",
            "copyable": true
          },
          {
            "detail": "python3 -m venv pyrit-env",
            "copyable": true
          },
          {
            "detail": "source pyrit-env/bin/activate",
            "copyable": true
          },
          {
            "detail": "pip install pyrit-ai",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Official containerized deployment",
        "steps": [
          {
            "detail": "docker pull mcr.microsoft.com/pyrit:latest",
            "copyable": true
          },
          {
            "detail": "docker run --rm -it mcr.microsoft.com/pyrit:latest",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Basic prompt injection test",
        "command": "pyrit --target-endpoint https://api.example.com/v1/chat/completions --attack-type prompt_injection",
        "notes": []
      },
      {
        "description": "Red team AI system with multiple attack vectors",
        "command": "pyrit --config config.yaml --attack-types prompt_injection,uid_hallucination,bias_testing",
        "notes": []
      },
      {
        "description": "Generate comprehensive red team report",
        "command": "pyrit --target-endpoint https://api.example.com/v1 --output-format json --output-file results.json",
        "notes": []
      },
      {
        "description": "Run predefined red team scenario",
        "command": "pyrit --scenario financial_ai_redteam --iterations 100",
        "notes": []
      }
    ],
    "common_flags": [
      {
        "flag": "--target-endpoint",
        "description": "Target AI API endpoint to test"
      },
      {
        "flag": "--attack-types",
        "description": "Types of attacks to run (prompt_injection, uid_hallucination, bias_testing, etc.)"
      },
      {
        "flag": "--config",
        "description": "Configuration file with test parameters"
      },
      {
        "flag": "--scenario",
        "description": "Predefined red team scenario"
      },
      {
        "flag": "--iterations",
        "description": "Number of test iterations per attack type"
      },
      {
        "flag": "--output-format",
        "description": "Output format (json, html, csv)"
      }
    ],
    "operational_tips": [
      "Configure API keys and authentication for target AI systems",
      "Use configuration files for complex red team scenarios",
      "Start with small iteration counts to validate setup",
      "Review generated reports for detailed vulnerability analysis",
      "Combine multiple attack types for comprehensive assessment"
    ],
    "step_sequences": [
      {
        "title": "AI System Red Team Assessment",
        "steps": [
          {
            "title": "Configure target access",
            "details": "Set up API access to target AI system",
            "command": "export TARGET_API_KEY=your_key_here"
          },
          {
            "title": "Create configuration file",
            "details": "Set up PyRIT configuration with attack parameters",
            "command": "cat > config.yaml << EOF\ntarget_endpoint: https://api.example.com/v1\nattack_types:\n  - prompt_injection\n  - uid_hallucination\n  - bias_testing\niterations: 50\nEOF"
          },
          {
            "title": "Execute red team attacks",
            "details": "Run comprehensive AI security testing",
            "command": "pyrit --config config.yaml"
          },
          {
            "title": "Generate assessment report",
            "details": "Create detailed red team findings report",
            "command": "pyrit --config config.yaml --output-format html --output-file redteam_report.html"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "AI Red Team Operations Pipeline",
        "stages": [
          {
            "label": "Target reconnaissance",
            "description": "Identify AI endpoints and capabilities",
            "command": "# Manual API discovery and documentation review"
          },
          {
            "label": "Attack scenario planning",
            "description": "Design red team scenarios based on target",
            "command": "pyrit --list-scenarios"
          },
          {
            "label": "Automated testing execution",
            "description": "Run PyRIT attack scenarios",
            "command": "pyrit --config redteam_config.yaml --iterations 100"
          },
          {
            "label": "Manual validation",
            "description": "Validate automated findings with manual testing",
            "command": "# Manual prompt crafting and testing"
          },
          {
            "label": "Report generation",
            "description": "Compile comprehensive red team report",
            "command": "pyrit --config redteam_config.yaml --output-format json --output-file final_report.json"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "VULNERABILITY: Prompt Injection",
        "meaning": "AI system susceptible to prompt injection attacks",
        "severity": "critical"
      },
      {
        "indicator": "BIAS DETECTED: Gender/ Race/ Age",
        "meaning": "Model shows bias in outputs",
        "severity": "high"
      },
      {
        "indicator": "UID HALLUCINATION: Success rate 85%",
        "meaning": "AI fabricates user identities or information",
        "severity": "high"
      },
      {
        "indicator": "SECURITY SCORE: 6.5/10",
        "meaning": "Overall security assessment score",
        "severity": "info"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom attack scenario development",
        "command": "pyrit --custom-scenario custom_attacks.yaml --iterations 200",
        "scenario": "Create and execute organization-specific attack scenarios",
        "notes": [
          "Develop custom attack prompts",
          "Test domain-specific vulnerabilities"
        ]
      },
      {
        "title": "Comparative AI security analysis",
        "command": "pyrit --multi-target config.yaml --compare-models",
        "scenario": "Compare security across multiple AI models",
        "notes": [
          "Identify relative vulnerabilities",
          "Benchmark security improvements"
        ]
      }
    ],
    "resources": [
      {
        "label": "PyRIT Documentation",
        "url": "https://github.com/Azure/PyRIT",
        "description": "Official PyRIT documentation and examples"
      },
      {
        "label": "Microsoft AI Red Team",
        "url": "https://www.microsoft.com/en-us/security/engineering/ai-red-team",
        "description": "Microsoft's AI red teaming methodology"
      },
      {
        "label": "AI Security Testing Guide",
        "url": "https://owasp.org/www-project-ai-security-and-privacy-guide/",
        "description": "OWASP AI security testing framework"
      }
    ]
  },
  {
    "id": "pentestgpt",
    "name": "PentestGPT",
    "summary": "PentestGPT is an LLM-driven penetration testing automation tool that leverages large language models to generate and execute penetration testing workflows, adapt to target environments, and produce detailed security reports.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip with Python 3.8+",
        "steps": [
          {
            "detail": "sudo apt update",
            "copyable": true
          },
          {
            "detail": "sudo apt install -y python3-pip python3-venv git",
            "copyable": true
          },
          {
            "detail": "git clone https://github.com/GreyDGL/PentestGPT.git",
            "copyable": true
          },
          {
            "detail": "cd PentestGPT && python3 -m pip install -e .",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Install with pentesting dependencies",
        "steps": [
          {
            "detail": "sudo apt update && sudo apt install -y python3-pip python3-venv git nmap",
            "copyable": true
          },
          {
            "detail": "git clone https://github.com/GreyDGL/PentestGPT.git",
            "copyable": true
          },
          {
            "detail": "cd PentestGPT && pip install -e .",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Containerized deployment with security tools",
        "steps": [
          {
            "detail": "docker pull greydgl/pentestgpt:latest",
            "copyable": true
          },
          {
            "detail": "docker run --rm -it greydgl/pentestgpt:latest",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Interactive penetration testing session",
        "command": "pentestgpt --target 192.168.1.100 --model gpt-4 --interactive",
        "notes": []
      },
      {
        "description": "Automated web application testing",
        "command": "pentestgpt --target https://example.com --type web_app --auto",
        "notes": []
      },
      {
        "description": "Network penetration testing with custom scope",
        "command": "pentestgpt --target 192.168.1.0/24 --type network --scope internal",
        "notes": []
      },
      {
        "description": "Generate pentest report from logs",
        "command": "pentestgpt --report --input-file pentest.log --output-format pdf",
        "notes": []
      }
    ],
    "common_flags": [
      {
        "flag": "--target",
        "description": "Target IP address, domain, or network range"
      },
      {
        "flag": "--model",
        "description": "LLM model to use (gpt-4, gpt-3.5-turbo, claude-3, etc.)"
      },
      {
        "flag": "--type",
        "description": "Pentest type (web_app, network, api, mobile)"
      },
      {
        "flag": "--interactive",
        "description": "Run in interactive mode with human oversight"
      },
      {
        "flag": "--auto",
        "description": "Run fully automated pentest workflow"
      },
      {
        "flag": "--scope",
        "description": "Testing scope (internal, external, limited)"
      }
    ],
    "operational_tips": [
      "Configure API keys for your preferred LLM provider",
      "Start with interactive mode to understand the workflow",
      "Review generated commands before execution in production",
      "Use scope limitations to prevent unauthorized testing",
      "Combine with traditional tools for comprehensive coverage"
    ],
    "step_sequences": [
      {
        "title": "AI-Driven Web Application Penetration Test",
        "steps": [
          {
            "title": "Configure LLM access",
            "details": "Set up API key for LLM provider",
            "command": "export OPENAI_API_KEY=your_key_here"
          },
          {
            "title": "Initialize pentest session",
            "details": "Start PentestGPT with target specification",
            "command": "pentestgpt --target https://example.com --type web_app --interactive"
          },
          {
            "title": "AI-guided reconnaissance",
            "details": "Let AI perform automated reconnaissance",
            "command": "# AI executes: nmap, dirb, whatweb, etc."
          },
          {
            "title": "Vulnerability analysis",
            "details": "AI analyzes findings and suggests exploits",
            "command": "# AI suggests and prepares exploit attempts"
          },
          {
            "title": "Report generation",
            "details": "Generate comprehensive pentest report",
            "command": "pentestgpt --report --input-file session.log --output-format html"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "AI-Augmented Penetration Testing Pipeline",
        "stages": [
          {
            "label": "Target definition",
            "description": "Define scope and objectives",
            "command": "pentestgpt --setup --target 192.168.1.100 --scope authorized"
          },
          {
            "label": "Intelligence gathering",
            "description": "AI-driven reconnaissance and enumeration",
            "command": "pentestgpt --target 192.168.1.100 --phase recon --auto"
          },
          {
            "label": "Vulnerability identification",
            "description": "AI analysis of discovered services and applications",
            "command": "pentestgpt --target 192.168.1.100 --phase vuln_analysis --interactive"
          },
          {
            "label": "Exploitation planning",
            "description": "AI suggests exploit strategies and prepares payloads",
            "command": "pentestgpt --target 192.168.1.100 --phase exploit_plan --interactive"
          },
          {
            "label": "Post-exploitation",
            "description": "AI-guided privilege escalation and persistence",
            "command": "pentestgpt --target 192.168.1.100 --phase post_exploit --interactive"
          },
          {
            "label": "Reporting",
            "description": "AI-generated comprehensive security report",
            "command": "pentestgpt --report --session-id session_123 --output-format pdf"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "FINDING: SQL Injection (Critical)",
        "meaning": "AI discovered critical SQL injection vulnerability",
        "severity": "critical"
      },
      {
        "indicator": "EXPLOIT: Successful privilege escalation",
        "meaning": "AI successfully escalated privileges on target",
        "severity": "high"
      },
      {
        "indicator": "RECOMMENDATION: Patch CVE-2023-1234",
        "meaning": "AI recommends specific security patches",
        "severity": "medium"
      },
      {
        "indicator": "RISK SCORE: 8.2/10",
        "meaning": "Overall risk assessment of target",
        "severity": "info"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom pentest playbook development",
        "command": "pentestgpt --create-playbook --industry finance --compliance pci_dss",
        "scenario": "Create industry-specific pentest playbooks",
        "notes": [
          "Tailor testing to compliance requirements",
          "Include industry-specific vulnerability checks"
        ]
      },
      {
        "title": "Multi-target coordinated testing",
        "command": "pentestgpt --multi-target targets.yaml --coordinated --max-concurrent 5",
        "scenario": "Simultaneous testing of multiple targets",
        "notes": [
          "Coordinate testing across network segments",
          "Aggregate findings across targets"
        ]
      }
    ],
    "resources": [
      {
        "label": "PentestGPT Repository",
        "url": "https://github.com/GreyDGL/PentestGPT",
        "description": "Official PentestGPT documentation and examples"
      },
      {
        "label": "AI in Penetration Testing",
        "url": "https://arxiv.org/abs/2308.06772",
        "description": "Research paper on AI-augmented penetration testing"
      },
      {
        "label": "LLM Security Applications",
        "url": "https://owasp.org/www-project-llm-top-ten/",
        "description": "OWASP LLM security guidelines"
      }
    ]
  },
  {
    "id": "nemo_guardrails",
    "name": "NeMo Guardrails",
    "summary": "NVIDIA NeMo Guardrails is an open-source toolkit for adding programmable guardrails to LLM-based applications, providing safety, security, and context-aware controls for AI conversations and outputs.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip with Python 3.9+",
        "steps": [
          {
            "detail": "sudo apt update",
            "copyable": true
          },
          {
            "detail": "sudo apt install -y python3-pip python3-venv",
            "copyable": true
          },
          {
            "detail": "python3 -m pip install nemoguardrails",
            "copyable": true
          },
          {
            "detail": "nemoguardrails --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Install with development dependencies",
        "steps": [
          {
            "detail": "sudo apt update && sudo apt install -y python3-pip python3-dev",
            "copyable": true
          },
          {
            "detail": "pip3 install nemoguardrails[dev]",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Official containerized deployment",
        "steps": [
          {
            "detail": "docker pull nvcr.io/nvidia/nemo:guardrails",
            "copyable": true
          },
          {
            "detail": "docker run --rm -it nvcr.io/nvidia/nemo:guardrails",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Basic safety guardrail configuration",
        "command": "nemoguardrails --config config.yaml --test-input \"How to hack into a system?\"",
        "notes": []
      },
      {
        "description": "Initialize new guardrails project",
        "command": "nemoguardrails init --project-name my_bot_guardrails",
        "notes": []
      },
      {
        "description": "Test custom guardrails configuration",
        "command": "nemoguardrails test --config ./config --test-file test_inputs.txt",
        "notes": []
      },
      {
        "description": "Run guardrails with specific LLM",
        "command": "nemoguardrails --model openai:gpt-4 --config ./config --interactive",
        "notes": []
      }
    ],
    "common_flags": [
      {
        "flag": "--config",
        "description": "Path to guardrails configuration directory"
      },
      {
        "flag": "--model",
        "description": "LLM model to use with guardrails"
      },
      {
        "flag": "--test-input",
        "description": "Single input to test against guardrails"
      },
      {
        "flag": "--test-file",
        "description": "File containing multiple test inputs"
      },
      {
        "flag": "--interactive",
        "description": "Run in interactive mode"
      },
      {
        "flag": "--verbose",
        "description": "Verbose output showing guardrail decisions"
      }
    ],
    "operational_tips": [
      "Start with predefined guardrail configurations before customizing",
      "Test guardrails with diverse input scenarios",
      "Use verbose mode to understand guardrail decision logic",
      "Combine multiple guardrail types for comprehensive protection",
      "Regularly update guardrails based on new threat patterns"
    ],
    "step_sequences": [
      {
        "title": "Implement LLM Safety Guardrails",
        "steps": [
          {
            "title": "Initialize guardrails project",
            "details": "Create new guardrails configuration",
            "command": "nemoguardrails init --project-name security_bot"
          },
          {
            "title": "Configure safety policies",
            "details": "Set up safety and security rules",
            "command": "cat > config/config.yml << EOF\nmodels:\n  - type: main\n    model: openai:gpt-4\n    parameters:\n      temperature: 0.0\n\nrails:\n  input:\n    flows:\n      - self check input\n  output:\n    flows:\n      - self check output\nEOF"
          },
          {
            "title": "Test guardrails configuration",
            "details": "Validate guardrails with test inputs",
            "command": "nemoguardrails test --config ./config --test-file security_tests.txt"
          },
          {
            "title": "Deploy with LLM integration",
            "details": "Deploy guardrails with your LLM application",
            "command": "nemoguardrails --config ./config --model openai:gpt-4 --interactive"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "AI Application Security Implementation",
        "stages": [
          {
            "label": "Threat modeling",
            "description": "Identify potential AI security risks",
            "command": "# Analyze AI application use cases and risks"
          },
          {
            "label": "Guardrail design",
            "description": "Design appropriate guardrail configurations",
            "command": "nemoguardrails init --template security --project-name app_security"
          },
          {
            "label": "Configuration development",
            "description": "Implement custom guardrails and policies",
            "command": "# Develop custom .colang files for guardrails"
          },
          {
            "label": "Testing and validation",
            "description": "Test guardrails against adversarial inputs",
            "command": "nemoguardrails test --config ./config --adversarial"
          },
          {
            "label": "Integration deployment",
            "description": "Integrate guardrails into production AI system",
            "command": "nemoguardrails serve --config ./config --port 8080"
          },
          {
            "label": "Monitoring and updates",
            "description": "Monitor guardrail effectiveness and update",
            "command": "# Review logs and update guardrail policies"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "BLOCKED: Input violates safety policy",
        "meaning": "Guardrail successfully blocked harmful input",
        "severity": "info"
      },
      {
        "indicator": "ALLOWED: Input passes all checks",
        "meaning": "Input cleared all guardrail validations",
        "severity": "info"
      },
      {
        "indicator": "MODIFIED: Output sanitized",
        "meaning": "Guardrail modified output for safety",
        "severity": "warning"
      },
      {
        "indicator": "VIOLATION: Prompt injection detected",
        "meaning": "Input identified as prompt injection attempt",
        "severity": "high"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom guardrail development",
        "command": "nemoguardrails add-rail --type custom --name security_policy --file custom_security.colang",
        "scenario": "Develop domain-specific guardrails",
        "notes": [
          "Write custom .colang guardrail logic",
          "Implement business-specific security policies"
        ]
      },
      {
        "title": "Multi-model guardrail orchestration",
        "command": "nemoguardrails --multi-model config.yaml --models openai:gpt-4,anthropic:claude-3",
        "scenario": "Apply consistent guardrails across multiple LLMs",
        "notes": [
          "Ensure consistent security across models",
          "Compare model responses under same guardrails"
        ]
      }
    ],
    "resources": [
      {
        "label": "NeMo Guardrails Documentation",
        "url": "https://github.com/NVIDIA/NeMo-Guardrails",
        "description": "Official NeMo Guardrails documentation and guides"
      },
      {
        "label": "Colang Language Reference",
        "url": "https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/colang.md",
        "description": "Programming language for guardrails logic"
      },
      {
        "label": "AI Safety and Security",
        "url": "https://www.nvidia.com/en-us/ai-data-science/ai-safety/",
        "description": "NVIDIA AI safety research and guidelines"
      }
    ]
  }
]