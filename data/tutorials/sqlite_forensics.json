{
  "id": "sqlite_forensics",
  "title": "SQLite Database Forensics",
  "description": "Comprehensive SQLite forensics covering database structure analysis, deleted record recovery, journal examination, and application artifact extraction.",
  "type": "tutorial",
  "steps": [
    {
      "id": "sqlite_fundamentals",
      "title": "SQLite Fundamentals for Forensics",
      "content": "OBJECTIVE: Understand SQLite database architecture and its significance in digital forensics investigations.\n\nACADEMIC BACKGROUND:\nSQLite is the most widely deployed database engine, embedded in virtually every application that stores structured data locally. Web browsers, mobile apps, messaging applications, and operating systems all use SQLite for data storage. Understanding SQLite internals enables recovery of deleted records and extraction of hidden artifacts.\n\nFORENSIC SIGNIFICANCE:\n\n1. WHERE SQLITE IS FOUND:\n   ```\n   Common SQLite Databases:\n   \n   Web Browsers:\n   | Browser | Database | Content |\n   |---------|----------|--------|\n   | Chrome | History | URLs, visits, downloads |\n   | Chrome | Cookies | Session cookies |\n   | Chrome | Login Data | Saved credentials |\n   | Chrome | Web Data | Autofill, credit cards |\n   | Firefox | places.sqlite | History, bookmarks |\n   | Firefox | cookies.sqlite | Cookies |\n   | Firefox | logins.json* | Credentials (JSON+SQLite) |\n   | Safari | History.db | Browsing history |\n   \n   Mobile Devices:\n   | Platform | Database | Content |\n   |----------|----------|--------|\n   | iOS | sms.db | Text messages |\n   | iOS | AddressBook.sqlitedb | Contacts |\n   | iOS | call_history.db | Call logs |\n   | Android | contacts2.db | Contacts |\n   | Android | mmssms.db | SMS/MMS |\n   | Android | telephony.db | Call history |\n   \n   Applications:\n   | App | Database | Content |\n   |-----|----------|--------|\n   | Skype | main.db | Messages, contacts |\n   | Dropbox | filecache.db | Sync history |\n   | Slack | slack-workspaces | Messages |\n   | Signal | signal.sqlite | Messages |\n   ```\n\n2. SQLITE FILE STRUCTURE:\n   ```\n   SQLite Database Format:\n   \n   Header (100 bytes at offset 0):\n   - Signature: \"SQLite format 3\\0\" (16 bytes)\n   - Page size: 2 bytes (offset 16)\n   - File format versions: offset 18-19\n   - Page cache size: offset 48\n   - Schema cookie: offset 40\n   - Schema format: offset 44\n   \n   Database Organization:\n   - Database = collection of pages\n   - Page sizes: 512 to 65536 bytes (power of 2)\n   - Page 1: Contains database header + schema\n   - Subsequent pages: B-tree pages, overflow, freelist\n   \n   Page Types:\n   - Lock-byte page (page 1 header)\n   - Freelist pages (deleted content)\n   - B-tree pages (table and index data)\n   - Overflow pages (large records)\n   - Pointer map pages (auto-vacuum mode)\n   ```\n\n3. VERIFYING SQLITE FILES:\n   ```bash\n   # Check file signature\n   xxd -l 16 database.db\n   # Should show: 53514c69746520666f726d617420330\n   # ASCII: \"SQLite format 3\"\n   \n   # Using file command\n   file database.db\n   # Output: SQLite 3.x database\n   \n   # Get header information\n   sqlite3 database.db \"PRAGMA page_size;\"\n   sqlite3 database.db \"PRAGMA page_count;\"\n   sqlite3 database.db \"PRAGMA schema_version;\"\n   sqlite3 database.db \"PRAGMA user_version;\"\n   \n   # Check integrity\n   sqlite3 database.db \"PRAGMA integrity_check;\"\n   ```\n\n4. BASIC SQLITE ANALYSIS:\n   ```bash\n   # List all tables\n   sqlite3 database.db \".tables\"\n   \n   # Show schema (all objects)\n   sqlite3 database.db \".schema\"\n   \n   # Show schema for specific table\n   sqlite3 database.db \".schema tablename\"\n   \n   # Describe table structure\n   sqlite3 database.db \"PRAGMA table_info(tablename);\"\n   \n   # Count records\n   sqlite3 database.db \"SELECT COUNT(*) FROM tablename;\"\n   \n   # Export to CSV\n   sqlite3 -header -csv database.db \"SELECT * FROM tablename;\" > output.csv\n   \n   # Query with formatted output\n   sqlite3 -column -header database.db \"SELECT * FROM tablename LIMIT 10;\"\n   ```\n\n5. TIMESTAMP HANDLING:\n   ```bash\n   # SQLite stores dates in various formats:\n   # 1. Unix epoch (seconds since 1970)\n   # 2. Unix epoch milliseconds\n   # 3. WebKit/Chrome epoch (microseconds since 1601)\n   # 4. TEXT (ISO8601: \"YYYY-MM-DD HH:MM:SS\")\n   \n   # Convert Unix epoch\n   sqlite3 database.db \"SELECT datetime(timestamp, 'unixepoch') FROM table;\"\n   \n   # Convert Unix milliseconds\n   sqlite3 database.db \"SELECT datetime(timestamp/1000, 'unixepoch') FROM table;\"\n   \n   # Convert WebKit/Chrome timestamp\n   sqlite3 database.db \"SELECT datetime((timestamp/1000000)-11644473600, 'unixepoch') FROM table;\"\n   \n   # Local time conversion\n   sqlite3 database.db \"SELECT datetime(timestamp, 'unixepoch', 'localtime') FROM table;\"\n   ```\n\n6. FORENSIC COPY PROCEDURES:\n   ```bash\n   # Always work on a copy, never original\n   \n   # Simple file copy (if database not in use)\n   cp database.db /forensics/database_copy.db\n   \n   # SQLite backup (handles locks)\n   sqlite3 database.db \".backup /forensics/database_copy.db\"\n   \n   # With hash verification\n   sha256sum database.db > /forensics/original_hash.txt\n   cp database.db /forensics/database_copy.db\n   sha256sum /forensics/database_copy.db >> /forensics/copy_hash.txt\n   \n   # Copy related files\n   cp database.db-journal /forensics/  # Rollback journal\n   cp database.db-wal /forensics/      # Write-ahead log\n   cp database.db-shm /forensics/      # Shared memory\n   ```\n\n7. BROWSER DATABASE LOCATIONS:\n   ```bash\n   # Chrome (Linux)\n   ~/.config/google-chrome/Default/History\n   ~/.config/google-chrome/Default/Cookies\n   ~/.config/google-chrome/Default/\"Login Data\"\n   \n   # Chrome (Windows)\n   %LOCALAPPDATA%\\Google\\Chrome\\User Data\\Default\\History\n   \n   # Chrome (macOS)\n   ~/Library/Application Support/Google/Chrome/Default/History\n   \n   # Firefox (Linux)\n   ~/.mozilla/firefox/*.default/places.sqlite\n   \n   # Firefox (Windows)\n   %APPDATA%\\Mozilla\\Firefox\\Profiles\\*.default\\places.sqlite\n   \n   # Safari (macOS)\n   ~/Library/Safari/History.db\n   ```\n\nDETECTION INDICATORS:\n- SQLite signature verified\n- Schema documented\n- Tables enumerated\n- Timestamps properly converted\n\nCOMMON PITFALLS:\n- Analyzing original instead of copy\n- Wrong timestamp epoch conversion\n- Missing WAL/journal files\n- Not checking for encryption\n\nFURTHER READING:\n- SQLite File Format documentation\n- \"Learning SQLite for Mobile Apps\"\n- Browser forensics guides",
      "tags": ["forensics", "sqlite", "database", "browser-forensics", "dfir"],
      "related_tools": ["bulk_extractor"]
    },
    {
      "id": "deleted_record_recovery",
      "title": "Deleted Record Recovery",
      "content": "OBJECTIVE: Recover deleted records from SQLite databases using freelist analysis, page examination, and carving techniques.\n\nACADEMIC BACKGROUND:\nWhen records are deleted in SQLite, the space is marked as free but the data often remains until overwritten. SQLite maintains freelists to track available space. Forensic analysis of these structures can recover deleted records that the application no longer displays.\n\nDELETED DATA RECOVERY:\n\n1. UNDERSTANDING DELETION IN SQLITE:\n   ```\n   What Happens When Data is Deleted:\n   \n   1. Record marked as deleted in page\n   2. Space added to freelist\n   3. Data remains until reused\n   4. VACUUM command truly removes data\n   \n   Freelist Structure:\n   - Freelist trunk pages: List of free pages\n   - Freelist leaf pages: Actual free pages\n   - Free block: Unused space within a page\n   \n   Recovery Opportunities:\n   | Scenario | Recovery Chance |\n   |----------|----------------|\n   | Recently deleted | High |\n   | Before VACUUM | High |\n   | After VACUUM | None |\n   | Auto-vacuum ON | Lower |\n   | Heavy DB activity | Lower |\n   ```\n\n2. FREELIST ANALYSIS:\n   ```bash\n   # Check freelist status\n   sqlite3 database.db \"PRAGMA freelist_count;\"\n   \n   # If freelist_count > 0, deleted data may exist\n   \n   # Check auto-vacuum mode\n   sqlite3 database.db \"PRAGMA auto_vacuum;\"\n   # 0 = None (best for recovery)\n   # 1 = Full auto-vacuum\n   # 2 = Incremental\n   \n   # Page statistics\n   sqlite3 database.db \"PRAGMA page_count;\"\n   sqlite3 database.db \"SELECT * FROM sqlite_master;\"\n   ```\n\n3. USING UNDARK TOOL:\n   ```bash\n   # undark - SQLite deleted record recovery\n   # https://github.com/inflex/undark\n   \n   # Basic usage\n   undark -i database.db > recovered_data.txt\n   \n   # Verbose output\n   undark -i database.db -v > recovered_data.txt\n   \n   # Output includes:\n   # - Table name (if determinable)\n   # - Recovered field values\n   # - Recovery confidence\n   ```\n\n4. USING SQLITERECOVER:\n   ```bash\n   # sqlite-recover (Python)\n   # pip install sqlite-recover\n   \n   # Recover all tables\n   python -m sqlite_recover database.db recovered.db\n   \n   # The tool:\n   # - Scans all pages including freelist\n   # - Recovers deleted records\n   # - Outputs to new database\n   ```\n\n5. MANUAL PAGE ANALYSIS:\n   ```bash\n   # Extract database pages for analysis\n   \n   # Get page size\n   PAGE_SIZE=$(sqlite3 database.db \"PRAGMA page_size;\")\n   \n   # Extract specific page (page 1 is at offset 0)\n   dd if=database.db of=page_5.bin bs=$PAGE_SIZE skip=4 count=1\n   \n   # View page in hex\n   xxd page_5.bin | head -50\n   \n   # Search for deleted data patterns\n   strings database.db | grep -i \"deleted_keyword\"\n   \n   # Search all pages for pattern\n   for i in $(seq 1 100); do\n       dd if=database.db bs=$PAGE_SIZE skip=$((i-1)) count=1 2>/dev/null | \\\n       strings | grep -l \"pattern\" && echo \"Found in page $i\"\n   done\n   ```\n\n6. B-TREE PAGE STRUCTURE:\n   ```\n   B-Tree Page Format:\n   \n   Page Header (8-12 bytes):\n   - Byte 0: Page type\n     - 0x02: Interior index b-tree page\n     - 0x05: Interior table b-tree page\n     - 0x0a: Leaf index b-tree page\n     - 0x0d: Leaf table b-tree page\n   - Bytes 1-2: First freeblock offset\n   - Bytes 3-4: Number of cells\n   - Bytes 5-6: Start of cell content area\n   - Byte 7: Fragmented free bytes\n   \n   Cell Content:\n   - Variable length integers (varints)\n   - Record format: header + data\n   - Header specifies column types\n   \n   Analyzing Page:\n   ```bash\n   # Check page type\n   xxd -l 1 page.bin\n   # 0x0d = leaf table page (contains records)\n   \n   # Check freeblock offset (bytes 1-2)\n   xxd -s 1 -l 2 page.bin\n   # Non-zero = deleted content exists\n   ```\n\n7. RECORD FORMAT PARSING:\n   ```python\n   #!/usr/bin/env python3\n   \"\"\"Simple SQLite record parser\"\"\"\n   \n   def parse_varint(data, offset):\n       \"\"\"Parse SQLite variable-length integer\"\"\"\n       result = 0\n       for i in range(9):\n           byte = data[offset + i]\n           if i < 8:\n               result = (result << 7) | (byte & 0x7f)\n               if byte < 0x80:\n                   return result, offset + i + 1\n           else:\n               result = (result << 8) | byte\n               return result, offset + i + 1\n       return result, offset + 9\n   \n   def analyze_record(data, offset):\n       \"\"\"Parse a SQLite record at given offset\"\"\"\n       # First varint is payload size\n       payload_size, offset = parse_varint(data, offset)\n       # Second varint is rowid\n       rowid, offset = parse_varint(data, offset)\n       # Then header size\n       header_size, new_offset = parse_varint(data, offset)\n       \n       print(f\"Payload: {payload_size}, RowID: {rowid}, Header: {header_size}\")\n       return offset + payload_size\n   \n   # Usage:\n   with open('page.bin', 'rb') as f:\n       data = f.read()\n   analyze_record(data, 8)  # Start after page header\n   ```\n\n8. FREEBLOCK CARVING:\n   ```bash\n   # Freeblocks contain deleted data\n   \n   # Manual freeblock identification\n   # Page header byte 1-2 = offset to first freeblock\n   # Freeblock first 2 bytes = next freeblock offset\n   # Freeblock bytes 3-4 = size of this freeblock\n   \n   # Extract freeblock content\n   FREEBLOCK_OFFSET=100  # Example offset from page header\n   FREEBLOCK_SIZE=50     # From freeblock header\n   \n   dd if=page.bin of=freeblock.bin bs=1 skip=$FREEBLOCK_OFFSET count=$FREEBLOCK_SIZE\n   strings freeblock.bin\n   ```\n\n9. COMPREHENSIVE RECOVERY WORKFLOW:\n   ```bash\n   #!/bin/bash\n   # SQLite deleted record recovery workflow\n   \n   DB=\"$1\"\n   OUTPUT=\"/forensics/sqlite_recovery\"\n   mkdir -p $OUTPUT\n   \n   echo \"[*] Database: $DB\"\n   echo \"[*] Output: $OUTPUT\"\n   \n   # Copy database\n   cp \"$DB\" \"$OUTPUT/working_copy.db\"\n   cp \"$DB-wal\" \"$OUTPUT/\" 2>/dev/null\n   cp \"$DB-journal\" \"$OUTPUT/\" 2>/dev/null\n   \n   # Get info\n   echo \"[*] Database info:\"\n   sqlite3 \"$OUTPUT/working_copy.db\" \"PRAGMA page_count;\"\n   sqlite3 \"$OUTPUT/working_copy.db\" \"PRAGMA freelist_count;\"\n   sqlite3 \"$OUTPUT/working_copy.db\" \"PRAGMA auto_vacuum;\"\n   \n   # Export current data\n   echo \"[*] Exporting current data...\"\n   for table in $(sqlite3 \"$OUTPUT/working_copy.db\" \".tables\"); do\n       sqlite3 -header -csv \"$OUTPUT/working_copy.db\" \"SELECT * FROM $table;\" > \"$OUTPUT/current_${table}.csv\"\n   done\n   \n   # Run undark\n   echo \"[*] Running undark...\"\n   undark -i \"$OUTPUT/working_copy.db\" > \"$OUTPUT/undark_recovery.txt\" 2>&1\n   \n   # String extraction\n   echo \"[*] Extracting strings...\"\n   strings \"$OUTPUT/working_copy.db\" > \"$OUTPUT/all_strings.txt\"\n   \n   echo \"[*] Done. Check $OUTPUT for results.\"\n   ```\n\nRECOVERY REPORT:\n```\nSQLite Deleted Record Recovery:\n\nDatabase: /evidence/Chrome/History\nPage Size: 4096 bytes\nTotal Pages: 1,247\nFreelist Pages: 23\nAuto-vacuum: Disabled (good for recovery)\n\nRecovery Results:\n\n| Table | Current Records | Recovered | Total |\n|-------|-----------------|-----------|-------|\n| urls | 1,234 | 156 | 1,390 |\n| visits | 4,567 | 423 | 4,990 |\n| downloads | 45 | 12 | 57 |\n\nRecovered URLs Sample:\n- https://sensitivesite.com/documents/... (deleted 2024-03-10)\n- https://pastebin.com/raw/... (deleted 2024-03-12)\n- https://mega.nz/file/... (deleted 2024-03-14)\n\nForensic Value: HIGH\n- Recovered URLs show access to file sharing\n- Deletion pattern suggests intentional cleanup\n- Timeline correlates with incident\n```\n\nDETECTION INDICATORS:\n- Freelist analyzed for deleted content\n- Multiple recovery tools applied\n- Page-level analysis performed\n- Recovered records documented\n\nCOMMON PITFALLS:\n- Database was VACUUMed (data truly gone)\n- Auto-vacuum enabled\n- Not recovering related WAL/journal\n- Corrupted freelist\n\nFURTHER READING:\n- SQLite Forensic Analysis papers\n- undark documentation\n- SQLite Database File Format",
      "tags": ["forensics", "sqlite", "deleted-records", "data-recovery", "dfir"],
      "related_tools": ["bulk_extractor", "foremost", "strings"]
    },
    {
      "id": "journal_wal_analysis",
      "title": "Journal and WAL Analysis",
      "content": "OBJECTIVE: Analyze SQLite journal files and Write-Ahead Logs (WAL) to recover transaction history and uncommitted data.\n\nACADEMIC BACKGROUND:\nSQLite uses journaling for crash recovery. The rollback journal (traditional) or Write-Ahead Log (WAL) contains transaction data that can reveal recent database operations, even those that were rolled back. These files are forensic goldmines for understanding recent activity.\n\nJOURNAL TYPES:\n\n1. ROLLBACK JOURNAL:\n   ```\n   Rollback Journal (.db-journal):\n   \n   Purpose:\n   - Stores original page content before modification\n   - Enables rollback on crash or error\n   - Created at transaction start\n   - Deleted on commit (usually)\n   \n   Journal Modes:\n   | Mode | Behavior |\n   |------|----------|\n   | DELETE | Journal deleted on commit (default) |\n   | TRUNCATE | Journal truncated to zero |\n   | PERSIST | Journal header zeroed |\n   | MEMORY | Journal in RAM only |\n   | WAL | Write-ahead logging |\n   | OFF | No journal (dangerous) |\n   \n   Check current mode:\n   sqlite3 database.db \"PRAGMA journal_mode;\"\n   ```\n\n2. WRITE-AHEAD LOG (WAL):\n   ```\n   WAL Mode Files:\n   - database.db-wal: The write-ahead log\n   - database.db-shm: Shared memory file (index)\n   \n   WAL Advantages:\n   - Better concurrency\n   - Readers don't block writers\n   - Faster for many workloads\n   \n   WAL Forensic Value:\n   - Contains recent transactions\n   - May have uncommitted data\n   - Survives application crashes\n   \n   Check if WAL mode:\n   sqlite3 database.db \"PRAGMA journal_mode;\"\n   # Returns \"wal\" if enabled\n   ```\n\n3. ANALYZING ROLLBACK JOURNAL:\n   ```bash\n   # Journal header structure (28 bytes):\n   # Bytes 0-7: Magic number\n   # Bytes 8-11: Page count in journal\n   # Bytes 12-15: Random nonce\n   # Bytes 16-19: Initial database page count\n   # Bytes 20-23: Sector size\n   # Bytes 24-27: Page size\n   \n   # Check journal magic\n   xxd -l 8 database.db-journal\n   # Valid: d9 d5 05 f9 20 a1 63 d7\n   \n   # Extract journal pages\n   PAGE_SIZE=4096\n   \n   # Each journal entry: 4-byte page number + page content + 4-byte checksum\n   # Skip 28-byte header\n   dd if=database.db-journal of=journal_page1.bin bs=1 skip=32 count=$PAGE_SIZE\n   \n   # Examine page content\n   strings journal_page1.bin\n   ```\n\n4. ANALYZING WAL FILE:\n   ```bash\n   # WAL Header (32 bytes):\n   # Bytes 0-3: Magic (0x377f0682 or 0x377f0683)\n   # Bytes 4-7: File format version\n   # Bytes 8-11: Database page size\n   # Bytes 12-15: Checkpoint sequence number\n   # Bytes 16-19: Salt-1\n   # Bytes 20-23: Salt-2\n   # Bytes 24-27: Checksum-1\n   # Bytes 28-31: Checksum-2\n   \n   # Check WAL magic\n   xxd -l 4 database.db-wal\n   # 377f0682 = big-endian\n   # 377f0683 = little-endian\n   \n   # WAL Frame Header (24 bytes per frame):\n   # Bytes 0-3: Page number\n   # Bytes 4-7: Commit size (or 0 if not commit frame)\n   # Bytes 8-11: Salt-1 copy\n   # Bytes 12-15: Salt-2 copy\n   # Bytes 16-19: Checksum-1\n   # Bytes 20-23: Checksum-2\n   \n   # Extract WAL frames\n   PAGE_SIZE=4096\n   FRAME_SIZE=$((24 + PAGE_SIZE))\n   \n   # First frame (after 32-byte header)\n   dd if=database.db-wal of=frame1.bin bs=1 skip=32 count=$FRAME_SIZE\n   \n   # Page content within frame (skip 24-byte frame header)\n   dd if=frame1.bin of=frame1_page.bin bs=1 skip=24\n   strings frame1_page.bin\n   ```\n\n5. WAL RECOVERY TOOLS:\n   ```bash\n   # walitean - WAL analysis tool\n   # https://github.com/pschmitt/walitean\n   \n   # Analyze WAL file\n   python walitean.py database.db-wal\n   \n   # Alternative: Use SQLite itself\n   # Force checkpoint to merge WAL into main database\n   # WARNING: This modifies the database - use on COPY only!\n   sqlite3 database_copy.db \"PRAGMA wal_checkpoint(TRUNCATE);\"\n   \n   # After checkpoint, WAL content merged into database\n   # Original WAL preserved in evidence\n   ```\n\n6. TIMELINE FROM JOURNAL:\n   ```bash\n   # WAL frames are chronologically ordered\n   # Frame sequence = transaction sequence\n   \n   # Script to extract all WAL frames\n   #!/bin/bash\n   WAL_FILE=\"$1\"\n   OUTPUT_DIR=\"$2\"\n   PAGE_SIZE=4096\n   FRAME_HEADER=24\n   WAL_HEADER=32\n   \n   mkdir -p \"$OUTPUT_DIR\"\n   \n   # Get WAL file size\n   WAL_SIZE=$(stat -c%s \"$WAL_FILE\")\n   \n   # Calculate frames\n   FRAME_SIZE=$((FRAME_HEADER + PAGE_SIZE))\n   NUM_FRAMES=$(( (WAL_SIZE - WAL_HEADER) / FRAME_SIZE ))\n   \n   echo \"WAL contains $NUM_FRAMES frames\"\n   \n   for i in $(seq 0 $((NUM_FRAMES - 1))); do\n       OFFSET=$((WAL_HEADER + (i * FRAME_SIZE)))\n       dd if=\"$WAL_FILE\" of=\"$OUTPUT_DIR/frame_$i.bin\" \\\n          bs=1 skip=$OFFSET count=$FRAME_SIZE 2>/dev/null\n       \n       # Extract page number from frame header\n       PAGE_NUM=$(xxd -s 0 -l 4 -p \"$OUTPUT_DIR/frame_$i.bin\")\n       echo \"Frame $i: Page $((16#$PAGE_NUM))\"\n   done\n   ```\n\n7. RECOVERING UNCOMMITTED TRANSACTIONS:\n   ```bash\n   # Uncommitted data in WAL:\n   # - Last frames without commit marker\n   # - Commit frame has non-zero bytes 4-7\n   \n   # Identify commit frames\n   for frame in output/frame_*.bin; do\n       COMMIT=$(xxd -s 4 -l 4 -p \"$frame\")\n       if [ \"$COMMIT\" != \"00000000\" ]; then\n           echo \"$frame is commit frame (database size: $((16#$COMMIT)) pages)\"\n       fi\n   done\n   \n   # Frames after last commit = uncommitted\n   # May contain:\n   # - Deleted but not committed\n   # - Inserted but crashed before commit\n   # - Modified data rolled back\n   ```\n\n8. BROWSER WAL ANALYSIS:\n   ```bash\n   # Chrome History WAL analysis\n   \n   CHROME_HISTORY=\"$HOME/.config/google-chrome/Default/History\"\n   CHROME_WAL=\"${CHROME_HISTORY}-wal\"\n   \n   # Copy for analysis\n   cp \"$CHROME_HISTORY\" /forensics/\n   cp \"$CHROME_WAL\" /forensics/ 2>/dev/null\n   cp \"${CHROME_HISTORY}-shm\" /forensics/ 2>/dev/null\n   \n   # Extract strings from WAL\n   strings /forensics/History-wal | grep -E \"^https?://\" | sort -u > /forensics/wal_urls.txt\n   \n   # Compare with committed history\n   sqlite3 /forensics/History \"SELECT url FROM urls;\" | sort -u > /forensics/db_urls.txt\n   \n   # URLs in WAL but not database = potentially deleted/uncommitted\n   comm -23 /forensics/wal_urls.txt /forensics/db_urls.txt > /forensics/wal_only_urls.txt\n   ```\n\n9. COMPREHENSIVE JOURNAL ANALYSIS:\n   ```bash\n   #!/bin/bash\n   # SQLite journal analysis script\n   \n   DB=\"$1\"\n   OUTPUT=\"/forensics/journal_analysis\"\n   mkdir -p \"$OUTPUT\"\n   \n   echo \"[*] Analyzing: $DB\"\n   \n   # Check journal mode\n   MODE=$(sqlite3 \"$DB\" \"PRAGMA journal_mode;\")\n   echo \"[*] Journal mode: $MODE\"\n   \n   # Copy all related files\n   cp \"$DB\" \"$OUTPUT/database.db\"\n   cp \"$DB-journal\" \"$OUTPUT/\" 2>/dev/null && echo \"[*] Found rollback journal\"\n   cp \"$DB-wal\" \"$OUTPUT/\" 2>/dev/null && echo \"[*] Found WAL file\"\n   cp \"$DB-shm\" \"$OUTPUT/\" 2>/dev/null && echo \"[*] Found SHM file\"\n   \n   # Analyze WAL if present\n   if [ -f \"$OUTPUT/database.db-wal\" ]; then\n       echo \"[*] Analyzing WAL...\"\n       \n       # WAL statistics\n       WAL_SIZE=$(stat -c%s \"$OUTPUT/database.db-wal\")\n       PAGE_SIZE=$(sqlite3 \"$OUTPUT/database.db\" \"PRAGMA page_size;\")\n       FRAME_SIZE=$((24 + PAGE_SIZE))\n       NUM_FRAMES=$(( (WAL_SIZE - 32) / FRAME_SIZE ))\n       \n       echo \"[*] WAL size: $WAL_SIZE bytes\"\n       echo \"[*] Frame count: $NUM_FRAMES\"\n       \n       # Extract strings\n       strings \"$OUTPUT/database.db-wal\" > \"$OUTPUT/wal_strings.txt\"\n       \n       # Checkpoint to working copy\n       cp \"$OUTPUT/database.db\" \"$OUTPUT/database_checkpointed.db\"\n       cp \"$OUTPUT/database.db-wal\" \"$OUTPUT/database_checkpointed.db-wal\"\n       sqlite3 \"$OUTPUT/database_checkpointed.db\" \"PRAGMA wal_checkpoint(PASSIVE);\"\n   fi\n   \n   echo \"[*] Analysis complete: $OUTPUT\"\n   ```\n\nANALYSIS REPORT:\n```\nSQLite Journal/WAL Analysis:\n\nDatabase: /evidence/Signal/signal.sqlite\nJournal Mode: WAL\n\nFiles Analyzed:\n- signal.sqlite (main database)\n- signal.sqlite-wal (WAL file, 2.3 MB)\n- signal.sqlite-shm (shared memory)\n\nWAL Statistics:\n- WAL Size: 2,359,296 bytes\n- Page Size: 4,096 bytes\n- Total Frames: 568\n- Commit Frames: 45\n- Uncommitted Frames: 12\n\nRecovery from WAL:\n\n| Content Type | Count | Status |\n|--------------|-------|--------|\n| Messages | 23 | Recovered from uncommitted |\n| Contacts | 5 | In WAL only |\n| Attachments | 3 | Reference recovered |\n\nNotable Findings:\n- 12 uncommitted frames contain message data\n- Messages appear to be from deletion-in-progress\n- Recovered content shows communication with [SUBJECT]\n- Timestamps indicate activity during incident window\n```\n\nDETECTION INDICATORS:\n- Journal mode identified\n- WAL frames extracted\n- Commit points identified\n- Uncommitted data recovered\n\nCOMMON PITFALLS:\n- Checkpointing evidence WAL (destroys data)\n- Missing -shm file for WAL analysis\n- Not preserving original journal files\n- Confusing journal modes\n\nFURTHER READING:\n- SQLite Write-Ahead Logging documentation\n- Journal file format specification\n- Database forensics papers",
      "tags": ["forensics", "sqlite", "wal", "journal", "transactions", "dfir"],
      "related_tools": ["strings", "xxd", "hexedit"]
    },
    {
      "id": "application_artifact_extraction",
      "title": "Application Artifact Extraction",
      "content": "OBJECTIVE: Extract and analyze SQLite artifacts from common applications including browsers, messaging apps, and mobile devices.\n\nACADEMIC BACKGROUND:\nApplications store user data, preferences, and activity logs in SQLite databases. Forensic analysis of these databases reveals user behavior, communication patterns, and potentially incriminating evidence. Understanding application-specific schemas is essential for comprehensive investigation.\n\nBROWSER FORENSICS:\n\n1. CHROME HISTORY ANALYSIS:\n   ```bash\n   # Chrome History database location\n   # Linux: ~/.config/google-chrome/Default/History\n   # Windows: %LOCALAPPDATA%\\Google\\Chrome\\User Data\\Default\\History\n   # macOS: ~/Library/Application Support/Google/Chrome/Default/History\n   \n   # Copy database (Chrome must be closed, or copy -wal too)\n   cp ~/.config/google-chrome/Default/History /forensics/chrome_history.db\n   \n   # Key tables:\n   # urls - Visited URLs\n   # visits - Visit timestamps\n   # downloads - Downloaded files\n   # segments - URL segments\n   # keyword_search_terms - Search queries\n   \n   # Extract browsing history\n   sqlite3 -header -csv /forensics/chrome_history.db \"\n   SELECT \n       urls.url,\n       urls.title,\n       urls.visit_count,\n       datetime(urls.last_visit_time/1000000-11644473600, 'unixepoch') as last_visit,\n       visits.transition\n   FROM urls\n   JOIN visits ON urls.id = visits.url\n   ORDER BY visits.visit_time DESC\n   LIMIT 100;\n   \" > /forensics/chrome_browsing.csv\n   \n   # Extract downloads\n   sqlite3 -header -csv /forensics/chrome_history.db \"\n   SELECT \n       target_path,\n       tab_url,\n       total_bytes,\n       datetime(start_time/1000000-11644473600, 'unixepoch') as start_time,\n       datetime(end_time/1000000-11644473600, 'unixepoch') as end_time\n   FROM downloads\n   ORDER BY start_time DESC;\n   \" > /forensics/chrome_downloads.csv\n   \n   # Extract search terms\n   sqlite3 -header -csv /forensics/chrome_history.db \"\n   SELECT \n       term,\n       url_id\n   FROM keyword_search_terms;\n   \" > /forensics/chrome_searches.csv\n   ```\n\n2. FIREFOX HISTORY ANALYSIS:\n   ```bash\n   # Firefox places.sqlite\n   # Contains history, bookmarks, downloads\n   \n   cp ~/.mozilla/firefox/*.default/places.sqlite /forensics/\n   \n   # Extract history\n   sqlite3 -header -csv /forensics/places.sqlite \"\n   SELECT \n       moz_places.url,\n       moz_places.title,\n       moz_places.visit_count,\n       datetime(moz_historyvisits.visit_date/1000000, 'unixepoch') as visit_date,\n       moz_historyvisits.visit_type\n   FROM moz_places\n   JOIN moz_historyvisits ON moz_places.id = moz_historyvisits.place_id\n   ORDER BY moz_historyvisits.visit_date DESC;\n   \"\n   \n   # Firefox cookies.sqlite\n   sqlite3 -header -csv /forensics/cookies.sqlite \"\n   SELECT \n       name,\n       value,\n       host,\n       path,\n       datetime(expiry, 'unixepoch') as expiry,\n       datetime(lastAccessed/1000000, 'unixepoch') as last_accessed\n   FROM moz_cookies;\n   \"\n   ```\n\n3. MESSAGING APP ANALYSIS:\n   ```bash\n   # Signal Desktop (Linux)\n   # ~/.config/Signal/sql/db.sqlite\n   \n   # Skype\n   # ~/.Skype/*/main.db\n   \n   # Example: Signal message extraction\n   sqlite3 -header -csv /forensics/signal.sqlite \"\n   SELECT \n       conversationId,\n       sent_at,\n       received_at,\n       body,\n       type\n   FROM messages\n   ORDER BY sent_at DESC;\n   \"\n   \n   # WhatsApp (Android)\n   # /data/data/com.whatsapp/databases/msgstore.db\n   \n   sqlite3 -header -csv /forensics/msgstore.db \"\n   SELECT \n       key_remote_jid as contact,\n       data as message,\n       datetime(timestamp/1000, 'unixepoch') as time,\n       status\n   FROM messages\n   WHERE data IS NOT NULL\n   ORDER BY timestamp DESC;\n   \"\n   ```\n\n4. MOBILE DEVICE DATABASES:\n   ```bash\n   # iOS SMS/iMessage\n   # /private/var/mobile/Library/SMS/sms.db\n   \n   sqlite3 -header -csv /forensics/sms.db \"\n   SELECT \n       message.rowid,\n       handle.id as contact,\n       message.text,\n       datetime(message.date/1000000000 + 978307200, 'unixepoch') as date,\n       message.is_from_me\n   FROM message\n   LEFT JOIN handle ON message.handle_id = handle.rowid\n   ORDER BY message.date DESC;\n   \"\n   \n   # iOS Call History\n   # /private/var/mobile/Library/CallHistoryDB/CallHistory.storedata\n   \n   sqlite3 -header -csv /forensics/CallHistory.storedata \"\n   SELECT \n       ZADDRESS as number,\n       ZDURATION as duration,\n       datetime(ZDATE + 978307200, 'unixepoch') as date,\n       ZCALLTYPE as type\n   FROM ZCALLRECORD\n   ORDER BY ZDATE DESC;\n   \"\n   \n   # Android Contacts\n   # /data/data/com.android.providers.contacts/databases/contacts2.db\n   \n   sqlite3 -header -csv /forensics/contacts2.db \"\n   SELECT \n       display_name,\n       data1 as phone_email\n   FROM raw_contacts\n   JOIN data ON raw_contacts._id = data.raw_contact_id\n   WHERE mimetype_id IN (5, 1);\n   \"\n   ```\n\n5. CLOUD SYNC DATABASES:\n   ```bash\n   # Dropbox\n   # ~/.dropbox/instance1/filecache.db\n   \n   sqlite3 -header -csv /forensics/filecache.db \"\n   SELECT \n       local_filename,\n       server_path,\n       size,\n       datetime(modified, 'unixepoch') as modified\n   FROM file_journal;\n   \"\n   \n   # OneDrive\n   # %LOCALAPPDATA%\\Microsoft\\OneDrive\\settings\\Business1\\\n   # Contains .dat files (SQLite format)\n   \n   sqlite3 -header -csv /forensics/onedrive.dat \"\n   SELECT * FROM sqlite_master;\n   \" # First discover schema\n   ```\n\n6. COMPREHENSIVE BROWSER ANALYSIS:\n   ```bash\n   #!/bin/bash\n   # Browser forensics script\n   \n   OUTPUT=\"/forensics/browser_analysis\"\n   mkdir -p \"$OUTPUT\"\n   \n   # Chrome\n   CHROME=\"$HOME/.config/google-chrome/Default\"\n   if [ -d \"$CHROME\" ]; then\n       echo \"[*] Analyzing Chrome...\"\n       mkdir -p \"$OUTPUT/chrome\"\n       \n       # Copy databases\n       cp \"$CHROME/History\" \"$OUTPUT/chrome/\" 2>/dev/null\n       cp \"$CHROME/History-journal\" \"$OUTPUT/chrome/\" 2>/dev/null\n       cp \"$CHROME/Cookies\" \"$OUTPUT/chrome/\" 2>/dev/null\n       cp \"$CHROME/Login Data\" \"$OUTPUT/chrome/\" 2>/dev/null\n       cp \"$CHROME/Web Data\" \"$OUTPUT/chrome/\" 2>/dev/null\n       \n       # Extract data\n       sqlite3 -header -csv \"$OUTPUT/chrome/History\" \"\n       SELECT url, title, visit_count,\n              datetime(last_visit_time/1000000-11644473600,'unixepoch') as last_visit\n       FROM urls ORDER BY last_visit_time DESC;\n       \" > \"$OUTPUT/chrome/urls.csv\"\n   fi\n   \n   # Firefox\n   FIREFOX=$(find ~/.mozilla/firefox -name \"places.sqlite\" 2>/dev/null | head -1)\n   if [ -n \"$FIREFOX\" ]; then\n       echo \"[*] Analyzing Firefox...\"\n       mkdir -p \"$OUTPUT/firefox\"\n       \n       PROFILE=$(dirname \"$FIREFOX\")\n       cp \"$PROFILE/places.sqlite\" \"$OUTPUT/firefox/\" 2>/dev/null\n       cp \"$PROFILE/cookies.sqlite\" \"$OUTPUT/firefox/\" 2>/dev/null\n       cp \"$PROFILE/formhistory.sqlite\" \"$OUTPUT/firefox/\" 2>/dev/null\n       \n       sqlite3 -header -csv \"$OUTPUT/firefox/places.sqlite\" \"\n       SELECT url, title, visit_count,\n              datetime(last_visit_date/1000000,'unixepoch') as last_visit\n       FROM moz_places ORDER BY last_visit_date DESC;\n       \" > \"$OUTPUT/firefox/urls.csv\"\n   fi\n   \n   echo \"[*] Browser analysis complete: $OUTPUT\"\n   ```\n\n7. ENCRYPTED DATABASE HANDLING:\n   ```bash\n   # Some apps encrypt SQLite databases\n   \n   # Signal Desktop - encrypted with SQLCipher\n   # Key stored in config.json\n   \n   # Check if encrypted\n   file database.db\n   # \"data\" instead of \"SQLite\" = likely encrypted\n   \n   # Using sqlcipher (if key known)\n   sqlcipher database.db\n   > PRAGMA key = 'encryption_key';\n   > SELECT * FROM messages;\n   \n   # WhatsApp encryption (Android)\n   # Key file: /data/data/com.whatsapp/files/key\n   # Use specialized tools: wa-crypt-tools\n   ```\n\n8. AUTOMATED ARTIFACT EXTRACTION:\n   ```python\n   #!/usr/bin/env python3\n   \"\"\"SQLite artifact extractor\"\"\"\n   \n   import sqlite3\n   import os\n   import csv\n   from datetime import datetime\n   \n   def chrome_timestamp(webkit_ts):\n       \"\"\"Convert Chrome/WebKit timestamp to datetime\"\"\"\n       if webkit_ts:\n           return datetime.fromtimestamp((webkit_ts / 1000000) - 11644473600)\n       return None\n   \n   def extract_chrome_history(db_path, output_dir):\n       \"\"\"Extract Chrome browsing history\"\"\"\n       conn = sqlite3.connect(db_path)\n       cursor = conn.cursor()\n       \n       cursor.execute(\"\"\"\n           SELECT urls.url, urls.title, urls.visit_count,\n                  urls.last_visit_time, visits.visit_time\n           FROM urls\n           JOIN visits ON urls.id = visits.url\n           ORDER BY visits.visit_time DESC\n       \"\"\")\n       \n       with open(os.path.join(output_dir, 'chrome_history.csv'), 'w', newline='') as f:\n           writer = csv.writer(f)\n           writer.writerow(['URL', 'Title', 'Visit Count', 'Last Visit', 'Visit Time'])\n           for row in cursor.fetchall():\n               writer.writerow([\n                   row[0], row[1], row[2],\n                   chrome_timestamp(row[3]),\n                   chrome_timestamp(row[4])\n               ])\n       \n       conn.close()\n   \n   if __name__ == '__main__':\n       import sys\n       extract_chrome_history(sys.argv[1], sys.argv[2])\n   ```\n\nARTIFACT REPORT:\n```\nApplication Artifact Extraction:\n\nAnalyzed Applications:\n- Google Chrome (Linux)\n- Mozilla Firefox (Linux)\n- Signal Desktop\n\nBrowser Activity Summary:\n\n| Browser | URLs | Downloads | Searches | Date Range |\n|---------|------|-----------|----------|------------|\n| Chrome | 12,456 | 234 | 567 | 2024-01-01 to 2024-03-15 |\n| Firefox | 3,421 | 45 | 123 | 2024-02-01 to 2024-03-15 |\n\nNotable Findings:\n\n1. Chrome - Suspicious Downloads:\n   - /tmp/payload.exe (deleted from disk)\n   - Downloaded from: https://malicious-site.com/...\n   - Timestamp: 2024-03-14 09:45:32\n\n2. Chrome - Search History:\n   - \"how to delete browser history\" (2024-03-15 08:00)\n   - \"evidence removal tools\" (2024-03-15 08:15)\n\n3. Signal - Recovered Messages:\n   - 156 messages recovered from WAL\n   - Communications with [SUBJECT] on dates of interest\n\n4. Deleted Records Recovered:\n   - 234 URLs from Chrome freelist\n   - 23 messages from Signal WAL\n```\n\nDETECTION INDICATORS:\n- All relevant databases identified\n- Timestamps properly converted\n- Deleted records recovered\n- Cross-application correlation\n\nCOMMON PITFALLS:\n- Wrong timestamp epoch\n- Missing encrypted databases\n- Not analyzing WAL files\n- Incomplete schema understanding\n\nFURTHER READING:\n- Browser Forensics guides\n- Mobile device forensics documentation\n- Application-specific schema references",
      "tags": ["forensics", "sqlite", "browser-forensics", "mobile-forensics", "artifacts", "dfir"],
      "related_tools": ["bulk_extractor", "exiftool", "strings"]
    }
  ]
}
