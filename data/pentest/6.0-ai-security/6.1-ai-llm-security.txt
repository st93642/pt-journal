# Domain 6.0 - AI & LLM Security
# Subdomain 6.1 - Prompt Injection & Jailbreaks
# Format: question|answer_a|answer_b|answer_c|answer_d|correct_idx|explanation|domain|subdomain

What is prompt injection in the context of Large Language Models?|A technique to improve model training data|A method to override or manipulate AI system instructions through crafted user inputs|A way to inject malicious code into AI models|A process for updating model parameters|1|Prompt injection involves crafting inputs that override the system's intended behavior or safety instructions, allowing attackers to manipulate AI responses.|6.0 AI & LLM Security|6.1 Prompt Injection & Jailbreaks

Which of the following is a common jailbreaking technique used against AI models?|Using encrypted communication channels|Creating alternative personas like \"DAN\" (Do Anything Now) that ignore safety rules|Implementing differential privacy|Conducting membership inference attacks|1|Jailbreaking often involves creating uncensored personas or developer modes that coerce the AI into breaking its safety restrictions.|6.0 AI & LLM Security|6.1 Prompt Injection & Jailbreaks

What is the primary defense against prompt injection attacks?|Increasing model size|Implementing input sanitization and system prompt hardening|Using only proprietary models|Disabling all user inputs|1|Input sanitization, clear system prompts, and output filtering are key defenses against prompt injection attacks.|6.0 AI & LLM Security|6.1 Prompt Injection & Jailbreaks

Which type of attack attempts to determine if specific data was used in training an AI model?|Model inversion attack|Membership inference attack|Adversarial example attack|Data poisoning attack|1|Membership inference attacks try to determine whether particular data samples were part of the model's training dataset.|6.0 AI & LLM Security|6.2 LLM Data Exfiltration

What is model inversion in the context of AI security?|Reversing the model's decision-making process|Reconstructing training data from model outputs|Flipping model predictions|Encrypting model weights|1|Model inversion attacks attempt to reconstruct or infer the original training data from the model's outputs and behavior.|6.0 AI & LLM Security|6.2 LLM Data Exfiltration

Which defense technique helps protect against data exfiltration from AI models?|Adversarial training|Differential privacy during training|Input validation only|Model compression|1|Differential privacy adds noise to the training process, making it harder to extract specific training data samples.|6.0 AI & LLM Security|6.2 LLM Data Exfiltration

What is data poisoning in machine learning pipelines?|Encrypting training data|Introducing malicious or corrupted data into the training set|Compressing model files|Monitoring model performance|1|Data poisoning involves manipulating training data to cause the model to learn incorrect patterns or include backdoors.|6.0 AI & LLM Security|6.3 ML Pipeline Threats

Which attack generates small perturbations to input data that cause misclassification?|Data poisoning|Evasion attack via adversarial examples|Model inversion|Membership inference|1|Adversarial examples are carefully crafted inputs that look normal to humans but cause AI models to make incorrect predictions.|6.0 AI & LLM Security|6.3 ML Pipeline Threats

What is the Fast Gradient Sign Method (FGSM) used for?|Training neural networks|Generating adversarial examples|Optimizing model performance|Compressing model weights|1|FGSM is a common technique for generating adversarial examples by computing gradients and adding small perturbations to inputs.|6.0 AI & LLM Security|6.3 ML Pipeline Threats

Which ML pipeline stage is most vulnerable to supply chain attacks?|Data preprocessing|Model training|Third-party model deployment|Performance monitoring|2|Deployment and integration of pre-trained models from third parties can introduce supply chain vulnerabilities.|6.0 AI & LLM Security|6.3 ML Pipeline Threats

What does the term \"concept drift\" refer to in ML security?|Model weights changing during training|The model's performance degrading over time due to changing data patterns|Data being encrypted during transmission|Models being deployed to multiple environments|1|Concept drift occurs when the statistical properties of the target variable change over time, affecting model performance.|6.0 AI & LLM Security|6.3 ML Pipeline Threats

Which tool is commonly used for testing AI model safety and prompt injection vulnerabilities?|Nmap|Metasploit|Garak|Burp Suite|2|Garak is an AI safety testing framework specifically designed for testing prompt injection and other AI security vulnerabilities.|6.0 AI & LLM Security|6.4 AI Security Tools

What is the purpose of adversarial training in ML security?|To make models smaller|To improve model accuracy on clean data|To increase robustness against adversarial examples|To speed up training|2|Adversarial training involves including adversarial examples in the training data to make models more robust to such attacks.|6.0 AI & LLM Security|6.4 AI Security Tools

Which of the following is NOT a common AI security threat?|SQL injection|Prompt injection|Model evasion|Data exfiltration|0|SQL injection is a traditional web application vulnerability, not specific to AI systems.|6.0 AI & LLM Security|6.4 AI Security Tools

What is the OWASP AI Security and Privacy Guide primarily concerned with?|Hardware security|AI system vulnerabilities and privacy protection|Network security|Operating system hardening|1|The OWASP AI Security and Privacy Guide provides comprehensive guidance on securing AI systems and protecting privacy.|6.0 AI & LLM Security|6.4 AI Security Tools

Which regulatory framework specifically addresses AI system risks and requires security assessments?|GDPR|PCI DSS|EU AI Act|SOX|2|The EU AI Act classifies AI systems by risk level and requires security assessments and mitigations for high-risk systems.|6.0 AI & LLM Security|6.4 AI Security Tools

What is a \"backdoor\" attack in machine learning?|A way to access the model's source code|Hidden triggers that cause incorrect model behavior|Encrypting model communications|A method to speed up inference|1|Backdoor attacks involve training models with poisoned data that includes hidden triggers causing specific misbehavior.|6.0 AI & LLM Security|6.4 AI Security Tools

Which defense technique involves adding random noise to training data to prevent data extraction?|Federated learning|Differential privacy|Model watermarking|Adversarial training|1|Differential privacy adds controlled noise to training data, making it difficult to extract specific information about individual data points.|6.0 AI & LLM Security|6.4 AI Security Tools

What is the primary goal of model stealing attacks?|To physically destroy hardware|To create a copy of the model's functionality|To slow down model inference|To increase training time|1|Model stealing involves querying a model extensively to understand its behavior and create a surrogate model with similar functionality.|6.0 AI & LLM Security|6.4 AI Security Tools

Which AI security testing framework provides systematic evaluation of model vulnerabilities?|CleverHans|Adversarial Robustness Toolbox (ART)|Both A and B|Neither A nor B|2|Both CleverHans and ART are frameworks for testing adversarial examples and other AI security vulnerabilities.|6.0 AI & LLM Security|6.4 AI Security Tools

What does \"indirect prompt injection\" refer to?|Directly modifying system prompts|Malicious content in data that the AI processes as part of its context|Using encrypted prompts|Physical access to the model|1|Indirect prompt injection occurs when malicious content is embedded in data sources that the AI system processes, affecting its behavior without direct user interaction.|6.0 AI & LLM Security|6.4 AI Security Tools