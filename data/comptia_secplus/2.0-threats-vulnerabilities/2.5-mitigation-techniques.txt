# Domain 2.0 - Threats, Vulnerabilities, and Mitigations
# Subdomain 2.5 - Mitigation Techniques
# Format: question|answer_a|answer_b|answer_c|answer_d|correct_idx|explanation|domain|subdomain

What mitigation technique divides networks into isolated segments to limit attack spread?|Encryption|Segmentation|Authentication|Monitoring|1|Segmentation (network segmentation) divides networks into isolated segments or zones to limit lateral movement and contain potential breaches. If one segment is compromised, others remain protected. Encryption protects data confidentiality, authentication verifies identity, and monitoring detects threats.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Which mitigation technique ensures users have only the minimum necessary access?|Defense in depth|Least privilege|Separation of duties|Job rotation|1|Least privilege principle ensures users are granted only the minimum access rights necessary to perform their job functions, reducing potential damage from compromised accounts. Defense in depth uses layers, separation of duties divides responsibilities, and job rotation moves people between roles.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

What is the primary purpose of patching systems regularly?|Improve performance|Fix known vulnerabilities|Add new features|Reduce costs|1|Patching systems regularly fixes known vulnerabilities and security flaws before they can be exploited. While patches may include performance improvements or features, security is the primary purpose from a threat mitigation perspective.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Which technique uses multiple layers of security controls for defense?|Single sign-on|Defense in depth|Least privilege|Encryption|1|Defense in depth (layered security) implements multiple layers of security controls so that if one fails, others still provide protection. This redundancy prevents single points of failure. SSO simplifies authentication, least privilege limits access, and encryption protects data.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

What mitigation approach allows only approved applications to run?|Blocklist|Application allow list|Antivirus|Firewall|1|Application allow listing (whitelisting) allows only approved, known-good applications to execute, blocking everything else by default. This is more secure than blocklisting which tries to block known-bad applications. Antivirus detects malware, and firewalls control network traffic.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Which technique removes systems from production that are no longer needed?|Patching|Hardening|Decommissioning|Monitoring|2|Decommissioning is the secure removal of systems, applications, or services from production when they're no longer needed. This reduces attack surface by eliminating unnecessary assets. Patching fixes vulnerabilities, hardening strengthens security, and monitoring detects threats.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

What hardening technique disables unnecessary services and features?|Installing all available features|Reducing attack surface|Enabling guest accounts|Opening all ports|1|Reducing attack surface by disabling unnecessary services, features, ports, and accounts is a key hardening technique that minimizes potential entry points for attackers. Installing extra features, enabling guest accounts, and opening ports increase attack surface.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Which mitigation uses cryptography to protect data confidentiality?|Hashing|Encryption|Segmentation|Monitoring|1|Encryption uses cryptographic algorithms to protect data confidentiality by making it unreadable without the proper decryption key. Hashing creates fingerprints for integrity, segmentation isolates networks, and monitoring detects threats.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

What technique isolates untrusted or suspicious systems from the main network?|Integration|Isolation|Correlation|Federation|1|Isolation separates untrusted, suspicious, or compromised systems from the main network to prevent spread of malware or lateral movement. Integration connects systems, correlation analyzes relationships, and federation connects identity systems.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Which technique continuously watches systems and networks for security events?|Patching|Encryption|Monitoring|Segmentation|2|Monitoring continuously observes systems, networks, and user activities for security events, anomalies, or indicators of compromise. This enables early threat detection and response. Patching fixes vulnerabilities, encryption protects data, and segmentation isolates networks.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

What mitigation technique standardizes and enforces security settings?|Configuration enforcement|Random configuration|Varied settings|Ad-hoc management|0|Configuration enforcement (configuration management) standardizes and enforces security settings across systems through baselines, templates, and automated tools. This ensures consistent security posture and prevents configuration drift. Random, varied, or ad-hoc approaches reduce security.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Which access control method grants permissions based on job roles?|Mandatory Access Control (MAC)|Role-Based Access Control (RBAC)|Discretionary Access Control (DAC)|Rule-Based Access Control|1|Role-Based Access Control (RBAC) assigns permissions based on job roles, simplifying administration and ensuring users have appropriate access for their positions. MAC enforces classifications, DAC allows owners to control access, and rule-based uses conditions.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

What zero trust principle requires continuous verification regardless of location?|Trust but verify|Never trust, always verify|Verify then trust indefinitely|Trust internal users only|1|Zero trust operates on "never trust, always verify" - no entity inside or outside the network is trusted by default. Every access request requires authentication, authorization, and continuous validation of security posture, regardless of location. This eliminates implicit trust based on network location. Traditional perimeter security trusts internal users, but zero trust requires continuous verification for everyone. Implemented through microsegmentation, identity verification, device health checks, conditional access policies, and least privilege access. Google's BeyondCorp and NIST SP 800-207 are zero trust frameworks. This approach prevents lateral movement after initial breach since every resource access requires re-authentication.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Microsegmentation applies security controls at what level?|Network perimeter only|Individual workload or application|Data center only|User level only|1|Microsegmentation applies security policies at granular levels - individual workloads, applications, or even processes - rather than broad network segments. This creates fine-grained security zones that limit lateral movement even within the same physical network. Each workload gets its own security policy defining allowed communications. Particularly effective in cloud environments where workloads move dynamically. Implementation uses software-defined networking (SDN), host-based firewalls, or cloud security groups. Unlike traditional VLANs that segment at network layer, microsegmentation works at application layer. If one application is compromised, attackers cannot easily pivot to other applications on the same server. Commonly used in zero trust architectures, container security (Kubernetes network policies), and cloud security (AWS Security Groups, Azure NSGs). Cisco ACI, VMware NSX provide microsegmentation capabilities.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Application control policies that allow only digitally signed executables implement what?|Blocklist|Signature-based allowlist|Heuristic analysis|Behavior monitoring|1|Signature-based allowlisting allows only executables with valid digital signatures from trusted publishers, ensuring code authenticity and preventing unsigned or self-signed malware from executing. Digital signatures use PKI to verify publisher identity and code integrity. Windows Defender Application Control (WDAC), AppLocker with publisher rules, and macOS Gatekeeper use this approach. More scalable than hash-based allowlisting since new versions from trusted publishers automatically work. Attackers must compromise legitimate publishers' signing keys (difficult) or find signed but vulnerable legitimate software for "living off the land" attacks. Organizations maintain trusted publisher list and require code signing for internal applications. Combines security with operational flexibility. Certificate revocation lists (CRLs) and OCSP handle compromised signing certificates. Supplemented with reputation services (Microsoft Intelligent Security Graph) for additional trust scoring.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Input validation that rejects all characters except explicitly allowed ones implements what principle?|Blocklist|Allowlist (positive security model)|Reactive filtering|Permissive validation|1|Allowlist input validation (positive security model) explicitly defines permitted characters, patterns, or values and rejects everything else. This is more secure than blocklisting which attempts to identify all malicious inputs (impossible). For example, if expecting phone number, allow only digits and dashes rather than trying to block SQL injection characters. Prevents zero-day injection attacks since even unknown attack patterns are rejected if not in allowed set. Used in: web application input validation (OWASP guidelines recommend allowlisting), SQL parameterized queries, XML schema validation, email address validation with regex patterns. Examples include: zip code field allowing only 5 digits, name field allowing only alphanumeric and spaces, date picker limiting to valid date ranges. More restrictive than blocklisting but provides stronger security. Requires careful design to avoid breaking legitimate use cases. Common implementation: regex patterns, type checking, range validation, length limits.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Safe configuration practices recommend changing what on all new devices?|IP addresses|Default credentials|MAC addresses|Serial numbers|1|Changing default credentials (default usernames/passwords like admin/admin, root/password) on all new devices is critical since attackers use automated scanners searching for systems with factory defaults. Manufacturer default credentials are publicly documented, posted on websites, and used in botnet attacks like Mirai. The 2016 Mirai botnet compromised 600K IoT devices using 60 default credential combinations. Attackers use tools like Shodan to find internet-exposed devices then try default credentials. Best practices: force password change on first login, prohibit common passwords, require complex passwords meeting policy, document all credential changes, use unique credentials per device (not same password everywhere), implement account lockout after failed attempts, monitor for default credential usage attempts, include credential changes in deployment checklists. Applies to: routers, switches, firewalls, IP cameras, IoT devices, databases, applications, management interfaces.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Port security that limits MAC addresses per switch port mitigates what attack?|SQL injection|MAC flooding (CAM table overflow)|DNS poisoning|XSS|1|Port security with MAC address limiting prevents MAC flooding attacks where attackers overflow the switch's CAM (Content Addressable Memory) table with fake MAC addresses, forcing the switch into hub mode (broadcasting all traffic) allowing eavesdropping. By limiting each port to specific MAC addresses (static assignment or learned maximum), switches reject frames from unauthorized MACs. Configuration examples: limit to 1-2 MACs per user port, statically assign server MAC addresses, use sticky MAC learning (remembers first MACs learned). Violation actions include: protect (silently drop unauthorized frames), restrict (drop and log), shutdown (disable port requiring admin re-enable). Also prevents unauthorized device connections and MAC spoofing. Combined with 802.1X (port-based NAC) for authentication. Cisco: "switchport port-security maximum 2", "switchport port-security violation restrict". Critical for preventing ARP spoofing, man-in-the-middle, and network reconnaissance. Modern switches support per-VLAN MAC limits for finer control.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Disabling unnecessary network protocols and services implements what hardening principle?|Increasing complexity|Reducing attack surface|Expanding functionality|Enabling all features|1|Reducing attack surface by disabling unnecessary protocols, services, and features minimizes potential vulnerabilities and attack vectors. Every running service is potential entry point - unused services provide no value but inherit their vulnerabilities. Examples: disable Telnet (use SSH instead), disable FTP (use SFTP/FTPS), disable SMBv1 (WannaCry exploited this), disable LLMNR/NetBIOS (used in responder attacks), disable IPv6 if not used, disable HTTP (use HTTPS only), remove unnecessary network stacks. Operating system hardening: disable guest accounts, remove unnecessary software, disable autorun, disable SSID broadcast. Server hardening: run only required services, disable web server directory listing, remove default websites/applications, disable unused database features. CIS Benchmarks and DISA STIGs provide detailed hardening guidance. Principle of least functionality from NIST. Regular security audits identify unnecessary services. Netstat, service management tools help inventory running services. Balance security with operational requirements.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Group Policy Objects (GPO) in Windows Active Directory enable what mitigation?|Individual manual configuration|Centralized configuration management|Decentralized settings|Random security policies|1|Group Policy Objects (GPO) enable centralized security configuration management across Windows domains, ensuring consistent security settings on thousands of systems simultaneously. GPOs enforce: password policies (complexity, length, expiration), account lockout thresholds, software restrictions (AppLocker rules), audit policies, user rights assignments, firewall rules, security options, and desktop restrictions. Benefits: consistent security posture, prevents configuration drift, reduces administrator workload, enables rapid security response (push emergency updates), enforces compliance requirements, provides inheritance and filtering. Security-relevant GPO settings: disable LM hash storage, enable SMB signing, restrict NTLM authentication, configure Windows Defender, enable BitLocker, deploy security baselines. GPO precedence: Local → Site → Domain → OU (LSDOU). Security filtering applies GPOs to specific security groups. WMI filters enable conditional application. GPO reporting (Group Policy Results) validates effective settings. Regular GPO audits identify conflicts or weak settings. Combined with Microsoft Security Compliance Toolkit (SCT) baselines.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Separation of duties requires dividing critical tasks between multiple people to prevent what?|Collaboration|Fraud or insider abuse|Efficiency|Communication|1|Separation of duties (segregation of duties, SoD) divides critical tasks among multiple people so no single person can complete a sensitive transaction alone, preventing fraud, insider abuse, and errors. One person initiates, another approves, third reviews. Examples: check writing requires both signer and approver, code deployments require developer and separate approver, database changes require DBA and security approval, financial transactions need initiator and authorizer, privileged access requires request and approval. IT examples: developers shouldn't have production access, security admins shouldn't audit themselves, backup operators shouldn't restore without approval. Implements "four eyes principle" and "maker-checker" controls. Sarbanes-Oxley (SOX) mandates SoD for financial controls. Conflicts of interest must be avoided: don't assign security AND auditing to same person. Identity governance tools detect SoD violations. Challenge: small organizations with limited staff may struggle; compensating controls (logging, management review) help. Related to least privilege (limits what each person can do).|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Mandatory vacation policies help detect what security issue?|System failures|Insider fraud requiring continuous access|Network outages|Hardware problems|1|Mandatory vacation (forced leave) policies require employees, especially those with privileged access or financial responsibilities, to take consecutive time off, during which others perform their duties. This detects insider fraud or embezzlement requiring continuous access to hide (someone else discovers irregularities). Fraudsters can't maintain their schemes from afar. Famous example: Nick Leeson's £827M fraud at Barings Bank might have been detected with mandatory vacation. IT security benefits: detects rogue administrators hiding malicious activities, identifies undocumented processes (knowledge not shared), reveals toxic key person dependencies, tests backup procedures and cross-training, ensures accounts aren't being used 365 days/year (impossible if truly on vacation). Best practices: require 5-10 consecutive business days, rotate responsibilities during vacation, monitor for remote access during leave, review accounts and transactions during absence. Combined with job rotation (periodically moving people between roles) provides similar benefits. Financial sector regulations often mandate vacation policies. Creates resilience while detecting fraud.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Change management requiring approval before system changes mitigates what risk?|Too few changes|Unauthorized or dangerous changes|Excessive communication|Reduced innovation|1|Change management processes require documentation, testing, and approval before implementing system changes, mitigating risks of unauthorized modifications, system instability, security vulnerabilities, and operational disruptions. Changes without approval can introduce vulnerabilities, break integrations, cause outages, or bypass security controls. Change Advisory Board (CAB) reviews proposals for: business justification, risk assessment, impact analysis, testing evidence, rollback plans, scheduling during maintenance windows. ITIL change management framework categorizes: standard changes (pre-approved, low risk), normal changes (require CAB review), emergency changes (expedited approval for urgent security issues). Documentation includes: what changed, why, who authorized, when implemented, how to rollback. Benefits: prevents unauthorized changes, maintains audit trails, coordinates between teams, schedules changes to minimize impact, enables rollback if problems arise, meets compliance requirements (SOX, PCI-DSS mandate change controls). Configuration management databases (CMDB) track authorized configurations. Version control systems (Git) manage infrastructure-as-code changes. Balances security and agility - DevSecOps adapts change management for rapid deployment.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Vulnerability scanning on a regular schedule (weekly/monthly) enables what mitigation activity?|Ignoring findings|Identifying and prioritizing patching|Disabling security tools|Reducing visibility|1|Regular vulnerability scanning (weekly for critical systems, monthly for others) systematically identifies security weaknesses, misconfigurations, and missing patches, enabling risk-based prioritization of remediation efforts. Scanners like Nessus, Qualys, Rapid7, and OpenVAS detect: missing OS/application patches, weak passwords, default credentials, SSL/TLS vulnerabilities, open ports, insecure configurations, compliance violations (PCI-DSS, HIPAA). Authenticated scans (with credentials) find more issues than unauthenticated external scans. Vulnerability databases like CVE and CVSS scores help prioritization - fix CVSS 9-10 critical issues immediately, CVSS 7-8.9 high severity quickly, lower severity per risk tolerance. Continuous scanning provides real-time visibility as new vulnerabilities emerge. Integration with patch management ensures findings drive remediation. Challenges include: false positives requiring validation, scan impact on production (schedule during low-usage), patch testing before deployment, tracking remediation with ticketing systems. Trend analysis shows improving or worsening security posture over time. Penetration testing validates scanner findings with actual exploitation.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Sandboxing untrusted executables before allowing them on the network mitigates what?|Known malware only|Zero-day and unknown malware|Physical attacks|Social engineering|1|Sandboxing executes suspicious files in isolated virtual environments to observe behavior before allowing them on the production network, detecting zero-day and unknown malware that signature-based antivirus misses. Sandboxes like FireEye, Palo Alto WildFire, and Cuckoo Sandbox monitor: network connections (C2 beaconing), file system modifications (creating malicious files), registry changes (persistence mechanisms), process creation (spawning cmd.exe), API calls (suspicious sequences), memory manipulation. Advanced evasion-aware sandboxes counter malware that detects VMs (checking for VMware tools, limited RAM, specific MAC addresses). Detonation time varies - quick scans analyze 30-60 seconds, deep analysis runs 5-10 minutes. Use cases: email attachments before delivery, downloaded executables, USB device files, zero-day protection for web traffic. Limitations: sophisticated malware may delay execution (time bombs), require specific triggers (open specific file), or detect sandboxes. Combined with threat intelligence - if sandbox identifies new malware, signatures distributed network-wide. Increasingly important as attackers use polymorphic and metamorphic techniques defeating static analysis.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing allow lists for application execution combined with code signing creates what security model?|Open access|Application control (default-deny)|Unrestricted execution|Reactive blocking|1|Application control with allowlisting plus code signing creates a default-deny security model where only explicitly approved and cryptographically verified applications execute, blocking all others including zero-day malware. Combines allowlist approaches: digital signature verification (trusted publishers), path-based rules (only from Program Files), hash-based rules (specific file fingerprints), certificate-based rules (specific signing certs). Windows Defender Application Control (WDAC), AppLocker, and Linux AppArmor implement this. Blocks: unsigned executables, malware, unauthorized software, shadow IT applications, potentially unwanted programs (PUPs). Benefits: prevents ransomware/malware execution (even if downloaded), stops script-based attacks (PowerShell restrictions), enforces software licensing, reduces support costs (users can't install problematic software). Challenges: initial deployment effort creating rules, managing exceptions for legitimate new software, user frustration with restrictions, performance overhead. Best practices: start in audit mode, use managed installer rules (trust software deployment tools), implement gradually (critical servers first), maintain exception process. Industries with strict compliance (finance, government, critical infrastructure) increasingly mandate application control.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Security orchestration, automation, and response (SOAR) platforms automate what?|Manual processes only|Incident response and threat remediation|Physical security|Access requests|1|SOAR platforms automate incident response workflows and threat remediation, orchestrating actions across multiple security tools to respond faster than manual processes. Automated playbooks handle common scenarios: isolate infected endpoints, block malicious IPs at firewall, disable compromised accounts, quarantine malicious emails, submit suspicious files to sandboxes, enrich indicators with threat intelligence. Human analysts trigger playbooks or SOAR auto-responds to high-confidence detections. Examples: phishing playbook automatically retrieves emails, analyzes URLs, checks against threat intel, quarantines mailboxes, blocks sender domain, notifies users - all in minutes vs. hours manually. Benefits: consistent response (no human error/oversight), rapid response (seconds/minutes vs. hours), scales security team (handle more incidents), reduces analyst burnout, captures institutional knowledge in playbooks. SOAR platforms (Palo Alto Cortex XSOAR, Splunk Phantom, IBM Resilient) integrate with: SIEM, EDR, firewalls, IAM, ticketing systems, threat intelligence feeds. Low/medium severity incidents fully automated; high severity involves human approval. Requires: well-defined processes, tool API integrations, playbook maintenance, metrics to measure effectiveness. Reduces mean time to respond (MTTR) from hours to minutes.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Jump servers (bastion hosts) for privileged access provide what security benefit?|Direct access|Single point of control and monitoring|Multiple entry points|Unrestricted access|1|Jump servers (bastion hosts) act as hardened intermediaries for privileged administrative access, providing a single controlled point for accessing critical systems. Administrators connect to jump server first, then to target systems, enabling centralized: session recording (audit what admins did), access control (who can reach what), MFA enforcement, privilege elevation (just-in-time access), monitoring and alerting. Benefits: all administrative traffic flows through monitored chokepoint, compromised admin workstation doesn't directly expose production, credentials never stored on user devices (stay on jump server), implements least privilege with time-limited access. Architecture: jump server in DMZ or management VLAN, restricted network rules (only jump server can SSH/RDP to production), hardened OS (minimal software, strict configuration), session recording solutions (BeyondTrust, CyberArk). Modern alternatives: privileged access management (PAM) solutions, cloud bastion services (AWS Systems Manager Session Manager), zero-trust network access (ZTNA). Best practices: no internet access from jump server, MFA required, all sessions recorded, regular security audits, separate jump servers for different environments (prod/dev), rotate credentials frequently. Compliance requirements (PCI-DSS, HIPAA) often mandate controlled administrative access.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing HTTP Strict Transport Security (HSTS) prevents what attack?|SQL injection|SSL stripping (downgrade to HTTP)|XSS|CSRF|1|HTTP Strict Transport Security (HSTS) prevents SSL stripping attacks where attackers force connections to downgrade from HTTPS to unencrypted HTTP, exposing sensitive data. HSTS header instructs browsers to always use HTTPS for specified domain, refusing HTTP connections. Header format: "Strict-Transport-Security: max-age=31536000; includeSubDomains; preload". How SSL stripping works: attacker performing MitM intercepts initial HTTP request (before HTTPS redirect), establishes HTTP with victim and HTTPS with server, sees all traffic. HSTS prevents this because browser enforces HTTPS even for first visit (if preloaded). HSTS preload list: browsers include major domains with HSTS by default, submit sites to hstspreload.org. Benefits: prevents SSL stripping, prevents certificate warnings bypass (users can't click through), improves performance (no HTTP-to-HTTPS redirect). Configuration: set long max-age (1+ year), include subdomains if all use HTTPS, submit to preload list for maximum protection. Caveats: first visit before HSTS cached is vulnerable (preload solves this), requires all content served via HTTPS (mixed content breaks pages), difficult to remove once set (long cache). Complemented by HTTPS Everywhere browser extension forcing HTTPS.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Certificate pinning in mobile apps prevents what attack?|SQL injection|Rogue certificate authorities/MitM|DDoS|Buffer overflow|1|Certificate pinning prevents man-in-the-middle attacks using rogue or compromised Certificate Authorities by hardcoding expected certificate or public key in the application code. App validates server certificate matches pinned value, rejecting all others even if properly signed by trusted CA. Protects against: compromised CAs (DigiNotar 2011), government-mandated interception certificates, corporate SSL inspection tools, self-signed certificates in MitM attacks. Pinning options: certificate pinning (exact certificate), public key pinning (certificate can rotate if same key), CA pinning (trust specific CA only). Implementation: mobile apps (iOS/Android), API clients, embedded devices. Example: banking app pins to specific certificate, attacker intercepts with different certificate (even from trusted CA) → connection fails. Challenges: certificate rotation requires app update (use backup pins), breaks legitimate SSL inspection, users can't bypass expired pins without updates. Google/Twitter/GitHub major pinning breach incidents highlighted risks and updates needed. HTTP Public Key Pinning (HPKP) header was web equivalent but deprecated due to risks. Modern approach: certificate transparency logs + pinning. Protects high-value communications but adds operational complexity.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing DNS Security Extensions (DNSSEC) prevents what attack?|SQL injection|DNS cache poisoning/spoofing|XSS|Brute force|1|DNSSEC (DNS Security Extensions) uses cryptographic signatures to authenticate DNS responses, preventing cache poisoning attacks where attackers inject false DNS records redirecting users to malicious sites. DNSSEC provides origin authentication (DNS response came from authoritative server) and data integrity (response wasn't modified), but NOT confidentiality (DNS queries still visible). How it works: each DNS zone has public/private key pair, responses include RRSIG records (digital signatures), recursive resolvers validate signatures using DNSKEY and DS records in chain of trust from root servers. Benefits: prevents DNS spoofing (redirecting example.com to attacker IP), stops pharming attacks, validates domain ownership, protects against DNS hijacking. 2008 Dan Kaminsky DNS vulnerability made cache poisoning trivial without DNSSEC. Challenges: increased DNS response sizes (signatures add data), requires recursive resolver support (most major providers support), zone signing complexity, key rollover procedures. Adoption slowly growing - .gov, .com, .org support DNSSEC. Not solving: DNS privacy (use DNS-over-HTTPS/DNS-over-TLS for that), DDoS amplification (DNSSEC responses larger). Validation: dig +dnssec queries show RRSIG records, "ad" flag indicates authenticated data.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing egress filtering at network perimeter blocks what?|Inbound attacks only|Outbound malware C2 and data exfiltration|Internal communications|Legitimate traffic|1|Egress filtering controls outbound traffic from internal networks to Internet, blocking malware command-and-control (C2) communications and data exfiltration that pass through permissive ingress firewalls. Traditional firewalls focus on ingress (blocking inbound attacks) but malware often calls home via outbound connections that unrestricted egress allows. Egress filtering blocks: outbound connections from servers (servers shouldn't browse web), connections to suspicious countries/IPs, connections on unusual ports (C2 often uses high ports like 4444, 8080), DNS queries to known malicious domains, outbound mail except from mail servers (prevents spam botnets). Implementation: default-deny outbound with explicit allows for business needs, application-aware filtering (identify applications beyond port numbers), geo-blocking (block connections to high-risk countries), threat intelligence integration (block known C2 IPs/domains), DNS filtering (block malicious domains). Benefits: contains compromised systems (malware can't phone home), prevents data exfiltration (large uploads blocked), detects infections (blocked C2 attempts trigger alerts). Caveats: requires understanding legitimate business needs, cloud services complicate (need many IPs allowed), encrypted tunnels (VPNs, HTTPS) can bypass. Modern firewalls with SSL inspection help.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Rate limiting authentication attempts at the application level mitigates what?|XSS|Brute force and credential stuffing|SQL injection|CSRF|1|Rate limiting restricts number of authentication attempts from single IP address or user account within time window, mitigating brute force attacks and credential stuffing. Example: allow 5 login attempts per 15 minutes per IP, then block/CAPTCHA required. Protects against: brute force (systematically trying passwords), credential stuffing (using leaked credentials from other breaches), password spraying (trying common passwords across many accounts). Implementation locations: application layer (code/framework), web application firewall (WAF), load balancer, API gateway, reverse proxy. Variations: per-IP rate limits (5 attempts/15 min), per-account rate limits (prevents distributed attacks targeting one account), progressive delays (exponential backoff after failures), CAPTCHA after threshold, temporary account lockout. Must balance security and usability - legitimate users occasionally mistype passwords. Consider: distributed attacks from botnets need different mitigation (CAPTCHA more effective than IP blocks), mobile users behind carrier NAT may share IPs (account-based limits help), VPN users appear from same IPs. Monitor for: spike in failed attempts (attack indicator), lockout patterns suggesting DoS. Complementary: strong password policies, MFA (makes brute force impractical), account monitoring (impossible travel detection), threat intelligence (block known bot IPs).|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Deploying Web Application Firewalls (WAF) with OWASP ModSecurity Core Rule Set mitigates what?|Network attacks|Web application attacks (OWASP Top 10)|Physical attacks|Insider threats|1|Web Application Firewalls (WAF) protect web applications by filtering HTTP/HTTPS traffic, blocking common attacks like SQL injection, XSS, CSRF, and other OWASP Top 10 vulnerabilities. WAFs inspect: request URLs, POST data, cookies, headers for malicious patterns. ModSecurity Core Rule Set (CRS) provides 200+ rules detecting: SQL injection (UNION SELECT, OR 1=1), XSS (<script> tags, event handlers), path traversal (../../../), remote file inclusion, command injection, protocol violations, malicious bots. Deployment modes: reverse proxy (inline traffic), bridge mode (layer 2 transparent), monitoring only (detect but don't block). Cloud WAFs (Cloudflare, AWS WAF, Azure Application Gateway) protect without on-premises hardware. Benefits: virtual patching (protect vulnerable apps until code fixed), compliance (PCI-DSS requires WAF), bot mitigation, DDoS protection, API security. Configuration: blocking mode vs. detection mode initially (tune rules before enforcing), custom rules for application-specific logic, IP reputation integration, geo-blocking, rate limiting per URL. Challenges: false positives requiring tuning, encrypted traffic requires SSL termination, sophisticated attacks may evade signatures. Complements secure coding practices (WAF is defense-in-depth, not replacement for secure development).|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing Content Security Policy (CSP) headers mitigates what web attack?|SQL injection|Cross-site scripting (XSS)|Brute force|DNS poisoning|1|Content Security Policy (CSP) is an HTTP response header that restricts sources from which browsers load content, significantly mitigating cross-site scripting (XSS) attacks by preventing inline scripts and unauthorized external resources. CSP directives specify: script-src (JavaScript sources), style-src (CSS sources), img-src (image sources), connect-src (AJAX/WebSocket endpoints), frame-ancestors (embedding in iframes). Example policy: "Content-Security-Policy: default-src 'self'; script-src 'self' https://apis.example.com; style-src 'self' 'unsafe-inline'; img-src 'self' data:". How it mitigates XSS: even if attacker injects <script>malicious_code</script>, browser refuses to execute inline scripts unless 'unsafe-inline' allowed (which defeats purpose). Attackers can't load external scripts from their domains unless explicitly allowed. Benefits: blocks reflected/stored/DOM XSS, prevents clickjacking (frame-ancestors), limits data exfiltration, reduces attack surface. Implementation: start with report-only mode (CSP-Report-Only header) collecting violations without blocking, analyze reports, refine policy, switch to enforcement mode. Challenges: requires cataloging all legitimate content sources, breaks inline event handlers (onclick), legacy code may need refactoring, third-party widgets complicated. Report-URI directive sends violation reports to endpoint for monitoring. Browser support excellent (95%+). Level 2 CSP adds nonces/hashes for specific inline scripts, Level 3 adds strict-dynamic for modern apps.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing account lockout with exponential backoff mitigates what while avoiding DoS?|XSS|Brute force without enabling easy account lockout DoS|SQL injection|Phishing|1|Account lockout with exponential backoff temporarily locks accounts after failed login attempts, with increasing delays for repeated failures, mitigating brute force attacks while reducing denial-of-service risk from attackers deliberately locking legitimate user accounts. Traditional account lockout problem: after 5 failed attempts lock 30 minutes → attacker can lock all users by trying wrong passwords (account lockout DoS attack). Exponential backoff solution: 1st failure = no delay, 2nd = 2 seconds, 3rd = 4 seconds, 4th = 8 seconds, 5th = 16 seconds, continuing up to maximum (e.g., 5 minutes). Legitimate users rarely notice (few failures) but automated attacks become impractically slow (try 10K passwords but each attempt takes minutes). Variations: per-IP rate limiting (slow down attackers without affecting account), CAPTCHA after threshold (stops bots but allows humans), account lockout only after CAPTCHA failures (user gets CAPTCHA opportunity before lockout), risk-based authentication (unusual location/device triggers stricter checks). Best practices: notify users of lockout attempts (alerts to account takeover attempts), monitor for patterns (many lockouts suggests attack), provide account recovery process, whitelist trusted IPs (employees from office), log all authentication failures. Balance: security (stop attacks) vs. usability (don't frustrate legitimate users) vs. availability (don't enable DoS). Multi-factor authentication (MFA) makes brute force impractical regardless of lockout policy.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Data loss prevention (DLP) tools monitoring egress points prevent what?|Data imports|Sensitive data exfiltration|Inbound attacks|Authentication|1|Data Loss Prevention (DLP) tools monitor egress points (email, web uploads, USB, printing, cloud services) for sensitive data leaving the organization, preventing intellectual property theft, compliance violations, and data breaches. DLP identifies sensitive data via: pattern matching (credit card numbers, SSNs), regular expressions (account numbers), keywords (confidential, proprietary), fingerprinting (exact document matches), statistical analysis (similar documents), machine learning (classifying content). Enforcement actions: block (prevent transmission), quarantine (hold for review), encrypt (secure transmission), alert (notify security), log only (audit mode). Deployment locations: network DLP (monitors email/web traffic), endpoint DLP (monitors files copied to USB/printed/uploaded), cloud DLP (monitors SaaS applications like Box/Dropbox/O365), discovery (finds sensitive data at rest). Use cases: prevent accidental disclosure (employee emails customer list to personal account), stop malicious exfiltration (disgruntled employee copying trade secrets), compliance (PCI-DSS requires DLP for card data, HIPAA for PHI, GDPR for personal data). Challenges: high false positive rates requiring tuning, encrypted traffic requires SSL inspection, performance impact scanning all traffic, legitimate business needs for external sharing. Major vendors: Symantec, McAfee, Forcepoint, Microsoft Purview. Complement with insider threat programs, least privilege, user training.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Security information and event management (SIEM) systems provide what primary capability?|Encryption|Centralized log aggregation and correlation|Patching|Segmentation|1|SIEM systems centrally aggregate, normalize, correlate, and analyze logs from diverse sources (firewalls, IDS/IPS, servers, endpoints, applications, cloud), providing security visibility and threat detection across the enterprise. SIEM capabilities: real-time monitoring (dashboards showing security posture), correlation rules (detect attack patterns across multiple events), alerting (notify analysts of suspicious activity), forensic analysis (investigate incidents via historical data), compliance reporting (demonstrate logging for PCI-DSS, HIPAA, SOX). Example correlation: detect lateral movement by correlating failed login from IP X on server A, then successful login from same IP on server B minutes later (potential compromised credentials). Log sources: Windows Event Logs, syslog from Linux/network devices, firewall denies, IDS alerts, antivirus detections, authentication logs, DNS queries, proxy logs, cloud API calls. Use cases: detect compromised accounts (impossible travel), find malware (C2 beaconing patterns), identify privilege escalation, discover policy violations, provide audit trails. Major platforms: Splunk, IBM QRadar, ArcSight, LogRhythm, Microsoft Sentinel. Challenges: expensive (licensing per GB/day ingested), complex tuning (reduce false positives), skilled analysts required (interpret findings), alert fatigue (too many low-value alerts). Success factors: define use cases before deployment, prioritize high-fidelity alerts, integrate threat intelligence feeds, automate response with SOAR.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Immutable infrastructure where systems are replaced rather than patched implements what concept?|Persistent modifications|Ephemeral infrastructure (disposable)|Manual configuration|Long-lived servers|1|Immutable infrastructure treats servers as disposable rather than persistent - systems are never modified after deployment; changes require building new system images and replacing old instances. Opposes traditional "pets" model (servers maintained for years, manually patched/configured) favoring "cattle" model (instances easily replaced). How it works: developer commits code → CI/CD pipeline builds new container image or VM → automated deployment replaces running instances → old instances terminated. Benefits: eliminates configuration drift (all instances identical), prevents persistent malware (redeployment removes infections), simplifies rollback (redeploy previous image), ensures consistent security baselines, reduces complexity (no patch testing per server). Implementation: containers (Docker images never modified, replaced entirely), serverless (functions ephemeral by nature), infrastructure-as-code (Terraform/CloudFormation define configurations), image-based deployment (Packer creates VM images). Challenges: stateful data requires separate persistence (databases, object storage), requires mature CI/CD pipeline, cultural shift from traditional administration, initial setup investment. Security benefits: compromised server replaced in minutes (automated redeployment), patching simplified (rebuild base image, redeploy all), attack surface reduced (no SSH access needed to running servers). Cloud-native architectures embrace immutability - AWS Auto Scaling, Kubernetes pods exemplify concept.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing geo-blocking to restrict access by country/region mitigates what?|Local attacks|Attacks from high-risk regions|Insider threats|Physical theft|1|Geo-blocking uses IP geolocation to restrict or allow access based on user's geographic location, blocking traffic from countries where organization has no business presence, reducing attack surface from high-risk regions. Common implementation: block countries known for high attack volumes (often Russia, China, Nigeria, North Korea - though legitimate users exist in all regions). Use cases: e-commerce allowing only customer countries, government systems allowing only domestic access, banking restricting to service regions, blocking compliance-prohibited countries (OFAC sanctions). Technologies: firewall geo-IP rules, CDN geo-restrictions (Cloudflare, Akamai), WAF geo-blocking, load balancer filtering. Benefits: reduces brute force from botnets, blocks state-sponsored attacks, simplifies compliance (data sovereignty requirements), reduces attack surface by 90%+ if limited countries needed. Limitations: VPNs/proxies allow bypassing (attackers route through allowed countries), blocks legitimate users traveling abroad, may violate discrimination laws in some jurisdictions, can't stop attacks from allowed countries or local attackers. Ethical considerations: blanket country blocking may be perceived as discriminatory; consider risk-based approach (require MFA from certain countries rather than absolute block). Combine with threat intelligence (block known malicious IPs regardless of country, allow known-good IPs regardless of country). Review periodically since attack origins shift over time.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing file integrity monitoring (FIM) enables detection of what?|Network attacks|Unauthorized file modifications|Physical theft|Password attacks|1|File Integrity Monitoring (FIM) uses cryptographic hashes to detect unauthorized modifications to critical system files, configurations, and binaries, alerting to malware infections, backdoors, policy violations, and insider tampering. How it works: establish baseline (hash all monitored files), periodically rehash files, compare against baseline, alert on differences. Monitored items: OS system files (/bin, /sbin, system32), application binaries, configuration files, web application code, database files, log files (detect deletion/modification). Use cases: detect rootkits (system binary modifications), find backdoors (unexpected files in system directories), verify patch application (system files updated correctly), compliance (PCI-DSS requires FIM for cardholder data systems). Change types detected: modification (content changed), creation (new files), deletion (files removed), permission changes (chmod alterations), ownership changes. Tools: Tripwire (dedicated FIM), AIDE (Linux), OSSEC (open source), Windows File Replication Service, commercial SIEM/EDR solutions. Implementation: define monitoring scope (too much generates noise), establish legitimate change process (approved changes shouldn't alert), integrate with change management (baseline updates after approved changes), tune alert thresholds (reduce false positives). Challenges: high-change environments (web servers with frequent deployments) generate many alerts, requires baseline maintenance, performance impact of continuous hashing. Critical for compliance (PCI-DSS 11.5, HIPAA, FISMA mandate FIM).|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Security awareness training teaching users to recognize phishing mitigates what attack vector?|Technical vulnerability exploitation|Social engineering attacks|DDoS|Brute force|1|Security awareness training educates users to recognize and report phishing emails, malicious links, suspicious attachments, social engineering calls, and other human-targeted attacks, reducing the most successful attack vector (humans). Training topics: identifying phishing emails (sender verification, urgency tactics, unexpected attachments, suspicious links), recognizing social engineering (pretexting, impersonation, authority abuse), safe browsing practices (HTTPS verification, avoiding unknown sites), password security (unique passwords, complexity, managers), physical security (tailgating, badge sharing, secure disposal), incident reporting (when/how to report suspicious activity), data handling (classification, encryption, sharing policies). Delivery methods: annual mandatory training, monthly security tips, simulated phishing campaigns (send fake phishing, track who clicks, retrain failures), posters/reminders, lunch-and-learn sessions, onboarding training for new hires. Metrics: phishing simulation click rates (track improvement over time), time-to-report suspicious emails, training completion rates, incident reports per user. Best practices: make engaging not boring (gamification, real examples, short modules), positive reinforcement (praise reporters, don't punish victims), executive participation (leaders model behavior), continuous reinforcement (not just annual), measure effectiveness (before/after phishing click rates). Regulatory requirements: PCI-DSS, HIPAA, GDPR require security awareness. Most breaches involve human error - training is cost-effective mitigation reducing social engineering success from 30%+ to under 5%.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Network access control (NAC) solutions that verify device health before network access implement what?|Unrestricted access|Posture assessment/compliance checking|Open network|Guest access|1|Network Access Control (NAC) systems verify device security posture (health checks) before granting network access, ensuring only compliant devices connect, quarantining non-compliant devices for remediation. Pre-admission checks: antivirus running and updated, OS patches current, firewall enabled, disk encryption active, unauthorized software absent, device registered/authorized. Post-admission actions: full network access if compliant, quarantine VLAN if non-compliant (access only to remediation servers), block if unregistered device. Technologies: 802.1X authentication (port-based NAC), agentless (scan devices remotely), agent-based (persistent/dissolvable agents on devices). Use cases: BYOD (personal devices meet security baseline), guest access (visitors to isolated network), contractor access (temporary workers limited access), IoT devices (medical/industrial equipment may not support agents - profiling helps). Benefits: prevents compromised devices spreading malware, enforces patch compliance, discovers/inventories network devices, supports BYOD securely, adapts access to risk (different access for managed vs. unmanaged). Implementation challenges: legacy devices may not support 802.1X (MAC authentication bypass fallback), performance impact of continuous monitoring, device profiling accuracy (identify printers, cameras, IoT), user experience (avoid disrupting legitimate users). Major vendors: Cisco ISE, Aruba ClearPass, Fortinet FortiNAC. Requires: strong identity management (Active Directory integration), network infrastructure support (802.1X-capable switches), remediation processes (patch deployment, antivirus updates). Critical for zero trust architectures (device trust verification).|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing DNS sinkholing redirects malware C2 traffic to what?|Real C2 server|Controlled server for monitoring/blocking|User workstation|Public DNS|1|DNS sinkholing redirects malware command-and-control (C2) domain lookups to controlled IP addresses (sinkholes), preventing infected systems from reaching actual C2 servers while enabling monitoring of infections. How it works: security researchers/organizations identify malicious domains, configure DNS servers to return sinkhole IP (0.0.0.0, localhost, or monitoring server) instead of real C2 IP, malware connections fail or reach sinkhole. Benefits: blocks malware C2 (prevents attacker commands, data exfiltration, additional payload downloads), identifies infected systems (devices querying sinkholed domains are compromised), provides intelligence (number of infections, malware families, targeted organizations). Implementation: internal DNS servers with blacklist domains, DNS firewall/RPZ (Response Policy Zones) at recursive resolvers, commercial threat intelligence feeds providing sinkhole lists, ISP-level sinkholes (major ISPs sinkhole known malware). Example: Conficker worm generated thousands of domains daily - security researchers sinkholed them preventing C2 communication. Major botnet takedowns use sinkholes - Emotet, TrickBot, Avalanche operations seized/sinkholed domains affecting millions of bots. Monitoring benefits: log queries to sinkholed domains identifying infected devices, observe infection patterns, track malware campaigns. Limitations: malware with DGA (domain generation algorithms) creates many domains (can't sinkhole all), P2P botnets don't rely on DNS, HTTPS makes traffic inspection harder. Combine with endpoint detection and remediation tools once infections identified via sinkhole.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing least privilege for service accounts and automated processes mitigates what risk?|Insufficient permissions|Excessive permissions enabling lateral movement|User convenience|System performance|1|Least privilege for service accounts (applications, scripts, scheduled tasks running under dedicated accounts) grants only permissions required for specific functions, preventing compromised services from being used for lateral movement or privilege escalation. Service account risks: often have excessive permissions (sometimes Domain Admin), rarely have password changes (static credentials), not monitored like user accounts, high-value targets (24/7 access, no MFA). Best practices: dedicated account per service (don't share), permissions scoped to specific resources (read only specific database, not entire server), deny interactive login (service accounts shouldn't log in at consoles), strong unique passwords (32+ character random), regular password rotation (automated if possible), monitoring service account usage (alerts on unusual activity). Examples: web application service account needs database read/write to specific schema (not sa/root), backup service account needs read access to data and write to backup location (not full admin), monitoring service needs read-only access to polled systems. Implementation: group managed service accounts (gMSA) in Active Directory (automatic password rotation), Kubernetes service accounts (pod-level permissions), AWS IAM roles for EC2/Lambda (temporary credentials), Azure Managed Identity. Attackers often target service accounts after initial compromise - Mimikatz extracts cached service credentials for privilege escalation. Review regularly: audit service account permissions quarterly, remove unused service accounts, document purpose/permissions. Reduces blast radius if service compromised.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing database activity monitoring (DAM) enables detection of what?|Network attacks|Unauthorized database access and SQL injection|Email threats|Physical intrusion|1|Database Activity Monitoring (DAM) continuously monitors database traffic, analyzing queries, access patterns, and privileged user activity to detect policy violations, SQL injection, privilege abuse, and data exfiltration. DAM capabilities: real-time monitoring (alert on suspicious queries), policy enforcement (block violating queries), audit trail (complete query history for forensics/compliance), anomaly detection (unusual access patterns), privileged user monitoring (track DBA activities), sensitive data tracking (who accessed SSNs, credit cards). Detection examples: SQL injection attempts (UNION SELECT, OR 1=1 patterns), privilege escalation (escalating from read-only to admin), excessive data retrieval (SELECT * returning millions of rows suggesting exfiltration), off-hours access (queries at 2 AM from unusual source), unauthorized schema changes (DROP/ALTER without approval). Deployment: network-based (monitor database traffic passively), agent-based (installed on database servers), log-based (analyze database audit logs). Benefits: compensates for weak database authentication (monitors what authenticated users do), meets compliance requirements (PCI-DSS 10.2 requires DAM for cardholder data, SOX for financial data, HIPAA for PHI), detects insider threats (malicious DBAs), provides forensics for breach investigation. Challenges: encrypted database traffic requires SSL decryption, performance impact (inline blocking mode), false positives requiring tuning, requires database expertise to write effective policies. Major vendors: Imperva, IBM Guardium, Oracle Database Activity Monitoring. Complements but doesn't replace: database encryption, access controls, vulnerability scanning.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Threat hunting proactively searching for threats not detected by automated tools finds what?|Known malware only|Advanced persistent threats and zero-day attacks|False positives|Misconfigurations|1|Threat hunting is proactive, hypothesis-driven investigation searching for threats that evaded automated security controls (SIEM alerts, antivirus), discovering advanced persistent threats (APTs), zero-day exploits, and sophisticated attacks living undetected in environment for weeks or months. Differs from incident response (hunting is proactive before alerts, IR is reactive to known incidents). Hunting process: form hypothesis (based on threat intelligence, TTPs, industry threats), collect data (logs, network traffic, endpoint telemetry), analyze for indicators (pivot on findings), respond if confirmed threat, document for detection engineering (create SIEM rules preventing future similar attacks). Example hypotheses: "Adversary using PowerShell for persistence - search for suspicious scheduled tasks launching PowerShell", "APT group targets our industry with spear phishing - search mailboxes for similar tactics", "Insider threat exfiltrating data - search for large file transfers to cloud storage". Data sources: SIEM logs, EDR telemetry, NetFlow/PCAP, threat intelligence feeds, memory forensics, user behavior analytics. Tools: Splunk, ELK stack, carbon Black, CrowdStrike Falcon, Velociraptor. Skills required: understand attacker TTPs (MITRE ATT&CK framework), know normal baseline (to spot anomalies), data analysis, security tool expertise. Findings: ~25% of hunts find real threats missed by automation, reduce dwell time (time from compromise to detection) from average 200+ days to weeks. Organizations mature to continuous hunting (dedicated threat hunting teams) vs. periodic exercises.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing honeypots and honeynets as deception technology provides what?|Production services|Early warning and attacker intelligence|User access|Data storage|1|Honeypots are decoy systems with no legitimate business purpose, designed to attract and detect attackers, providing early warning of reconnaissance/attacks and gathering intelligence on attacker techniques. Honeynets are networks of honeypots simulating realistic environments. Types: low-interaction honeypots (simulate services, safe but less realistic - Honeyd), high-interaction honeypots (real systems in controlled environment, realistic but risky). Deployment locations: internal network (detect lateral movement from compromised systems), DMZ (detect external attacks, divert attackers from real systems), cloud (attract Internet-wide scanning). Value: any interaction is suspicious (no legitimate traffic expected), see attacker techniques unfiltered (not constrained by real system protection), waste attacker time/resources on fake systems, detect zero-day exploits (attacks on unpatched honeypots), gather malware samples, study attacker behavior. Examples: Cowrie (SSH/Telnet honeypot), Dionaea (malware capture), Glastopf (web application honeypot), Canary tokens (honeytokens detecting specific file/URL access). Implementation: isolated from production (attackers can't pivot to real systems), realistic appearance (outdated software, vulnerable configurations attractive to attackers), instrumentation for monitoring (all activity logged), legal considerations (check if active defense legal in jurisdiction). Organizations use: detect insider threats (fake database with sensitive data only accessible to insiders), detect ransomware (early warning when honeypot encrypted), identify compromised credentials (honeypot SSH attempts with real-looking usernames). Challenges: must maintain realism (sophisticated attackers detect honeypots), potential liability if honeypot used to attack others, resource intensive for high-interaction. Commercial: TrapX, Illusive Networks, Attivo provide turnkey deception platforms.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing security baselines using CIS Benchmarks or DISA STIGs provides what?|Random configurations|Standardized secure configurations|Weak settings|Varied security|0|Security baselines define standardized, pre-tested secure configurations for operating systems, applications, and network devices, ensuring consistent security posture and compliance. CIS (Center for Internet Security) Benchmarks provide consensus-developed configurations for 100+ technologies - Windows, Linux, cloud platforms, databases, network devices. DISA STIGs (Security Technical Implementation Guides) are DoD-mandated configurations required for government systems. Benefits: proven secure configurations (tested by security community), compliance shortcuts (PCI-DSS, HIPAA, NIST recommend CIS/STIGs), consistency across fleet (all servers identically configured), automation-friendly (scripts apply baselines), reduce configuration errors. Hardening areas: disable unnecessary services/protocols, enforce strong authentication (password policies, account lockout), enable audit logging, apply least privilege, configure firewalls, enable security features (AppLocker, SELinux), remove default accounts/passwords. Implementation: download baseline for technology, test in non-production (some settings break applications), apply via automation (Group Policy, Ansible, Chef, Terraform), validate with assessment tools (CIS-CAT for CIS, STIG Viewer for STIGs), document exceptions (justified deviations from baseline), reassess quarterly (baselines update as threats evolve). Benchmark levels: Level 1 (practical, minimal impact), Level 2 (defense-in-depth, may affect functionality/performance). Tools: Microsoft Security Compliance Toolkit (SCT), CIS-CAT Pro Assessor. Organizations typically start with baseline, customize for environment, maintain as living documents. Alternative frameworks: NIST SP 800-53 controls, ISO 27002. Immutable infrastructure makes baselines easier - bake into images rather than configure post-deployment.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing robust backup and recovery procedures with offline/immutable backups mitigates what?|Hardware upgrades|Ransomware and data loss|Network optimization|User training|1|Robust backup strategies with offline or immutable backups protect against ransomware, hardware failures, human errors, and disasters by maintaining recoverable copies of critical data that attackers cannot encrypt or delete. Backup best practices: 3-2-1 rule (3 copies of data, 2 different media types, 1 off-site), test restores regularly (untested backups may fail when needed), document recovery procedures, encrypt backups (protect confidentiality), protect backup credentials (separate from production). Ransomware mitigation: offline backups (tape, disconnected drives - ransomware can't reach), immutable backups (write-once, append-only storage - can't be modified/deleted even with admin credentials), air-gapped backups (network-isolated), versioning (keep multiple restore points). Technologies: tape backup (offline by nature), cloud object storage with object lock (AWS S3 Glacier with compliance mode, Azure Blob immutable storage), backup appliances (Rubrik, Cohesity with immutability), snapshots (NetApp, ZFS with read-only snapshots). Recovery time objective (RTO): acceptable downtime duration - guides backup frequency. Recovery point objective (RPO): acceptable data loss - guides backup frequency (RPO 1 hour = hourly backups). Test scenarios: restore single file, restore entire server, restore to different hardware, restore after ransomware (verify clean backups). Common mistakes: backups on same network as production (ransomware encrypts both), not testing restores (discover media errors during emergency), weak backup admin credentials (attackers delete backups before ransomware), insufficient retention (only 1 day backup, attackers established for weeks). Ransomware response: isolate infected systems, identify clean backup point before infection, restore from offline/immutable backups. Compliance requirements: SOX, HIPAA mandate data backup/recovery capabilities.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing privilege access management (PAM) solutions with just-in-time access mitigates what?|Public access|Standing privileged access (always-on admin rights)|Guest access|Anonymous access|1|Privileged Access Management (PAM) solutions control, monitor, and audit elevated privileges, implementing just-in-time (JIT) access where users request temporary administrative rights only when needed, reducing persistent privileged account exposure. Traditional problem: users/service accounts have standing privileges (permanent admin rights) - if compromised, attackers gain immediate elevated access. PAM solutions: credential vaulting (store privileged passwords securely, rotate frequently), session management (record privileged sessions for audit), just-in-time elevation (temporary admin rights for specific task, automatic revocation), approval workflows (require manager/security approval for privilege requests). JIT access: user requests Domain Admin for 2 hours to perform server maintenance, approval granted, automatic privilege elevation, task completed, privileges automatically revoked after time expires or manual relinquishment. Benefits: reduces attack surface (fewer permanent admins), limits blast radius (compromised account has minimal privileges), provides audit trail (who had privileges when), enforces least privilege, supports compliance (SOX, PCI-DSS segregation requirements). Technologies: CyberArk, BeyondTrust, Thycotic Secret Server, Azure Privileged Identity Management (PIM), AWS IAM temporary security credentials. Additional features: privileged session monitoring (video recordings), keystroke logging, command filtering (prevent dangerous commands), credential rotation (change passwords automatically after each use), breakglass procedures (emergency access when PAM unavailable). Use cases: Windows Domain Admin access, Unix root access, database DBA privileges, cloud administrator roles, application admin accounts. Reduces insider threat risk and credential theft impact - attackers stealing credentials of de-privileged account gain nothing. Many breaches exploit harvested privileged credentials - PAM significantly reduces this risk vector.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques

Implementing secure software development lifecycle (SSDLC) with security testing at each phase mitigates what?|Coding speed|Application vulnerabilities before production|Network attacks|Physical threats|1|Secure Software Development Lifecycle (SSDLC) integrates security activities into each development phase (requirements, design, coding, testing, deployment) to identify and fix vulnerabilities before production, when fixes are cheaper and attacks impossible. SSDLC phases: requirements (define security requirements, threat modeling), design (security architecture review, abuse case analysis), development (secure coding standards, code review), testing (SAST/DAST/IAST, penetration testing), deployment (security configuration, patch management), maintenance (vulnerability management, incident response). Security testing types: SAST (Static Application Security Testing - white-box source code analysis, finds SQL injection/XSS patterns), DAST (Dynamic Application Security Testing - black-box testing running application, finds runtime vulnerabilities), IAST (Interactive - combines SAST/DAST during QA testing), dependency scanning (finds vulnerable third-party libraries), fuzzing (invalid inputs to find crashes). Benefits: cheaper to fix (vulnerabilities found in development cost 10-100x less than production), reduces breach risk (fewer vulnerabilities reach production), compliance (PCI-DSS 6.3 requires secure development), improves code quality, reduces technical debt. Integration: shift-left security (earlier testing), DevSecOps (security automation in CI/CD), pre-commit hooks (block commits with vulnerabilities), pull request security checks, automated security gates. Tools: SonarQube (SAST), Checkmarx, Veracode (SAST/DAST), Snyk (dependency scanning), OWASP ZAP (DAST). Challenges: false positives requiring triage, developer training (security awareness), cultural change (security as enabler not blocker), tool integration complexity. Industry frameworks: OWASP SAMM, Microsoft SDL, NIST SSDF (Secure Software Development Framework). Security champions programs train developers in security practices.|2.0 Threats, Vulnerabilities, and Mitigations|2.5 Mitigation Techniques
