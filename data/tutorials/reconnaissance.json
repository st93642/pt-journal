{
  "id": "reconnaissance",
  "title": "Reconnaissance",
  "description": "Comprehensive reconnaissance phase covering all aspects of information gathering and target enumeration",
  "type": "tutorial",
  "steps": [
    {
      "id": "subdomain_enumeration",
      "title": "Subdomain enumeration",
      "content": "OBJECTIVE: Discover all subdomains associated with the target domain to expand the attack surface and identify potential entry points.\n\nACADEMIC BACKGROUND:\nSubdomain enumeration is a critical first step in penetration testing based on the principle that organizations often have inconsistent security postures across different subdomains. According to OWASP Web Security Testing Guide (WSTG-INFO-02), proper reconnaissance can reveal development environments, staging servers, and forgotten assets that may have weaker security controls than production systems.\n\nThe MITRE ATT&CK framework categorizes this activity under Reconnaissance (TA0043) > Active Scanning: Scanning IP Blocks (T1595.001) and Gather Victim Network Information: Domain Properties (T1590.001).\n\nSTEP-BY-STEP PROCESS:\n\n1. PASSIVE RECONNAISSANCE (No Direct Target Interaction):\n   a) Certificate Transparency (CT) Logs:\n      - Visit crt.sh: https://crt.sh/?q=%.target.com\n      - Query via API: curl -s \"https://crt.sh/?q=%.target.com&output=json\" | jq\n      - Alternative: cert.sh, censys.io, certspotter.com\n      - Why: SSL/TLS certificates are publicly logged and reveal all domains/subdomains\n\n   b) DNS Aggregators and Search Engines:\n      - SecurityTrails: historical DNS data and subdomain discovery\n      - VirusTotal: Check passive DNS and URL scanner results\n      - DNSDumpster: Free domain research tool with visual maps\n      - Shodan.io: Query: ssl.cert.subject.cn:\"target.com\"\n      - Google Dorks: site:*.target.com -www\n\n   c) Web Archives:\n      - Wayback Machine (archive.org): Historical subdomain snapshots\n      - Common Crawl: Petabyte-scale web crawl data\n      - Usage: Check old pages for links to now-defunct subdomains\n\n   d) Code Repositories:\n      - GitHub search: org:target \"target.com\" OR \"*.target.com\"\n      - GitLab, Bitbucket: Search for hardcoded subdomains in config files\n      - Look for: API endpoints, staging URLs, internal documentation\n\n2. ACTIVE ENUMERATION (Direct DNS Queries):\n   a) Install and Configure Tools:\n      - Subfinder: go install -v github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest\n      - Amass: sudo apt install amass OR https://github.com/owasp-amass/amass\n      - Assetfinder: go install github.com/tomnomnom/assetfinder@latest\n      - Findomain: https://github.com/Findomain/Findomain\n\n   b) Run Parallel Enumeration:\n      ```bash\n      # Subfinder (fast, API-integrated)\n      subfinder -d target.com -all -recursive -o subfinder.txt -v\n      \n      # Amass (comprehensive, OWASP recommended)\n      amass enum -passive -d target.com -o amass_passive.txt\n      amass enum -active -d target.com -o amass_active.txt -brute -w /usr/share/wordlists/dns.txt\n      \n      # Assetfinder (simple, effective)\n      assetfinder --subs-only target.com > assetfinder.txt\n      \n      # DNS Bruteforce with Gobuster\n      gobuster dns -d target.com -w /usr/share/wordlists/SecLists/Discovery/DNS/subdomains-top1million-5000.txt -o gobuster.txt\n      ```\n\n   c) Permutation Generation:\n      - Install altdns: pip3 install py-altdns\n      - Generate variations: altdns -i subfinder.txt -o permutations.txt -w words.txt\n      - Test permutations: altdns -i permutations.txt -o resolved_permutations.txt -r -s resolved.txt\n\n3. VALIDATION AND VERIFICATION:\n   a) Combine and Deduplicate:\n      ```bash\n      cat subfinder.txt amass_*.txt assetfinder.txt gobuster.txt | \\\n      sort -u | \\\n      grep -v '*' > all_subdomains.txt  # Remove wildcards\n      ```\n\n   b) DNS Resolution:\n      - dnsx: cat all_subdomains.txt | dnsx -silent -a -resp-only -o resolved_ips.txt\n      - Check for: Multiple IPs, CNAME chains, CDN usage\n      - Filter out: Dead domains, wildcard responses, honeypots\n\n   c) HTTP/HTTPS Probing:\n      - httpx: cat all_subdomains.txt | httpx -silent -status-code -tech-detect -o live_hosts.txt\n      - Identify: Technologies, status codes, redirects, title tags\n      - Screenshot: gowitness file -f live_hosts.txt (visual reconnaissance)\n\n   d) Verify Ownership:\n      - WHOIS lookups: whois subdomain.target.com\n      - Check NS records: dig subdomain.target.com NS +short\n      - Confirm in-scope: Ensure subdomains belong to target organization\n\n4. DOCUMENTATION AND ANALYSIS:\n   - Create spreadsheet with columns: Subdomain | IP | Status Code | Technologies | Notes\n   - Categorize by function: API endpoints, admin panels, dev/staging, production\n   - Priority ranking: Based on interesting technologies, potential vulnerabilities\n   - Timeline: Note when each subdomain was discovered and last verified\n\nWHAT TO LOOK FOR:\n- Development/Staging environments (often less secure): dev.*, stage.*, test.*, qa.*\n- API endpoints and microservices: api.*, ws.*, graphql.*, rest.*\n- Administrative interfaces: admin.*, cpanel.*, webmail.*, login.*\n- Third-party integrations and SaaS: jira.*, confluence.*, gitlab.*, jenkins.*\n- Cloud storage: s3.*, azure.*, gcp.*, cdn.*, assets.*\n- Email infrastructure: mail.*, smtp.*, mx.*, webmail.*\n- VPN/Remote access: vpn.*, remote.*, citrix.*, owa.*\n- Legacy systems: old.*, legacy.*, archive.*, backup.*\n- Geographic variants: us.*, eu.*, asia.*, london.*\n\nCOMMON PITFALLS:\n- Wildcard DNS Records: Test with random subdomain (asdjklqwer123.target.com) to detect wildcards\n- Rate Limiting: Space out requests, use multiple DNS resolvers, respect robots.txt and scope\n- Certificate Mismatch: Some subdomains use shared hosting with mismatched SSL certificates\n- Geoblocking: Subdomains may only respond from specific countries/IP ranges\n- Internal-Only: Some subdomains resolve only from internal networks (VPN required)\n- False Positives: CDN edge nodes may show up as subdomains but aren't actual assets\n- Scope Creep: Always verify subdomains are owned by target, not third-party partners\n\nDOCUMENTATION REQUIREMENTS:\n- List of all discovered subdomains (categorized by type)\n- Screenshots of interesting interfaces\n- Network diagram showing subdomain relationships\n- Technology stack identified per subdomain\n- Evidence of any immediate security concerns (default creds page, directory listings, etc.)\n\nTOOLS REFERENCE:\n- Subfinder: https://github.com/projectdiscovery/subfinder\n- Amass: https://github.com/owasp-amass/amass (OWASP Project)\n- crt.sh: Certificate Transparency logs\n- DNSDumpster: https://dnsdumpster.com\n- SecurityTrails: https://securitytrails.com\n- VirusTotal: https://www.virustotal.com\n- httpx: https://github.com/projectdiscovery/httpx\n- dnsx: https://github.com/projectdiscovery/dnsx\n\nFURTHER READING:\n- OWASP WSTG v4.2: Section 4.2 Information Gathering\n- PTES Technical Guidelines: Section 3 - Intelligence Gathering\n- NIST SP 800-115: Technical Guide to Information Security Testing",
      "tags": ["recon", "subdomain", "enumeration", "dns", "passive", "active"]
    },
    {
      "id": "dns_enumeration",
      "title": "DNS enumeration",
      "content": "OBJECTIVE: Map the complete DNS infrastructure to understand domain structure, identify misconfigurations, and discover additional attack vectors.\n\nACADEMIC BACKGROUND:\nThe Domain Name System (DNS) is a hierarchical distributed naming system that translates human-readable domain names into IP addresses. According to RFC 1035 and subsequent RFCs, DNS uses various record types to store different kinds of information. Security misconfigurations in DNS can expose internal network structure, enable cache poisoning attacks (CVE-2008-1447), or facilitate email spoofing.\n\nThis activity aligns with OWASP WSTG-INFO-02 (Fingerprint Web Server) and MITRE ATT&CK T1590.002 (Gather Victim Network Information: DNS).\n\nDNS RECORD TYPES EXPLAINED:\n- A Record: Maps hostname to IPv4 address\n- AAAA Record: Maps hostname to IPv6 address\n- CNAME: Creates an alias from one name to another\n- MX: Specifies mail servers and priority\n- NS: Delegates a DNS zone to authoritative name servers\n- TXT: Holds arbitrary text data (SPF, DKIM, DMARC, verification tokens)\n- SOA: Specifies authoritative information about DNS zone\n- SRV: Generalized service location record\n- PTR: Reverse DNS lookup (IP to hostname)\n- CAA: Specifies which certificate authorities can issue certificates\n\nSTEP-BY-STEP PROCESS:\n\n1. BASIC DNS RECORD ENUMERATION:\n   a) A and AAAA Records (IP Addresses):\n      ```bash\n      # IPv4 address resolution\n      dig target.com A +short\n      dig @8.8.8.8 target.com A +noall +answer\n      nslookup target.com\n      host target.com\n      \n      # IPv6 address resolution\n      dig target.com AAAA +short\n      \n      # Check all discovered subdomains\n      cat subdomains.txt | while read sub; do echo \"$sub: $(dig +short $sub A)\"; done\n      ```\n      Analysis: Multiple A records indicate load balancing; AAAA presence shows IPv6 support\n\n   b) CNAME Records (Aliasing):\n      ```bash\n      dig target.com CNAME +short\n      \n      # Follow CNAME chains\n      dig target.com +trace\n      ```\n      Security note: Long CNAME chains can indicate third-party services or CDN usage\n\n   c) Name Server Records:\n      ```bash\n      dig target.com NS +short\n      dig target.com NS +norecurse\n      whois target.com | grep \"Name Server\"\n      \n      # Check if nameservers are authoritative\n      dig @ns1.target.com target.com SOA\n      ```\n      What to check: Are nameservers from same provider? Mix of providers can indicate complexity\n\n   d) Mail Exchange Records:\n      ```bash\n      dig target.com MX +short\n      \n      # Check each mail server\n      dig mx1.target.com A +short\n      nmap -p 25,587,465 mx1.target.com\n      ```\n      Security implications: Identifies email infrastructure for phishing analysis\n\n2. ADVANCED DNS RECORD TYPES:\n   a) TXT Records (Critical for Email Security):\n      ```bash\n      dig target.com TXT +short\n      \n      # Look specifically for email authentication\n      dig target.com TXT | grep -i \"spf\\|dkim\\|dmarc\"\n      \n      # Check DMARC record\n      dig _dmarc.target.com TXT +short\n      \n      # Common DKIM selectors\n      for selector in default google k1 dkim mail; do\n          dig $selector._domainkey.target.com TXT +short\n      done\n      ```\n      \n      SPF Record Analysis:\n      - v=spf1: SPF version identifier\n      - ip4:192.0.2.0/24: Authorized IP ranges\n      - include:_spf.google.com: Include third-party SPF records\n      - -all: Fail (strict), ~all: Soft fail, +all: Pass (insecure!)\n      \n      DMARC Policy Checks:\n      - p=none: Monitoring only (weakest)\n      - p=quarantine: Move suspicious emails to spam\n      - p=reject: Block spoofed emails (strongest)\n      - pct=100: Apply policy to 100% of messages\n      \n      Security Findings:\n      - SPF with +all or missing -all: Allows email spoofing\n      - No DMARC record: No protection against domain spoofing\n      - DMARC p=none: Monitoring only, not enforcing\n      - Overly permissive SPF includes\n\n   b) SRV Records (Service Discovery):\n      ```bash\n      # Common services\n      dig _sip._tcp.target.com SRV +short\n      dig _ldap._tcp.target.com SRV +short\n      dig _jabber._tcp.target.com SRV +short\n      dig _autodiscover._tcp.target.com SRV +short  # Microsoft Exchange\n      \n      # Enumerate all SRV records (requires wordlist)\n      for service in sip ldap xmpp jabber h323 kerberos ldaps; do\n          for proto in tcp udp; do\n              dig _$service._$proto.target.com SRV +short\n          done\n      done\n      ```\n      What this reveals: Internal services, VoIP systems, LDAP directories\n\n   c) CAA Records (Certificate Authority Authorization):\n      ```bash\n      dig target.com CAA +short\n      ```\n      Example: 0 issue \"letsencrypt.org\"\n      Security: Restricts which CAs can issue certificates (prevents fraudulent certs)\n\n   d) SOA Records (Zone Information):\n      ```bash\n      dig target.com SOA +short\n      dig target.com SOA +norecurse  # Query authoritative server directly\n      ```\n      Information gained:\n      - Primary nameserver\n      - Admin email (with @ replaced by .)\n      - Serial number (zone version)\n      - Refresh, retry, expiry intervals\n      - Minimum TTL\n\n3. ZONE TRANSFER TESTING (AXFR):\n   a) Identify Name Servers:\n      ```bash\n      dig target.com NS +short > nameservers.txt\n      ```\n   \n   b) Attempt Zone Transfer on Each:\n      ```bash\n      # Test each nameserver\n      cat nameservers.txt | while read ns; do\n          echo \"[*] Testing $ns\"\n          dig @$ns target.com AXFR\n      done\n      \n      # Using host command\n      host -l target.com ns1.target.com\n      \n      # Using nmap NSE script\n      nmap --script dns-zone-transfer --script-args dns-zone-transfer.domain=target.com -p 53 ns1.target.com\n      ```\n   \n   c) Analysis of Results:\n      - Successful AXFR: CRITICAL vulnerability, immediately report\n      - Provides complete DNS database: All subdomains, IPs, internal hostnames\n      - Historical issue: Many DNS servers patched, but legacy systems may still be vulnerable\n      - Modern protection: DNSSEC, TSIG authentication, ACLs\n\n4. REVERSE DNS AND PTR RECORDS:\n   ```bash\n   # Get IP addresses first\n   dig target.com A +short > ips.txt\n   \n   # Reverse lookup\n   cat ips.txt | while read ip; do\n       echo \"$ip: $(dig -x $ip +short)\"\n   done\n   \n   # Check entire subnet (if in scope)\n   for i in {1..254}; do\n       dig -x 192.0.2.$i +short\n   done | grep -v \"^$\" >> reverse_dns.txt\n   ```\n   Use cases: Discover additional hostnames, verify IP ownership, find neighboring assets\n\n5. DNS SECURITY EXTENSIONS (DNSSEC):\n   ```bash\n   # Check if DNSSEC is enabled\n   dig target.com +dnssec\n   \n   # Validate DNSSEC chain\n   dig target.com +dnssec +multi\n   delv @8.8.8.8 target.com\n   ```\n   Security assessment:\n   - DNSSEC enabled: Better protection against cache poisoning\n   - No DNSSEC: Vulnerable to man-in-the-middle DNS attacks\n   - Misconfigured DNSSEC: Can cause legitimate resolution failures\n\n6. DNS ANALYTICS AND PASSIVE DNS:\n   a) Historical DNS Data:\n      - SecurityTrails API: Historical IP addresses and nameserver changes\n      - VirusTotal: Passive DNS resolution data\n      - PassiveTotal (RiskIQ): Comprehensive historical DNS data\n   \n   b) DNS Monitoring Services:\n      ```bash\n      # Query SecurityTrails\n      curl -H \"APIKEY: your_key\" https://api.securitytrails.com/v1/history/target.com/dns/a\n      \n      # VirusTotal API\n      curl https://www.virustotal.com/api/v3/domains/target.com\n      ```\n      \n   c) Analysis:\n      - IP address changes: Recent migrations, cloud providers\n      - Nameserver changes: Infrastructure updates, provider changes\n      - Subdomain additions: Business expansion, new services\n\nWHAT TO LOOK FOR:\n- SPF Records: +all or ~all (weak), missing -all (spoofable)\n- DMARC Missing: No protection against email spoofing\n- DMARC p=none: Monitoring mode only, not enforcing\n- Zone Transfer Enabled: Critical vulnerability exposing all DNS records\n- Internal IP Addresses: RFC1918 addresses (10.x, 172.16.x, 192.168.x) in public DNS\n- Wildcard Records: *.target.com pointing to catch-all server\n- Deprecated Services: FTP, Telnet, old email ports in SRV records\n- Inconsistent TTL Values: Very low TTL may indicate frequent changes or instability\n- Third-Party Dependencies: Many includes in SPF, external MX records\n- Subdomain Takeover Risks: CNAME pointing to non-existent services (GitHub Pages, AWS S3, Heroku)\n- Missing CAA Records: No restriction on certificate issuance\n- IPv6 Gaps: A records exist but no AAAA records (incomplete IPv6 deployment)\n\nCOMMON PITFALLS:\n- DNS Caching: Results may be cached; use +trace or query authoritative servers directly\n- Geographic Variations: Some DNS responses vary by geographic location (GeoDNS)\n- Time-Based Changes: DNS records may change based on time of day or load\n- Split-Horizon DNS: Different responses for internal vs external queries\n- DNS Firewalls: May block or filter certain query types\n- Rate Limiting: Excessive queries may trigger rate limits or blacklisting\n- Anycast Networks: Same IP may be different physical servers in different locations\n- TTL Expiration: Short TTL means records change frequently, recheck periodically\n\nDOCUMENTATION REQUIREMENTS:\n- Complete DNS record inventory (all types, all subdomains)\n- Network diagram showing DNS hierarchy and dependencies\n- SPF/DMARC/DKIM configuration analysis\n- List of third-party services identified (email, CDN, cloud providers)\n- Evidence of any vulnerabilities (zone transfer, spoofing risks, etc.)\n- Historical changes and trends\n- Recommendations for DNS security improvements\n\nSECURITY IMPLICATIONS:\n- Zone Transfer Enabled: Exposes entire internal network structure\n- Weak SPF/DMARC: Enables email spoofing and phishing campaigns\n- Internal IPs Leaked: Provides network topology information\n- Subdomain Takeover: CNAME pointing to unclaimed resources\n- DNS Cache Poisoning: Lack of DNSSEC enables MITM attacks\n- Information Disclosure: TXT records may contain sensitive info, API keys, or credentials\n\nTOOLS REFERENCE:\n- dig: Standard DNS query tool (part of BIND utilities)\n- nslookup: Legacy DNS query tool (Windows/Linux)\n- host: Simple DNS lookup utility\n- dnsenum: Automated DNS enumeration script\n- fierce: DNS reconnaissance tool\n- dnsrecon: DNS enumeration and security assessment\n- amass intel: DNS intelligence gathering (OWASP)\n\nFURTHER READING:\n- RFC 1035: Domain Names - Implementation and Specification\n- OWASP WSTG v4.2: WSTG-INFO-02 Fingerprint Web Server\n- SANS: DNS Security Best Practices\n- NIST SP 800-81-2: Secure Domain Name System (DNS) Deployment Guide\n- OWASP Email Security Cheat Sheet",
      "tags": ["recon", "dns", "enumeration", "spf", "dmarc", "dkim", "zone-transfer"]
    },
    {
      "id": "port_scanning",
      "title": "Port scanning",
      "content": "OBJECTIVE: Identify all open ports and services running on discovered hosts to map the attack surface and potential entry points.\n\nACADEMIC BACKGROUND:\nPort scanning is the process of sending packets to specific TCP or UDP ports on a target system and analyzing responses to determine which ports are open, closed, or filtered. This technique, formalized in RFC 793 (TCP) and RFC 768 (UDP), is fundamental to network security assessment.\n\nAccording to the PTES (Penetration Testing Execution Standard), port scanning falls under the \"Vulnerability Analysis\" phase and helps identify services that may have known vulnerabilities. The MITRE ATT&CK framework categorizes this as T1046 (Network Service Scanning) under Discovery tactics.\n\nTCP/IP PORT FUNDAMENTALS:\n- Total ports available: 65,535 (0-65535) per protocol (TCP/UDP)\n- Well-known ports: 0-1023 (require root/admin privileges)\n- Registered ports: 1024-49151 (registered with IANA)\n- Dynamic/Private ports: 49152-65535 (ephemeral, temporary)\n- Common protocols: TCP (connection-oriented), UDP (connectionless)\n\nTCP HANDSHAKE REVIEW:\n1. SYN: Client initiates connection\n2. SYN-ACK: Server acknowledges (port open)\n3. ACK: Client completes handshake\n   OR\n2. RST: Server resets (port closed)\n   OR\n2. No response: Port filtered by firewall\n\nSCANNING TECHNIQUES EXPLAINED:\n- SYN Scan (Half-open): Sends SYN, doesn't complete handshake (stealthy)\n- Connect Scan: Completes full TCP handshake (detected in logs)\n- FIN/NULL/XMAS Scans: Use TCP flags to evade simple firewalls\n- UDP Scan: Sends UDP packets, infers open from lack of ICMP unreachable\n- ACK Scan: Determines firewall rulesets, doesn't identify open ports\n- Window Scan: Analyzes TCP window field to identify open ports\n- Idle/Zombie Scan: Uses third-party host to mask scanning source\n\nSTEP-BY-STEP PROCESS:\n\n1. INITIAL HOST DISCOVERY (Determine Live Hosts):\n   ```bash\n   # Ping sweep for live hosts\n   nmap -sn 192.168.1.0/24 -oA host_discovery\n   \n   # TCP SYN ping (when ICMP blocked)\n   nmap -PS22,80,443 192.168.1.0/24 -oA syn_ping\n   \n   # Using masscan for large networks\n   masscan 192.168.1.0/24 -p0-65535 --rate 10000\n   \n   # Fast host discovery with rustscan\n   rustscan -a 192.168.1.0/24 --greppable -o live_hosts.txt\n   ```\n   Analysis: Identify which hosts respond to different probe types (some may block ICMP)\n\n2. FAST INITIAL PORT SCAN (Top Ports):\n   ```bash\n   # Scan top 1000 most common ports (Nmap default)\n   nmap -T4 --top-ports 1000 target.com -oA quick_scan\n   \n   # Top 100 ports for even faster reconnaissance\n   nmap --top-ports 100 -T5 target.com\n   \n   # Using Rustscan (Rust-based, extremely fast)\n   rustscan -a target.com -g -o rustscan_results.txt\n   ```\n   Rationale: Quickly identify primary services before comprehensive scan\n\n3. COMPREHENSIVE TCP PORT SCANNING:\n   a) Full Port Range SYN Scan:\n      ```bash\n      # All 65,535 TCP ports (requires root/sudo)\n      sudo nmap -sS -p- -T4 -v target.com -oA full_tcp_syn\n      \n      # With service version detection\n      sudo nmap -sS -sV -p- -T4 target.com -oA full_tcp_services\n      \n      # Aggressive scan (OS detection, version, scripts, traceroute)\n      sudo nmap -A -p- -T4 target.com -oA aggressive_scan\n      \n      # Performance tuning for faster scans\n      sudo nmap -sS -p- -T4 --min-rate 1000 --max-retries 1 target.com\n      ```\n      Flags explained:\n      - -sS: TCP SYN scan (half-open, stealthy, requires root)\n      - -sV: Version detection (identifies service and version)\n      - -O: OS fingerprinting (identifies operating system)\n      - -A: Aggressive scan (combines -sV, -O, scripts, traceroute)\n      - -p-: Scan all 65,535 ports\n      - -T4: Timing template (0-5, where 5 is fastest)\n      - --min-rate: Minimum packets per second\n      - --max-retries: Reduce retries for faster scans\n   \n   b) Alternative High-Speed Scanners:\n      ```bash\n      # Masscan (fastest, but less accurate)\n      sudo masscan -p1-65535 target.com --rate=10000 -oL masscan_results.txt\n      \n      # Rate limiting (be cautious, high rates can crash systems)\n      sudo masscan -p1-65535 target.com --rate=1000 --wait=5\n      \n      # Rustscan piped to Nmap for service detection\n      rustscan -a target.com -- -sV -sC -oA rustscan_nmap\n      ```\n      Performance comparison:\n      - Nmap -T4: ~30 minutes for all ports\n      - Masscan --rate=10000: ~7 seconds for all ports\n      - Rustscan: ~few seconds, then hands off to Nmap for accuracy\n\n4. UDP PORT SCANNING (Often Overlooked):\n   ```bash\n   # Top UDP ports\n   sudo nmap -sU --top-ports 100 target.com -oA udp_top100\n   \n   # Common UDP services\n   sudo nmap -sU -p 53,67,68,69,123,135,137,138,139,161,162,445,514,631,1900 target.com\n   \n   # UDP with version detection (very slow)\n   sudo nmap -sUV -p 53,161,162,500 target.com\n   \n   # Combined TCP/UDP scan\n   sudo nmap -sS -sU -p T:80,443,U:53,161 target.com\n   ```\n   UDP Scanning Challenges:\n   - No handshake: Hard to distinguish open from filtered\n   - ICMP rate limiting: Firewalls limit \"port unreachable\" responses\n   - Extremely slow: Can take hours for full scan\n   - False negatives: Services may not respond to empty UDP packets\n   \n   Common UDP Services:\n   - 53: DNS (Domain Name System)\n   - 67/68: DHCP (Dynamic Host Configuration)\n   - 69: TFTP (Trivial File Transfer)\n   - 123: NTP (Network Time Protocol)\n   - 161/162: SNMP (Simple Network Management)\n   - 500: IKE (IPsec VPN)\n   - 514: Syslog\n   - 1900: SSDP (UPnP)\n\n5. OPERATING SYSTEM FINGERPRINTING:\n   ```bash\n   # Basic OS detection\n   sudo nmap -O target.com\n   \n   # Aggressive OS detection\n   sudo nmap -O --osscan-guess target.com\n   \n   # OS detection with version scanning\n   sudo nmap -sV -O -p- target.com -oA os_fingerprint\n   ```\n   Analysis techniques:\n   - TCP/IP stack fingerprinting\n   - TTL (Time To Live) values\n   - Window size analysis\n   - TCP options ordering\n   - ICMP responses\n   \n   Common OS TTL values:\n   - Linux/Unix: 64\n   - Windows: 128\n   - Network devices: 255\n\n6. SERVICE VERSION DETECTION:\n   ```bash\n   # Standard version detection\n   nmap -sV target.com\n   \n   # Intensive version detection (all probes)\n   nmap -sV --version-intensity 9 target.com\n   \n   # Fast version detection (fewer probes)\n   nmap -sV --version-intensity 0 target.com\n   \n   # Version detection with NSE scripts\n   nmap -sV -sC target.com\n   ```\n   Version detection methods:\n   - Banner grabbing: Reading service banners\n   - Probe matching: Sending specific probes\n   - NULL probe: Empty packets to trigger response\n   - Service signatures: Matching against nmap-service-probes database\n\n7. FIREWALL/IDS EVASION TECHNIQUES:\n   ```bash\n   # Fragmented packets\n   nmap -f target.com\n   \n   # Decoy scanning (mask your IP among decoys)\n   nmap -D RND:10 target.com\n   \n   # Spoof source IP (requires raw packet crafting)\n   nmap -S 192.168.1.5 target.com\n   \n   # Random data length\n   nmap --data-length 25 target.com\n   \n   # Slow scan to avoid detection\n   nmap -T0 target.com  # Paranoid (very slow)\n   nmap -T1 target.com  # Sneaky\n   \n   # Randomize target order\n   nmap --randomize-hosts target1.com target2.com\n   \n   # Zombie/Idle scan (requires zombie host)\n   nmap -sI zombie_host target.com\n   ```\n   Caution: Evasion techniques may be illegal without authorization\n\n8. NMAP SCRIPTING ENGINE (NSE):\n   ```bash\n   # Run default scripts\n   nmap -sC target.com\n   \n   # Run specific script categories\n   nmap --script vuln target.com        # Vulnerability detection\n   nmap --script exploit target.com     # Exploitation scripts\n   nmap --script discovery target.com   # Discovery scripts\n   nmap --script auth target.com        # Authentication scripts\n   \n   # Run specific scripts\n   nmap --script ssl-enum-ciphers -p 443 target.com\n   nmap --script http-enum -p 80,443 target.com\n   nmap --script smb-vuln* target.com\n   \n   # Update script database\n   nmap --script-updatedb\n   ```\n   Popular NSE Scripts:\n   - ssl-heartbleed: Checks for Heartbleed vulnerability\n   - http-sql-injection: Tests for SQL injection\n   - smb-vuln-ms17-010: EternalBlue vulnerability\n   - ftp-anon: Tests for anonymous FTP access\n   - ssh-brute: SSH brute force\n   - dns-zone-transfer: Tests for zone transfer\n\n9. OUTPUT FORMATS AND ANALYSIS:\n   ```bash\n   # All formats simultaneously\n   nmap target.com -oA scan_results\n   # Creates: scan_results.nmap, scan_results.xml, scan_results.gnmap\n   \n   # XML format (for parsing/importing)\n   nmap target.com -oX results.xml\n   \n   # Greppable format\n   nmap target.com -oG results.gnmap\n   \n   # Normal output to file\n   nmap target.com -oN results.txt\n   \n   # Parse XML results\n   xsltproc scan_results.xml -o scan_results.html\n   \n   # Import to database\n   nmap target.com -oX - | ./parse_nmap.py\n   ```\n\n10. SCAN RESULT ANALYSIS:\n    ```bash\n    # Extract open ports\n    grep \"open\" scan_results.nmap\n    \n    # Count services\n    grep -c \"open\" scan_results.nmap\n    \n    # Find specific services\n    grep -i \"http\" scan_results.nmap\n    \n    # Parse with grep\n    awk '/Nmap scan report/{getline; print}' scan_results.nmap\n    ```\n\nWHAT TO LOOK FOR:\n- **Common Services**: HTTP/HTTPS (80/443), SSH (22), RDP (3389), SMB (445), FTP (21)\n- **Non-Standard Ports**: Services running on unusual ports (SSH on 2222, HTTP on 8080)\n- **Legacy/Insecure Protocols**: Telnet (23), FTP (21), SNMP v1/v2 (161), TFTP (69)\n- **Database Ports**: MySQL (3306), PostgreSQL (5432), MSSQL (1433), MongoDB (27017), Redis (6379)\n- **Admin Interfaces**: Webmin (10000), cPanel (2082/2083), phpMyAdmin (custom)\n- **Development Services**: Jenkins (8080), GitLab (custom), Docker API (2375/2376)\n- **Remote Access**: VNC (5900), RDP (3389), TeamViewer (5938)\n- **Unusual Port Combinations**: May indicate backdoors or custom services\n- **Filtered Ports**: Indicate firewall rules, reveals security posture\n- **Version Numbers**: Outdated versions with known vulnerabilities\n- **Banner Information**: Reveals OS, service versions, sometimes internal hostnames\n\nSECURITY IMPLICATIONS:\n- **Open Telnet/FTP**: Unencrypted credentials transmitted in plaintext\n- **SMBv1 Enabled**: Vulnerable to EternalBlue (MS17-010), WannaCry, NotPetya\n- **Open SNMP**: Can leak detailed system information with default \"public\" community string\n- **Unnecessary Services**: Each service increases attack surface\n- **Non-Standard Ports**: May indicate backdoor or attacker persistence\n- **Database Direct Access**: Databases shouldn't be exposed to internet\n- **Version Disclosure**: Helps attackers identify specific exploits\n\nCOMMON PITFALLS:\n- **Firewall False Negatives**: IDS/IPS may drop scan packets, making ports appear closed\n- **Rate Limiting**: Aggressive scanning can crash systems or trigger alerts\n- **Geographic Restrictions**: Some services only respond from specific IP ranges\n- **Load Balancers**: May show different ports open on different requests\n- **UDP Reliability**: UDP scans are unreliable; closed ports may not send ICMP unreachable\n- **Timing Issues**: Fast scans (-T5) may miss responses; slow scans (-T0/-T1) take hours\n- **Permission Denied**: SYN scans require root/admin privileges\n- **False Positives**: Some firewalls respond as if ports are open (honeypot)\n- **Legal Issues**: Unauthorized scanning is illegal in many jurisdictions\n- **Service Disruption**: Aggressive scans can cause denial of service\n\nDOCUMENTATION REQUIREMENTS:\n- Complete port inventory (all open, closed, filtered ports)\n- Service version matrix (port → service → version)\n- Operating system fingerprint results\n- Evidence screenshots of scan outputs\n- Network diagram showing scanned hosts and open services\n- Timeline of when scans were performed\n- List of unexpected or unusual findings\n- Recommendations for port closure and service hardening\n\nPERFORMANCE OPTIMIZATION:\n- **Parallel Scanning**: Scan multiple hosts simultaneously\n- **Top Ports First**: Start with --top-ports 1000, then expand\n- **Rate Tuning**: Adjust --min-rate and --max-rate based on network capacity\n- **Timing Templates**: Use -T4 for most scenarios (balance of speed/accuracy)\n- **Skip Discovery**: Use -Pn if you know hosts are up (skips ping)\n- **Batch Processing**: Scan subnets in chunks during off-hours\n\nLEGAL AND ETHICAL CONSIDERATIONS:\n- **Authorization Required**: Always obtain written permission before scanning\n- **Scope Compliance**: Only scan IP ranges explicitly authorized\n- **Avoid Disruption**: Use conservative timing to prevent service outages\n- **Data Protection**: Scan results may contain sensitive information\n- **Third-Party Systems**: Don't scan cloud services or third-party infrastructure\n- **Laws Vary**: Computer Fraud and Abuse Act (US), Computer Misuse Act (UK)\n\nTOOLS REFERENCE:\n- Nmap: https://nmap.org/ (The standard, most comprehensive)\n- Masscan: https://github.com/robertdavidgraham/masscan (Fastest)\n- Rustscan: https://github.com/RustScan/RustScan (Modern, Rust-based)\n- Unicornscan: http://www.unicornscan.org/ (Asynchronous, fast)\n- ZMap: https://zmap.io/ (Internet-wide scanning)\n- Angry IP Scanner: https://angryip.org/ (GUI-based, cross-platform)\n\nFURTHER READING:\n- Nmap Network Scanning by Gordon \"Fyodor\" Lyon (official book)\n- PTES Technical Guidelines: Section 3.4 - Vulnerability Analysis\n- OWASP WSTG-INFO-02: Fingerprint Web Server\n- NIST SP 800-115: Technical Guide to Information Security Testing\n- RFC 793: Transmission Control Protocol (TCP)\n- RFC 768: User Datagram Protocol (UDP)\n- SANS: Network Penetration Testing Best Practices",
      "tags": ["recon", "port-scanning", "nmap", "tcp", "udp", "services", "enumeration"]
    },
    {
      "id": "cloud_asset_discovery",
      "title": "Cloud asset discovery",
      "content": "OBJECTIVE: Identify cloud-hosted assets including storage buckets, compute instances, databases, and serverless functions that may contain sensitive data or misconfigurations.\n\nACADEMIC BACKGROUND:\nCloud asset discovery is crucial in modern penetration testing as organizations increasingly migrate to cloud platforms. According to the Cloud Security Alliance (CSA), misconfigured cloud resources are among the top cloud security risks. This activity aligns with MITRE ATT&CK T1526 (Cloud Service Discovery) and T1619 (Cloud Storage Object Discovery).\n\nCloud platforms expose various attack surfaces through storage buckets, compute instances, databases, and serverless functions. These resources often contain sensitive data or have misconfigured permissions that can lead to data breaches.\n\nSTEP-BY-STEP PROCESS:\n\n1. S3 BUCKET ENUMERATION (AWS):\n   ```bash\n   # Install S3Scanner for bucket discovery and permissions testing\n   git clone https://github.com/sa7mon/S3Scanner.git\n   cd S3Scanner && pip3 install -r requirements.txt\n   \n   # Create wordlist of potential bucket names\n   echo -e \"backup\\ndev\\nstaging\\nprod\\nlogs\\nassets\\ndata\\nfiles\\nmedia\\nstatic\\ncontent\\narchive\\ntemp\\ntest\\nqa\\nuat\" > bucket-names.txt\n   \n   # Run S3Scanner with custom wordlist\n   python3 s3scanner.py --list bucket-names.txt --out results.txt\n   \n   # Test public access on discovered buckets\n   aws s3 ls s3://target-company-backup --no-sign-request 2>&1 | grep -q \"AccessDenied\" || echo \"Bucket is public!\"\n   \n   # Common naming patterns enumeration\n   for suffix in backup dev staging prod logs assets data files media static content archive temp test qa uat; do\n       echo \"Testing: target-company-$suffix\"\n       aws s3 ls s3://target-company-$suffix --no-sign-request 2>&1 | head -5\n   done\n   \n   # Use cloud_enum for comprehensive AWS discovery\n   python3 cloud_enum.py -k target-company --disable-gcp --disable-azure\n   ```\n   \n   S3 Bucket Analysis:\n   - Check bucket policies: aws s3api get-bucket-policy --bucket bucket-name\n   - List bucket contents: aws s3 ls s3://bucket-name --recursive\n   - Check versioning: aws s3api get-bucket-versioning --bucket bucket-name\n   - Test write access: aws s3 cp test.txt s3://bucket-name/test.txt\n\n2. AZURE RESOURCE ENUMERATION:\n   ```bash\n   # Install MicroBurst for Azure enumeration\n   git clone https://github.com/NetSPI/MicroBurst.git\n   cd MicroBurst && pip3 install -r requirements.txt\n   \n   # Enumerate Azure storage accounts\n   python3 MicroBurst.py -d target.com\n   \n   # Test blob storage access\n   curl -I https://targetstorageaccount.blob.core.windows.net/container\n   \n   # Check for Azure file shares\n   curl -I https://targetstorageaccount.file.core.windows.net/share\n   \n   # Test common Azure naming patterns\n   for service in storage backup dev staging prod logs assets data files; do\n       curl -I https://target${service}.blob.core.windows.net/ 2>/dev/null | head -1\n   done\n   \n   # Azure resource enumeration with az cli (if authenticated)\n   az storage account list --query \"[].name\" -o tsv\n   az vm list --query \"[].name\" -o tsv\n   ```\n   \n   Azure Security Checks:\n   - Storage account permissions\n   - Blob container access levels\n   - Shared access signatures (SAS) exposure\n   - Azure AD misconfigurations\n\n3. GOOGLE CLOUD PLATFORM (GCP) ENUMERATION:\n   ```bash\n   # Install GCPBucketBrute\n   git clone https://github.com/RhinoSecurityLabs/GCPBucketBrute.git\n   cd GCPBucketBrute && pip3 install -r requirements.txt\n   \n   # Run GCP bucket enumeration\n   python3 GCPBucketBrute.py -k target-company\n   \n   # Test bucket access\n   curl https://target-backup.storage.googleapis.com/\n   curl https://storage.googleapis.com/target-company-assets/\n   \n   # Check for GCP compute instances (requires gcloud auth)\n   gcloud compute instances list --filter=\"name~target\"\n   \n   # Enumerate GCP storage buckets\n   gsutil ls -p target-project 2>/dev/null\n   \n   # Test Firebase projects\n   curl https://target-company.firebaseio.com/.json\n   ```\n   \n   GCP Security Considerations:\n   - Bucket IAM policies\n   - Public access prevention settings\n   - Cloud Storage FUSE misconfigurations\n   - Firebase database exposure\n\n4. CLOUD METADATA ENDPOINT ANALYSIS:\n   ```bash\n   # AWS EC2 instance metadata (from within instance)\n   curl http://169.254.169.254/latest/meta-data/\n   curl http://169.254.169.254/latest/user-data/\n   curl http://169.254.169.254/latest/meta-data/iam/security-credentials/\n   \n   # Azure instance metadata\n   curl -H \"Metadata:true\" http://169.254.169.254/metadata/instance?api-version=2021-02-01\n   \n   # GCP instance metadata\n   curl \"http://metadata.google.internal/computeMetadata/v1/\" -H \"Metadata-Flavor: Google\"\n   \n   # DigitalOcean metadata\n   curl http://169.254.169.254/metadata/v1/\n   ```\n   \n   Metadata Endpoint Risks:\n   - IAM credentials exposure\n   - Instance configuration data\n   - Network information leakage\n   - User data scripts containing secrets\n\n5. SERVERLESS FUNCTION DISCOVERY:\n   ```bash\n   # AWS Lambda functions\n   aws lambda list-functions --query \"Functions[].FunctionName\" | grep target\n   \n   # Azure Functions\n   curl https://target-functions.azurewebsites.net/api/function-name\n   \n   # Google Cloud Functions\n   curl https://us-central1-target-project.cloudfunctions.net/function-name\n   \n   # Test for common function names\n   for func in api webhook process handler; do\n       curl https://target-api.azurewebsites.net/api/$func 2>/dev/null | head -1\n   done\n   ```\n   \n   Serverless Security Issues:\n   - Exposed function URLs\n   - Overly permissive IAM roles\n   - Environment variable secrets\n   - Cold start vulnerabilities\n\n6. CLOUD DATABASE DISCOVERY:\n   ```bash\n   # AWS RDS instances\n   aws rds describe-db-instances --query \"DBInstances[].Endpoint.Address\" | grep target\n   \n   # Azure Database services\n   az sql server list --query \"[].name\" | grep target\n   \n   # GCP Cloud SQL\n   gcloud sql instances list --filter=\"name~target\"\n   \n   # Test database connectivity (if credentials available)\n   mysql -h target-db.cluster-xyz.us-east-1.rds.amazonaws.com -u readonly\n   ```\n   \n   Database Exposure Risks:\n   - Public IP accessibility\n   - Weak authentication\n   - Unencrypted connections\n   - Backup file exposure\n\n7. CLOUD CDN AND EDGE SERVICE ANALYSIS:\n   ```bash\n   # AWS CloudFront distributions\n   aws cloudfront list-distributions --query \"DistributionList.Items[].DomainName\" | grep target\n   \n   # Azure CDN endpoints\n   az cdn endpoint list --query \"[].hostName\" | grep target\n   \n   # Cloudflare enumeration\n   curl -I https://target.com | grep -i cloudflare\n   \n   # Check for misconfigured CDN origins\n   dig cdn.target.com\n   ```\n\nWHAT TO LOOK FOR:\n- **Publicly Accessible Storage**: S3 buckets, Azure blobs, GCP buckets with public read access\n- **Exposed API Keys**: Hardcoded credentials in function code, environment variables\n- **Misconfigured Permissions**: Buckets writable by anyone, overly permissive IAM policies\n- **Development Resources**: Dev/staging/test environments exposed to internet\n- **Unencrypted Data**: Sensitive data stored without encryption\n- **Metadata Endpoints**: Exposed instance metadata containing secrets\n- **Serverless Functions**: Publicly accessible functions that can be triggered\n- **Database Exposure**: RDS/Aurora instances accessible from internet\n- **CDN Misconfigurations**: Origins exposed, cache poisoning opportunities\n- **Cloud Logging**: Exposed log storage with sensitive information\n- **Backup Files**: Database dumps, configuration backups in storage\n- **Secrets Management**: Exposed secret keys, API tokens, certificates\n\nSECURITY IMPLICATIONS:\n- **Data Breaches**: Public buckets can leak customer data, PII, intellectual property\n- **Malware Hosting**: Writable storage enables hosting malicious payloads\n- **Credential Theft**: Exposed IAM keys enable full account compromise\n- **Ransomware**: Public storage can be encrypted by attackers\n- **Infrastructure Mapping**: Metadata reveals network topology and configurations\n- **Supply Chain Attacks**: Compromised cloud functions can affect downstream systems\n- **Financial Loss**: Exposed resources can incur unexpected cloud costs\n- **Compliance Violations**: GDPR, HIPAA, PCI-DSS violations from data exposure\n- **Reputation Damage**: Public disclosure of sensitive information\n\nCOMMON PITFALLS:\n- **Intentionally Public Content**: Some buckets are meant to be public (CDN assets, documentation)\n- **Rate Limiting**: Cloud providers throttle enumeration attempts\n- **Authentication Requirements**: Many resources require valid credentials\n- **Regional Restrictions**: Resources may only be accessible from specific regions\n- **Naming Convention Variations**: Companies use different patterns for resource naming\n- **Temporary Resources**: Some resources exist only during development/testing\n- **Third-Party Services**: Resources may belong to vendors, not the target\n- **Access Logging**: Enumeration attempts may be logged and trigger alerts\n- **Cost Implications**: Extensive scanning can incur cloud provider charges\n- **Legal Boundaries**: Cross-region scanning may violate data sovereignty laws\n\nDOCUMENTATION REQUIREMENTS:\n- Complete inventory of discovered cloud resources\n- Access level assessment (public, authenticated, private)\n- Data classification of exposed information\n- Screenshots of accessible resources and their contents\n- Permission analysis and misconfiguration evidence\n- Risk assessment with business impact analysis\n- Remediation recommendations for each finding\n- Timeline of discovery and verification\n- Evidence of any data exfiltration attempts\n\nTOOLS REFERENCE:\n- **S3Scanner**: https://github.com/sa7mon/S3Scanner (AWS S3 bucket enumeration)\n- **cloud_enum**: https://github.com/initstring/cloud_enum (Multi-cloud asset discovery)\n- **MicroBurst**: https://github.com/NetSPI/MicroBurst (Azure resource enumeration)\n- **GCPBucketBrute**: https://github.com/RhinoSecurityLabs/GCPBucketBrute (GCP storage discovery)\n- **CloudBrute**: https://github.com/0xsha/CloudBrute (Multi-cloud brute force)\n- **CloudHunter**: https://github.com/belane/CloudHunter (AWS/GCP/Azure enumeration)\n- **AWS CLI**: https://aws.amazon.com/cli/ (Official AWS command line interface)\n- **Azure CLI**: https://docs.microsoft.com/en-us/cli/azure/ (Official Azure command line)\n- **gcloud CLI**: https://cloud.google.com/sdk/gcloud (Official GCP command line)\n\nFURTHER READING:\n- Cloud Security Alliance (CSA): Top Threats to Cloud Computing\n- AWS Security Best Practices: https://aws.amazon.com/architecture/security-identity-compliance/\n- Azure Security Documentation: https://docs.microsoft.com/en-us/azure/security/\n- GCP Security Foundations: https://cloud.google.com/security\n- MITRE ATT&CK Cloud Matrix: https://attack.mitre.org/matrices/enterprise/cloud/\n- OWASP Cloud Security Testing Cheat Sheet\n- NIST SP 800-171: Protecting Controlled Unclassified Information in Cloud Environments",
      "tags": ["recon", "cloud", "aws", "azure", "gcp", "s3", "storage", "enumeration", "misconfiguration"]
    },
    {
      "id": "service_enumeration",
      "title": "Service enumeration",
      "content": "OBJECTIVE: Extract detailed information about services running on open ports to identify versions, configurations, and potential vulnerabilities for exploitation planning.\n\nACADEMIC BACKGROUND:\nService enumeration, also known as service fingerprinting, is the process of interacting with network services to gather intelligence about software versions, configurations, supported authentication methods, and underlying operating systems. This phase bridges port scanning and vulnerability assessment.\n\nAccording to OWASP WSTG-INFO-02 (Fingerprint Web Server and Services), proper enumeration provides the foundation for identifying known vulnerabilities (CVEs) associated with specific service versions. The MITRE ATT&CK framework categorizes detailed service enumeration as T1046 (Network Service Scanning) and T1592 (Gather Victim Host Information).\n\nThe NIST SP 800-115 Technical Guide emphasizes that service enumeration should be exhaustive but non-disruptive, gathering maximum information while avoiding service crashes or authentication lockouts.\n\nENUMERATION METHODOLOGY:\n1. **Banner Grabbing**: Capture service identification strings\n2. **Protocol-Specific Queries**: Use protocol commands to elicit detailed responses\n3. **NSE Script Enumeration**: Leverage Nmap scripts for deep inspection\n4. **Vulnerability Cross-Reference**: Map versions to known CVEs\n5. **Configuration Analysis**: Identify misconfigurations and weak settings\n\nSTEP-BY-STEP PROCESS:\n\n1. BASIC BANNER GRABBING (All Services):\n   ```bash\n   # Netcat banner grab (TCP services)\n   nc -v target.com 21        # FTP banner\n   nc -v target.com 22        # SSH banner\n   nc -v target.com 25        # SMTP banner\n   nc -v target.com 80        # HTTP banner\n   nc -v target.com 110       # POP3 banner\n   nc -v target.com 143       # IMAP banner\n   \n   # Nmap banner grabbing\n   nmap -sV --script banner target.com\n   \n   # Automated banner collection\n   nmap -p 21,22,23,25,80,110,143,443,3306,3389,5432,8080 -sV --script banner target.com -oA banners\n   \n   # Quick banner grab with timeout\n   timeout 5 bash -c 'echo \"\" | nc target.com 80'\n   ```\n   \n   Analysis: Banner strings often reveal exact service versions (e.g., \"SSH-2.0-OpenSSH_7.4\")\n\n2. HTTP/HTTPS WEB SERVICE ENUMERATION:\n   a) HTTP Headers Analysis:\n      ```bash\n      # Basic header inspection\n      curl -I https://target.com\n      curl -s -I https://target.com | grep -i server\n      \n      # Verbose header analysis\n      curl -v https://target.com 2>&1 | grep -i '^< '\n      \n      # Check all HTTP methods\n      curl -X OPTIONS https://target.com -i\n      \n      # Examine security headers\n      curl -I https://target.com | grep -iE '(X-Frame|X-XSS|Content-Security|Strict-Transport)'\n      \n      # Using httpx for header inspection\n      echo \"target.com\" | httpx -silent -status-code -tech-detect -title -server\n      ```\n   \n   b) SSL/TLS Certificate Analysis:\n      ```bash\n      # View certificate details\n      openssl s_client -connect target.com:443 < /dev/null 2>/dev/null | openssl x509 -text -noout\n      \n      # Extract subject alternative names (SANs)\n      openssl s_client -connect target.com:443 < /dev/null 2>/dev/null | openssl x509 -noout -text | grep \"DNS:\"\n      \n      # Check supported TLS versions\n      nmap --script ssl-enum-ciphers -p 443 target.com\n      \n      # Detect SSL/TLS vulnerabilities\n      nmap --script ssl-* -p 443 target.com\n      \n      # SSLScan comprehensive analysis\n      sslscan target.com\n      \n      # TestSSL.sh detailed audit\n      testssl.sh https://target.com\n      ```\n      \n      Certificate intelligence:\n      - Organization details\n      - Internal hostnames in SANs\n      - Certificate issuer and chain\n      - Validity period\n      - Weak ciphers or protocols\n   \n   c) Web Technology Fingerprinting:\n      ```bash\n      # WhatWeb identification\n      whatweb -a 3 https://target.com\n      \n      # Wappalyzer CLI\n      wappalyzer https://target.com\n      \n      # Using httpx with tech detection\n      echo \"target.com\" | httpx -tech-detect -status-code\n      \n      # Nikto web scanner (includes tech detection)\n      nikto -h https://target.com -o nikto_results.txt\n      \n      # Retire.js for JavaScript library vulnerabilities\n      retire --jspath https://target.com\n      ```\n\n3. SSH SERVICE ENUMERATION (Port 22):\n   ```bash\n   # SSH version banner\n   nc target.com 22\n   \n   # Detailed SSH enumeration\n   nmap -p 22 --script ssh-* target.com\n   \n   # Check SSH key algorithms and ciphers\n   nmap -p 22 --script ssh2-enum-algos target.com\n   \n   # SSH audit for weak configurations\n   ssh-audit target.com\n   \n   # Detect SSH authentication methods\n   nmap -p 22 --script ssh-auth-methods target.com\n   \n   # Try SSH connection to see supported methods\n   ssh -v user@target.com\n   ```\n   \n   Analysis points:\n   - OpenSSH version (check against CVE database)\n   - Supported key exchange algorithms (weak ones: diffie-hellman-group1-sha1)\n   - Cipher suites (avoid: arcfour, 3des-cbc)\n   - Authentication methods (password, publickey, keyboard-interactive)\n   - Host key types (RSA, ECDSA, ED25519)\n\n4. FTP SERVICE ENUMERATION (Port 21):\n   ```bash\n   # Anonymous FTP access test\n   ftp target.com\n   # Try username: anonymous, password: anonymous@example.com\n   \n   # Nmap FTP scripts\n   nmap -p 21 --script ftp-* target.com\n   \n   # Check for anonymous access\n   nmap -p 21 --script ftp-anon target.com\n   \n   # FTP bounce attack test\n   nmap -p 21 --script ftp-bounce target.com\n   \n   # Enumerate FTP server capabilities\n   echo \"HELP\" | nc target.com 21\n   echo \"FEAT\" | nc target.com 21\n   ```\n   \n   Security checks:\n   - Anonymous login enabled? (major finding)\n   - Writable directories (potential malware upload)\n   - FTP version (ProFTPD 1.3.5 has known RCE)\n   - Cleartext transmission (recommend SFTP/FTPS)\n\n5. SMB/CIFS ENUMERATION (Ports 139, 445):\n   ```bash\n   # Enumerate SMB shares\n   smbclient -L //target.com -N\n   \n   # Comprehensive SMB enumeration\n   enum4linux -a target.com\n   \n   # Modern enum4linux-ng\n   enum4linux-ng -A target.com -oA enum4linux_results\n   \n   # Nmap SMB scripts\n   nmap -p 139,445 --script smb-* target.com\n   \n   # Check for SMB vulnerabilities\n   nmap -p 445 --script smb-vuln-* target.com\n   \n   # SMB version detection\n   nmap -p 445 --script smb-protocols target.com\n   \n   # CrackMapExec SMB enumeration\n   crackmapexec smb target.com --shares\n   crackmapexec smb target.com --users\n   crackmapexec smb target.com --groups\n   \n   # NetBIOS enumeration\n   nbtscan target.com\n   nmblookup -A target.com\n   ```\n   \n   Critical findings:\n   - SMBv1 enabled (EternalBlue MS17-010 vulnerability)\n   - Null session enumeration (no authentication required)\n   - Readable/writable shares\n   - User/group enumeration\n   - Domain controller identification\n   - Guest account enabled\n\n6. SMTP ENUMERATION (Port 25, 587, 465):\n   ```bash\n   # SMTP banner grab\n   nc target.com 25\n   \n   # SMTP commands\n   telnet target.com 25\n   HELO example.com\n   VRFY root           # Verify user exists\n   EXPN admin          # Expand mailing list\n   MAIL FROM:<test@example.com>\n   RCPT TO:<user@target.com>\n   \n   # Nmap SMTP scripts\n   nmap -p 25 --script smtp-* target.com\n   \n   # User enumeration\n   smtp-user-enum -M VRFY -U /usr/share/wordlists/users.txt -t target.com\n   \n   # Check for open relay\n   nmap -p 25 --script smtp-open-relay target.com\n   \n   # SMTP commands enumeration\n   nmap -p 25 --script smtp-commands target.com\n   ```\n   \n   Analysis:\n   - VRFY/EXPN enabled? (user enumeration vulnerability)\n   - Open relay (can send spam)\n   - SMTP version and vulnerabilities\n   - Supported authentication mechanisms\n   - StartTLS available?\n\n7. SNMP ENUMERATION (Ports 161, 162):\n   ```bash\n   # SNMP community string brute force\n   onesixtyone -c /usr/share/wordlists/snmp-strings.txt target.com\n   \n   # SNMP walk (requires community string)\n   snmpwalk -v2c -c public target.com\n   \n   # System information\n   snmpwalk -v2c -c public target.com system\n   \n   # Network interfaces\n   snmpwalk -v2c -c public target.com interfaces\n   \n   # Running processes\n   snmpwalk -v2c -c public target.com hrSWRunName\n   \n   # Installed software\n   snmpwalk -v2c -c public target.com hrSWInstalledName\n   \n   # Nmap SNMP scripts\n   nmap -sU -p 161 --script snmp-* target.com\n   \n   # SNMPv3 enumeration (requires credentials)\n   snmpwalk -v3 -l authPriv -u snmpuser -a SHA -A authpass -x AES -X privpass target.com\n   ```\n   \n   SNMP OIDs of interest:\n   - 1.3.6.1.2.1.1.1.0 (System description)\n   - 1.3.6.1.2.1.1.5.0 (Hostname)\n   - 1.3.6.1.2.1.25.4.2.1.2 (Running processes)\n   - 1.3.6.1.2.1.25.6.3.1.2 (Installed software)\n   - 1.3.6.1.2.1.2.2.1.2 (Network interfaces)\n\n8. DATABASE SERVICE ENUMERATION:\n   a) MySQL/MariaDB (Port 3306):\n      ```bash\n      # MySQL version detection\n      nmap -p 3306 --script mysql-info target.com\n      \n      # Enumerate MySQL users\n      nmap -p 3306 --script mysql-users --script-args mysqluser=root,mysqlpass='' target.com\n      \n      # MySQL vulnerabilities\n      nmap -p 3306 --script mysql-vuln-* target.com\n      \n      # MySQL empty password check\n      nmap -p 3306 --script mysql-empty-password target.com\n      \n      # Direct connection attempt\n      mysql -h target.com -u root -p\n      ```\n   \n   b) PostgreSQL (Port 5432):\n      ```bash\n      # PostgreSQL version\n      nmap -p 5432 --script pgsql-brute target.com\n      \n      # Direct connection\n      psql -h target.com -U postgres\n      ```\n   \n   c) MSSQL (Port 1433):\n      ```bash\n      # MSSQL info gathering\n      nmap -p 1433 --script ms-sql-info target.com\n      \n      # MSSQL empty password\n      nmap -p 1433 --script ms-sql-empty-password target.com\n      \n      # MSSQL command execution (if accessible)\n      nmap -p 1433 --script ms-sql-xp-cmdshell --script-args mssql.username=sa,mssql.password='' target.com\n      ```\n   \n   d) MongoDB (Port 27017):\n      ```bash\n      # MongoDB enumeration\n      nmap -p 27017 --script mongodb-* target.com\n      \n      # Check for no authentication\n      mongo target.com:27017\n      ```\n   \n   e) Redis (Port 6379):\n      ```bash\n      # Redis banner grab\n      nc target.com 6379\n      INFO\n      \n      # Redis enumeration\n      nmap -p 6379 --script redis-* target.com\n      \n      # Direct connection\n      redis-cli -h target.com\n      INFO\n      CONFIG GET *\n      ```\n\n9. RDP ENUMERATION (Port 3389):\n   ```bash\n   # RDP certificate information\n   nmap -p 3389 --script rdp-ntlm-info target.com\n   \n   # RDP encryption check\n   nmap -p 3389 --script rdp-enum-encryption target.com\n   \n   # RDP BlueKeep vulnerability check\n   nmap -p 3389 --script rdp-vuln-ms12-020 target.com\n   \n   # RDP connection with rdesktop\n   rdesktop -u username target.com\n   \n   # Using xfreerdp for connection attempt\n   xfreerdp /v:target.com /u:username /p:password\n   ```\n\n10. LDAP ENUMERATION (Ports 389, 636, 3268):\n    ```bash\n    # Anonymous LDAP bind attempt\n    ldapsearch -x -H ldap://target.com -b \"\" -s base\n    \n    # Enumerate naming contexts\n    ldapsearch -x -H ldap://target.com -s base namingContexts\n    \n    # Full LDAP dump (if anonymous allowed)\n    ldapsearch -x -H ldap://target.com -b \"dc=example,dc=com\"\n    \n    # Nmap LDAP scripts\n    nmap -p 389 --script ldap-* target.com\n    \n    # LDAP brute force\n    nmap -p 389 --script ldap-brute target.com\n    ```\n\n11. DNS SERVICE ENUMERATION (Port 53):\n    ```bash\n    # DNS version query\n    dig @target.com version.bind CHAOS TXT\n    \n    # DNS service info\n    nmap -p 53 --script dns-nsid target.com\n    \n    # Zone transfer attempt (from DNS enumeration phase)\n    dig @target.com target.com AXFR\n    \n    # DNS recursion test\n    nmap -p 53 --script dns-recursion target.com\n    ```\n\n12. VPN SERVICE ENUMERATION:\n    a) IKE/IPsec (Port 500 UDP):\n       ```bash\n       # IKE scan\n       ike-scan target.com\n       \n       # Nmap IKE scripts\n       nmap -sU -p 500 --script ike-version target.com\n       ```\n    \n    b) OpenVPN (Port 1194):\n       ```bash\n       # OpenVPN service detection\n       nmap -p 1194 --script openvpn-info target.com\n       ```\n\nWHAT TO LOOK FOR:\n- **Version Numbers**: Exact versions for CVE lookup (e.g., Apache 2.4.49 = CVE-2021-41773 RCE)\n- **Default Credentials**: admin/admin, root/toor, sa/sa, postgres/postgres\n- **Anonymous Access**: FTP anonymous, SMB null sessions, SNMP \"public\" community\n- **Weak Encryption**: SSLv3, TLS 1.0, weak ciphers (RC4, DES, 3DES)\n- **Information Disclosure**: Detailed error messages, verbose banners, directory listings\n- **Misconfigurations**: Open relays (SMTP), recursion (DNS), guest access (SMB)\n- **Deprecated Protocols**: Telnet, FTP, SNMPv1/v2c, SMBv1\n- **Service Combinations**: SQL+Web (SQL injection), LDAP+Web (LDAP injection)\n- **Unusual Services**: Custom applications, development servers (Jenkins, GitLab)\n- **Internal Hostnames**: In SSL certificates, SMTP banners, error messages\n\nSECURITY IMPLICATIONS:\n- **SMBv1**: Vulnerable to EternalBlue (MS17-010), WannaCry, NotPetya\n- **Anonymous FTP**: Potential data leakage or malware upload point\n- **SNMP \"public\"**: Entire system configuration and secrets exposed\n- **Open LDAP**: Full Active Directory enumeration without authentication\n- **Weak SSH**: Vulnerable to brute force, man-in-the-middle attacks\n- **MySQL Root No Password**: Direct database compromise\n- **Redis No Auth**: Can achieve RCE via SLAVEOF or CONFIG SET\n- **Elasticsearch Open**: Data exfiltration, potential RCE\n- **MongoDB No Auth**: Full database access (common misconfiguration)\n\nCOMMON PITFALLS:\n- **Service Hiding Versions**: Some services obscure version info in banners\n- **Virtual Hosting**: Web servers may respond differently per vhost\n- **Load Balancers**: May distribute requests to different backends with different versions\n- **WAFs/Firewalls**: May intercept and modify service responses\n- **Rate Limiting**: Aggressive enumeration triggers IPS/IDS blocks\n- **Application Firewalls**: Block enumeration attempts (e.g., SQL commands in MySQL probe)\n- **Protocol Complexity**: Some protocols require specific handshakes (Kerberos, NTLM)\n- **Timeouts**: Services may have connection limits or timeouts\n- **Authentication Required**: Many modern services require auth before revealing info\n\nDOCUMENTATION REQUIREMENTS:\n- **Service Inventory Matrix**:\n  | Port | Protocol | Service | Version | CVEs | Risk |\n  |------|----------|---------|---------|------|------|\n  | 22 | TCP | OpenSSH | 7.4 | CVE-2021-28041 | Medium |\n  \n- Complete banner captures (screenshots or text files)\n- Configuration findings (enabled/disabled features)\n- Evidence of misconfigurations with security impact\n- Comparison against vendor best practices\n- Recommendations for service hardening\n- List of services that should be disabled or firewalled\n\nAUTOMATION AND EFFICIENCY:\n```bash\n# All-in-one enumeration script\n#!/bin/bash\nTARGET=\"target.com\"\n\n# Web services\nwhatweb $TARGET\nnikto -h $TARGET\n\n# SSH\nssh-audit $TARGET\n\n# SMB\nenum4linux-ng $TARGET -oA enum4linux_out\n\n# SNMP\nonesixtyone $TARGET\nsnmpwalk -v2c -c public $TARGET system\n\n# Comprehensive Nmap NSE\nnmap -p- -sV --script \"default and safe\" $TARGET -oA full_enum\n```\n\nTOOLS REFERENCE:\n- **Nmap NSE**: https://nmap.org/nsedoc/ (600+ scripts for all services)\n- **enum4linux-ng**: https://github.com/cddmp/enum4linux-ng (Modern SMB enumeration)\n- **ssh-audit**: https://github.com/jtesta/ssh-audit (SSH configuration auditing)\n- **testssl.sh**: https://testssl.sh/ (SSL/TLS comprehensive testing)\n- **WhatWeb**: https://github.com/urbanadventurer/WhatWeb (Web tech identification)\n- **Nikto**: https://github.com/sullo/nikto (Web server scanner)\n- **CrackMapExec**: https://github.com/byt3bl33d3r/CrackMapExec (Multi-protocol pentesting)\n- **Impacket**: https://github.com/SecureAuthCorp/impacket (Python SMB/LDAP/Kerberos)\n\nFURTHER READING:\n- OWASP WSTG-INFO-02: Fingerprint Web Server\n- NIST SP 800-115: Section 7.3 - Service Identification\n- SANS SEC560: Network Penetration Testing\n- Nmap NSE Documentation: https://nmap.org/book/nse.html\n- PTES Technical Guidelines: Section 3.4 - Service Enumeration\n- CIS Benchmarks: Hardening guides for all major services",
      "tags": ["recon", "service", "enumeration", "banner-grabbing", "fingerprinting", "nmap", "vulnerabilities"]
    },
    {
      "id": "web_technology_fingerprinting",
      "title": "Web technology fingerprinting",
      "content": "OBJECTIVE: Identify web server software, frameworks, content management systems (CMS), libraries, and underlying technologies to map the application stack and identify version-specific vulnerabilities.\n\nACADEMIC BACKGROUND:\nWeb technology fingerprinting, as defined in OWASP WSTG-INFO-02 (Fingerprint Web Server and Web Application Framework), is the systematic identification of web technologies through analysis of HTTP headers, response patterns, file structures, cookies, HTML/JavaScript signatures, and behavior patterns.\n\nThis intelligence gathering enables targeted vulnerability assessment by identifying:\n- Known CVEs for specific software versions\n- Default credentials and paths\n- Framework-specific attack vectors\n- Plugin/extension vulnerabilities\n- Technology-specific misconfigurations\n\nThe MITRE ATT&CK framework categorizes this as T1594.002 (Search Victim-Owned Websites) and T1592.002 (Gather Victim Host Information: Software), emphasizing that public-facing web infrastructure reveals significant attack surface information.\n\nAccording to the PTES (Penetration Testing Execution Standard), technology identification precedes vulnerability analysis and helps prioritize testing based on known attack patterns for identified technologies.\n\nTECHNOLOGY STACK LAYERS:\n1. **Web Server**: Apache, Nginx, IIS, LiteSpeed, Caddy\n2. **Application Server**: Tomcat, JBoss, WebLogic, Gunicorn, Passenger\n3. **Programming Language**: PHP, Python, Ruby, Java, .NET, Node.js, Go\n4. **Framework**: Laravel, Django, Flask, Ruby on Rails, Express, Spring, ASP.NET\n5. **CMS/Platform**: WordPress, Joomla, Drupal, Magento, SharePoint\n6. **Database**: MySQL, PostgreSQL, MSSQL, MongoDB, Redis (inferred)\n7. **Frontend Libraries**: React, Angular, Vue.js, jQuery, Bootstrap\n8. **CDN/WAF**: Cloudflare, Akamai, AWS CloudFront, Sucuri\n9. **Analytics/Tracking**: Google Analytics, Adobe Analytics, Hotjar\n10. **Third-Party Services**: Payment gateways, chat widgets, CRMs\n\nSTEP-BY-STEP PROCESS:\n\n1. AUTOMATED TECHNOLOGY DETECTION:\n   a) WhatWeb (Comprehensive Scanner):\n      ```bash\n      # Basic scan\n      whatweb https://target.com\n      \n      # Aggressive scan (all plugins)\n      whatweb -a 3 https://target.com\n      \n      # Verbose output with plugin details\n      whatweb -v https://target.com\n      \n      # JSON output for parsing\n      whatweb --log-json=whatweb_results.json https://target.com\n      \n      # Scan multiple URLs from file\n      whatweb -i urls.txt --log-json=results.json\n      \n      # Custom user agent\n      whatweb -U \"Mozilla/5.0\" https://target.com\n      ```\n      \n      WhatWeb identifies: Web server, CMS, JavaScript libraries, analytics, frameworks, cookies\n   \n   b) Wappalyzer (Technology Profiler):\n      ```bash\n      # CLI usage (requires npm)\n      npm install -g wappalyzer\n      wappalyzer https://target.com\n      \n      # Multiple URLs\n      wappalyzer https://target.com https://target.com/admin\n      \n      # Browser extension (Chrome/Firefox)\n      # Install from https://www.wappalyzer.com/apps/\n      ```\n      \n      Wappalyzer categories: CMS, frameworks, web servers, analytics, CDN, databases (inferred)\n   \n   c) httpx (Fast HTTP Toolkit):\n      ```bash\n      # Technology detection\n      echo \"target.com\" | httpx -tech-detect\n      \n      # With title and status code\n      echo \"target.com\" | httpx -tech-detect -title -status-code\n      \n      # Server header extraction\n      echo \"target.com\" | httpx -silent -server\n      \n      # Full headers\n      echo \"target.com\" | httpx -include-response-header\n      \n      # Multiple subdomains\n      cat subdomains.txt | httpx -tech-detect -o tech_results.txt\n      ```\n   \n   d) Nikto (Web Server Scanner):\n      ```bash\n      # Full scan with tech detection\n      nikto -h https://target.com -o nikto_report.txt\n      \n      # Faster scan (skip some checks)\n      nikto -h https://target.com -Tuning 1,2,3\n      \n      # Identify server version and components\n      nikto -h https://target.com | grep -i \"server:\"\n      ```\n   \n   e) Webtech (Lightweight Fingerprinter):\n      ```bash\n      # Install and run\n      go install github.com/ShivangiReja/webtech@latest\n      webtech -u https://target.com\n      ```\n\n2. MANUAL HTTP HEADER ANALYSIS:\n   ```bash\n   # Basic header inspection\n   curl -I https://target.com\n   \n   # Verbose connection details\n   curl -v https://target.com 2>&1 | grep -i '^< '\n   \n   # Multiple redirects follow\n   curl -IL https://target.com\n   \n   # Extract specific headers\n   curl -s -I https://target.com | grep -i server\n   curl -s -I https://target.com | grep -i x-powered-by\n   curl -s -I https://target.com | grep -i x-aspnet-version\n   \n   # All security headers\n   curl -I https://target.com | grep -iE '(X-Frame|X-XSS|X-Content|Content-Security|Strict-Transport)'\n   \n   # Using http (HTTPie)\n   http HEAD https://target.com\n   ```\n   \n   Key Headers to Analyze:\n   - **Server**: Web server type and version (e.g., \"Apache/2.4.41\")\n   - **X-Powered-By**: Backend technology (e.g., \"PHP/7.4.3\", \"Express\")\n   - **X-AspNet-Version**: .NET framework version\n   - **X-AspNetMvc-Version**: ASP.NET MVC version\n   - **X-Drupal-Cache**: Drupal CMS\n   - **X-Generator**: CMS or framework (e.g., \"Drupal 9\")\n   - **X-Redirect-By**: WordPress plugin\n   - **X-Pingback**: WordPress XML-RPC endpoint\n   - **Via**: Proxy or load balancer information\n   - **X-Varnish**: Varnish cache\n   - **CF-RAY**: Cloudflare CDN\n   - **X-Amz-Cf-Id**: AWS CloudFront\n\n3. CMS IDENTIFICATION:\n   a) WordPress Detection:\n      ```bash\n      # Check for WordPress paths\n      curl -s https://target.com/wp-login.php | grep \"WordPress\"\n      curl -s https://target.com/wp-admin/\n      curl -I https://target.com/wp-json/wp/v2/users\n      \n      # Identify WordPress version\n      curl -s https://target.com/ | grep 'content=\"WordPress'\n      curl -s https://target.com/readme.html | grep \"Version\"\n      \n      # WPScan (comprehensive WordPress scanner)\n      wpscan --url https://target.com --enumerate vp,vt,u\n      # vp = vulnerable plugins, vt = vulnerable themes, u = users\n      \n      # Enumerate plugins\n      wpscan --url https://target.com --enumerate p\n      \n      # WordPress theme detection\n      curl -s https://target.com/ | grep -i \"wp-content/themes\"\n      \n      # WordPress version from generator meta tag\n      curl -s https://target.com/ | grep -i \"<meta name=\\\"generator\\\"\"\n      \n      # Check wp-json API\n      curl -s https://target.com/wp-json/ | jq '.'\n      ```\n      \n      WordPress Indicators:\n      - /wp-admin/, /wp-content/, /wp-includes/\n      - /wp-json/wp/v2/ (REST API)\n      - /xmlrpc.php (XML-RPC endpoint)\n      - Generator meta tag\n      - wp-emoji scripts\n      - X-Redirect-By header\n   \n   b) Joomla Detection:\n      ```bash\n      # Common Joomla paths\n      curl -I https://target.com/administrator/\n      curl -s https://target.com/administrator/manifests/files/joomla.xml | grep version\n      \n      # Joomla version from XML\n      curl -s https://target.com/language/en-GB/en-GB.xml | grep version\n      \n      # JoomScan tool\n      joomscan -u https://target.com\n      \n      # Components enumeration\n      curl -s https://target.com/ | grep -i \"com_\"\n      ```\n      \n      Joomla Indicators:\n      - /administrator/ (admin panel)\n      - /components/, /modules/, /plugins/\n      - /language/en-GB/\n      - Joomla! meta generator tag\n   \n   c) Drupal Detection:\n      ```bash\n      # Drupal paths\n      curl -I https://target.com/user/login\n      curl -s https://target.com/CHANGELOG.txt | head -5\n      \n      # Drupal version\n      curl -s https://target.com/ | grep 'content=\"Drupal'\n      \n      # Droopescan (Drupal scanner)\n      droopescan scan drupal -u https://target.com\n      \n      # Check for Drupal headers\n      curl -I https://target.com | grep -i x-drupal\n      curl -I https://target.com | grep -i x-generator\n      ```\n      \n      Drupal Indicators:\n      - /user/login, /node/, /admin/\n      - CHANGELOG.txt, README.txt\n      - /sites/all/modules/, /sites/default/\n      - X-Drupal-Cache header\n      - Drupal.settings JavaScript object\n   \n   d) Magento Detection:\n      ```bash\n      # Magento paths\n      curl -I https://target.com/admin\n      curl -I https://target.com/downloader/\n      \n      # Magento version detection\n      curl -s https://target.com/magento_version\n      \n      # Magescan tool\n      magescan scan:all https://target.com\n      ```\n      \n      Magento Indicators:\n      - /skin/, /media/, /js/mage/\n      - Mage.Cookies JavaScript\n      - X-Magento-* headers\n   \n   e) SharePoint Detection:\n      ```bash\n      # SharePoint paths\n      curl -I https://target.com/_layouts/\n      curl -s https://target.com/ | grep -i \"MicrosoftSharePointTeamServices\"\n      \n      # SharePoint version\n      curl -s https://target.com/ | grep -i \"x-sharepoint\"\n      ```\n\n4. WEB SERVER FINGERPRINTING:\n   ```bash\n   # Nginx detection\n   curl -I https://target.com | grep -i nginx\n   \n   # Apache version and modules\n   curl -I https://target.com | grep -i apache\n   # Look for: Apache/2.4.41 (Ubuntu)\n   \n   # IIS version\n   curl -I https://target.com | grep -i \"Microsoft-IIS\"\n   \n   # Server misconfigurations (verbose errors)\n   curl -s https://target.com/nonexistent | grep -i \"server\\|version\\|error\"\n   \n   # Check for server tokens\n   curl -I https://target.com | grep -i \"server:\"\n   \n   # HTTP methods allowed\n   curl -X OPTIONS https://target.com -i\n   ```\n   \n   Server-Specific Files:\n   - Apache: .htaccess, /server-status, /server-info\n   - Nginx: nginx.conf (shouldn't be accessible)\n   - IIS: web.config, /trace.axd, /elmah.axd\n\n5. FRAMEWORK IDENTIFICATION:\n   a) JavaScript Frameworks (Frontend):\n      ```bash\n      # View page source for framework signatures\n      curl -s https://target.com/ | grep -iE '(react|angular|vue|ember|backbone|jquery)'\n      \n      # React detection\n      curl -s https://target.com/ | grep -i \"react\"\n      curl -s https://target.com/ | grep -i \"__REACT\"\n      \n      # Angular detection\n      curl -s https://target.com/ | grep -i \"ng-app\"\n      curl -s https://target.com/ | grep -i \"angular\"\n      \n      # Vue.js detection\n      curl -s https://target.com/ | grep -i \"vue\"\n      curl -s https://target.com/ | grep -i \"v-app\"\n      \n      # Check JavaScript files\n      curl -s https://target.com/main.js | head -20\n      \n      # Retire.js (JavaScript library vulnerability scanner)\n      retire --js --jspath https://target.com\n      ```\n   \n   b) Backend Frameworks:\n      ```bash\n      # Laravel (PHP)\n      curl -s https://target.com/ | grep -i \"laravel\"\n      curl -I https://target.com | grep -i \"laravel_session\"\n      curl -s https://target.com/.env  # Misconfiguration check\n      \n      # Django (Python)\n      curl -I https://target.com | grep -i \"csrftoken\"\n      curl -s https://target.com/admin/  # Django admin\n      \n      # Flask (Python)\n      curl -I https://target.com | grep -i \"session\"\n      \n      # Ruby on Rails\n      curl -I https://target.com | grep -i \"_rails_session\"\n      curl -s https://target.com/ | grep -i \"csrf-token\"\n      \n      # Express (Node.js)\n      curl -I https://target.com | grep -i \"express\"\n      curl -I https://target.com | grep -i \"x-powered-by: Express\"\n      \n      # Spring (Java)\n      curl -s https://target.com/ | grep -i \"spring\"\n      curl -I https://target.com/actuator/  # Spring Boot Actuator\n      \n      # ASP.NET\n      curl -I https://target.com | grep -i \"aspnet\"\n      curl -s https://target.com/ | grep -i \"__VIEWSTATE\"\n      ```\n\n6. COOKIE ANALYSIS:\n   ```bash\n   # Extract all cookies\n   curl -I https://target.com | grep -i \"set-cookie\"\n   \n   # Detailed cookie inspection\n   curl -v https://target.com 2>&1 | grep -i cookie\n   \n   # Common framework cookies:\n   # - PHPSESSID (PHP)\n   # - JSESSIONID (Java/Tomcat)\n   # - ASP.NET_SessionId (.NET)\n   # - laravel_session (Laravel)\n   # - csrftoken (Django)\n   # - connect.sid (Express)\n   # - _rails_session (Ruby on Rails)\n   # - wordpress_* (WordPress)\n   ```\n\n7. SSL/TLS CERTIFICATE ANALYSIS:\n   ```bash\n   # Certificate details\n   openssl s_client -connect target.com:443 < /dev/null 2>/dev/null | openssl x509 -text -noout\n   \n   # Issuer and organization\n   openssl s_client -connect target.com:443 < /dev/null 2>/dev/null | openssl x509 -noout -issuer -subject\n   \n   # Subject Alternative Names (internal hostnames)\n   openssl s_client -connect target.com:443 < /dev/null 2>/dev/null | openssl x509 -noout -text | grep \"DNS:\"\n   ```\n   \n   Certificate Intelligence:\n   - Organization name and location\n   - Internal domain names in SANs\n   - Certificate authority (Let's Encrypt = automated, may indicate modern stack)\n   - Validity period (short = good security practice)\n   - Wildcard certificates\n\n8. FILE AND DIRECTORY STRUCTURE ANALYSIS:\n   ```bash\n   # Common static file directories\n   curl -I https://target.com/static/\n   curl -I https://target.com/assets/\n   curl -I https://target.com/public/\n   \n   # JavaScript files\n   curl -s https://target.com/app.js | head -50\n   curl -s https://target.com/main.js | grep -i \"webpack\\|react\\|angular\\|vue\"\n   \n   # CSS files (framework detection)\n   curl -s https://target.com/style.css | grep -i \"bootstrap\\|tailwind\\|foundation\"\n   \n   # Favicon analysis (framework-specific)\n   curl -I https://target.com/favicon.ico\n   \n   # robots.txt (reveals directory structure)\n   curl -s https://target.com/robots.txt\n   \n   # sitemap.xml (reveals URLs and structure)\n   curl -s https://target.com/sitemap.xml\n   ```\n\n9. ERROR PAGE ANALYSIS:\n   ```bash\n   # Trigger 404 error\n   curl -s https://target.com/nonexistent-page-12345 | grep -i \"server\\|version\\|error\"\n   \n   # Trigger 500 error (if possible)\n   curl -s \"https://target.com/page?param=../../../etc/passwd\"\n   \n   # Check for detailed error messages (development mode)\n   # Look for stack traces, file paths, framework names\n   ```\n   \n   Framework-Specific Error Pages:\n   - Django: \"DisallowedHost\", \"OperationalError\"\n   - Laravel: \"Whoops, looks like something went wrong\"\n   - ASP.NET: \"Server Error in '/' Application\"\n   - Express: \"Cannot GET /\"\n   - Rails: \"We're sorry, but something went wrong\"\n\n10. THIRD-PARTY SERVICE IDENTIFICATION:\n    ```bash\n    # View page source for third-party scripts\n    curl -s https://target.com/ | grep -iE '(google-analytics|gtag|facebook|twitter|stripe|paypal)'\n    \n    # CDN detection\n    curl -I https://target.com | grep -i \"cf-ray\\|x-amz\\|x-cache\"\n    \n    # Analytics platforms\n    curl -s https://target.com/ | grep -i \"ga('\\|gtag(\"\n    \n    # Payment gateways\n    curl -s https://target.com/checkout | grep -iE '(stripe|paypal|square|braintree)'\n    \n    # Chat widgets\n    curl -s https://target.com/ | grep -iE '(intercom|zendesk|livechat|drift)'\n    ```\n\n11. API ENDPOINT DISCOVERY:\n    ```bash\n    # Common API paths\n    curl -I https://target.com/api/\n    curl -I https://target.com/api/v1/\n    curl -s https://target.com/api/ | jq '.'\n    \n    # GraphQL endpoints\n    curl -s https://target.com/graphql -d '{\"query\":\"{__schema{types{name}}}\"}' -H \"Content-Type: application/json\"\n    \n    # Swagger/OpenAPI documentation\n    curl -s https://target.com/api-docs\n    curl -s https://target.com/swagger.json\n    curl -s https://target.com/v2/swagger.json\n    \n    # REST API version discovery\n    for i in {1..5}; do curl -I https://target.com/api/v$i/; done\n    ```\n\nWHAT TO LOOK FOR:\n- **Outdated Versions**: PHP 5.x, jQuery < 3.0, Angular < 8, Apache < 2.4.50\n- **Development Frameworks in Production**: Flask debug mode, Django DEBUG=True, Express dev environment\n- **Verbose Error Messages**: Stack traces, file paths, database errors\n- **Version Disclosure**: Exact version numbers in headers, meta tags, or files (README.txt, CHANGELOG.txt)\n- **Default Installations**: Default favicon, unchanged admin paths, sample pages\n- **Unpatched Software**: Known CVEs for identified versions\n- **Deprecated Technologies**: Flash, Silverlight, Java applets, ActiveX\n- **Multiple Frameworks**: Mixed technology stack (PHP + Python, unusual combinations)\n- **Information Leakage**: Internal hostnames, developer comments in source, debugging endpoints\n- **CDN/WAF**: Cloudflare, Akamai (may protect against some attacks)\n\nSECURITY IMPLICATIONS:\n- **PHP < 7.4**: Multiple RCE vulnerabilities (CVE-2019-11043, CVE-2019-11041)\n- **WordPress < 5.8**: XSS, CSRF, privilege escalation vulnerabilities\n- **Drupal < 9.2**: Drupalgeddon vulnerabilities (RCE)\n- **Apache Struts**: CVE-2017-5638 (Equifax breach), multiple RCE\n- **Laravel Debug Mode**: Full environment variable disclosure (DB credentials, API keys)\n- **Django DEBUG=True**: Source code disclosure, SQL query leakage\n- **jQuery < 3.0**: XSS via $.html() and $.get()\n- **Angular < 1.6**: XSS in templates and expressions\n- **Outdated TLS**: TLS 1.0/1.1 deprecated, vulnerable to BEAST, POODLE\n- **Server Version Disclosure**: Helps attackers identify specific exploits\n\nCOMMON PITFALLS:\n- **WAF Interference**: Cloudflare/Akamai may hide real server headers\n- **Header Stripping**: Security-conscious admins disable version headers\n- **Virtual Hosting**: Different technologies per vhost/subdomain\n- **Custom Headers**: Some orgs add fake headers to mislead attackers\n- **Caching Layers**: Varnish/Redis may modify responses\n- **Microservices**: Different technologies per API endpoint\n- **False Positives**: Generic error pages don't always reveal real technology\n\nDOCUMENTATION REQUIREMENTS:\n- **Technology Matrix**:\n  | Layer | Technology | Version | CVEs | Risk |\n  |-------|------------|---------|------|------|\n  | Web Server | Nginx | 1.18.0 | CVE-2021-23017 | High |\n  | CMS | WordPress | 5.7 | Multiple XSS | Medium |\n  | Plugin | Contact Form 7 | 5.3.2 | SQL Injection | Critical |\n  \n- Screenshots of technology detection tools (WhatWeb, Wappalyzer)\n- HTTP header captures showing version disclosure\n- Evidence of identified frameworks (cookies, error pages, source code)\n- List of third-party services and integrations\n- CVE mapping for all identified versions\n- Comparison against vendor security advisories\n- Recommendations for version obfuscation and upgrades\n\nAUTOMATION SCRIPT:\n```bash\n#!/bin/bash\nTARGET=\"$1\"\n\necho \"[*] Technology Fingerprinting: $TARGET\"\necho \"\"\n\necho \"[+] WhatWeb Scan:\"\nwhatweb -a 3 \"$TARGET\"\necho \"\"\n\necho \"[+] HTTP Headers:\"\ncurl -I \"$TARGET\"\necho \"\"\n\necho \"[+] Certificate Info:\"\necho | openssl s_client -connect \"${TARGET#https://}:443\" 2>/dev/null | openssl x509 -noout -subject -issuer\necho \"\"\n\necho \"[+] CMS Detection:\"\ncurl -s \"$TARGET\" | grep -iE '(wordpress|joomla|drupal|magento)'\necho \"\"\n\necho \"[+] Framework Detection:\"\ncurl -I \"$TARGET\" | grep -iE '(x-powered-by|x-aspnet|laravel|django)'\necho \"\"\n\necho \"[+] JavaScript Frameworks:\"\ncurl -s \"$TARGET\" | grep -iE '(react|angular|vue|jquery)'\n```\n\nTOOLS REFERENCE:\n- **WhatWeb**: https://github.com/urbanadventurer/WhatWeb (Most comprehensive)\n- **Wappalyzer**: https://www.wappalyzer.com/ (Browser extension + CLI)\n- **Webanalyze**: https://github.com/rverton/webanalyze (Go-based, fast)\n- **httpx**: https://github.com/projectdiscovery/httpx (Modern HTTP toolkit)\n- **WPScan**: https://wpscan.com/ (WordPress security scanner)\n- **Joomscan**: https://github.com/OWASP/joomscan (Joomla scanner)\n- **Droopescan**: https://github.com/droope/droopescan (Drupal/SilverStripe scanner)\n- **Retire.js**: https://retirejs.github.io/retire.js/ (JavaScript library vulnerability scanner)\n- **Nikto**: https://github.com/sullo/nikto (Web server scanner)\n\nFURTHER READING:\n- OWASP WSTG-INFO-02: Fingerprint Web Server\n- OWASP WSTG-INFO-08: Fingerprint Web Application Framework\n- NIST SP 800-115: Section 7.4 - Web Application Testing\n- CWE-200: Exposure of Sensitive Information to an Unauthorized Actor\n- CAPEC-169: Footprinting\n- CVE Database: https://cve.mitre.org/ (Cross-reference versions)\n- Exploit-DB: https://www.exploit-db.com/ (Known exploits for identified software)",
      "tags": ["recon", "web", "fingerprinting", "cms", "frameworks", "headers", "technology-stack"]
    },
    {
      "id": "web_crawling_spidering",
      "title": "Web crawling and spidering",
      "content": "OBJECTIVE: Systematically map web application structure, discover all accessible pages, hidden directories, and functionality through automated crawling and directory enumeration to build a comprehensive attack surface map.\n\nACADEMIC BACKGROUND:\nWeb crawling (also called spidering) is the automated traversal of web applications following links and analyzing responses to discover all accessible content. As outlined in OWASP WSTG-INFO-05 (Review Webpage Content for Information Leakage) and WSTG-INFO-07 (Map Application Architecture), comprehensive content discovery reveals:\n- Hidden administrative interfaces\n- Backup and configuration files\n- API endpoints and documentation\n- Development/staging environments\n- Commented-out functionality\n- Forgotten test pages\n\nThe MITRE ATT&CK framework categorizes this as T1593 (Search Open Websites/Domains) under Reconnaissance, emphasizing that public-facing web content often reveals internal architecture and sensitive functionality.\n\nAccording to NIST SP 800-115, content discovery should employ both passive analysis (robots.txt, sitemaps) and active enumeration (directory brute-forcing, fuzzing) to ensure comprehensive coverage.\n\nCRAWLING METHODOLOGIES:\n1. **Passive Discovery**: robots.txt, sitemap.xml, search engine caches\n2. **Active Crawling**: Following links, parsing JavaScript, form submission\n3. **Directory Brute-forcing**: Wordlist-based path enumeration\n4. **Fuzzing**: Parameter and path mutation testing\n5. **Recursive Discovery**: Following discovered links to find more content\n\nSTEP-BY-STEP PROCESS:\n\n1. PASSIVE RECONNAISSANCE (No Direct Scanning):\n   ```bash\n   # robots.txt analysis (reveals disallowed paths)\n   curl -s https://target.com/robots.txt\n   \n   # Common robots.txt interesting entries:\n   # Disallow: /admin/\n   # Disallow: /backup/\n   # Disallow: /config/\n   # Disallow: /.git/\n   \n   # sitemap.xml parsing (complete URL structure)\n   curl -s https://target.com/sitemap.xml | grep -oP '(?<=<loc>)[^<]+'\n   \n   # sitemap_index.xml for large sites\n   curl -s https://target.com/sitemap_index.xml\n   \n   # Search engine cache exploration\n   # Google: site:target.com\n   # Bing: site:target.com\n   # Check Google cache for old/deleted pages\n   \n   # Wayback Machine (archive.org)\n   # View historical versions for removed content\n   curl -s \"http://web.archive.org/cdx/search/cdx?url=target.com/*&output=json\" | jq -r '.[] | .[2]' | sort -u\n   ```\n   \n   Intelligence: robots.txt often reveals admin panels, backup directories, and paths developers want hidden\n\n2. AUTOMATED WEB CRAWLERS (Spider):\n   a) Burp Suite Spider:\n      ```\n      1. Configure Burp Proxy (127.0.0.1:8080)\n      2. Navigate to Target → Site Map\n      3. Right-click domain → Spider this host\n      4. Configure Spider options:\n         - Check \"Crawler Settings\" → Form submission\n         - Set crawl limits (depth, threads)\n         - Configure authentication if needed\n      5. Review Site Map for discovered content\n      ```\n      \n      Advantages: Handles JavaScript, session management, form submission\n   \n   b) OWASP ZAP Spider:\n      ```bash\n      # CLI mode\n      zap-cli quick-scan -s all https://target.com\n      \n      # Traditional spider\n      zap-cli spider https://target.com\n      \n      # AJAX spider (for JavaScript-heavy apps)\n      zap-cli ajax-spider https://target.com\n      \n      # Export results\n      zap-cli report -o zap_report.html -f html\n      ```\n      \n      Advantages: Open-source, AJAX spider, automated scanning integration\n   \n   c) Hakrawler (Fast Go-based Crawler):\n      ```bash\n      # Crawl single domain\n      echo \"https://target.com\" | hakrawler\n      \n      # Crawl with depth\n      echo \"https://target.com\" | hakrawler -d 3\n      \n      # Include subdomains\n      echo \"https://target.com\" | hakrawler -subs\n      \n      # Plain URLs only (no parameters)\n      echo \"https://target.com\" | hakrawler -plain\n      \n      # Save results\n      echo \"https://target.com\" | hakrawler -d 2 > crawled_urls.txt\n      ```\n   \n   d) GoSpider (Modern Crawler):\n      ```bash\n      # Basic crawl\n      gospider -s \"https://target.com\" -o output\n      \n      # With depth and concurrency\n      gospider -s \"https://target.com\" -d 3 -c 10\n      \n      # Include subdomains\n      gospider -s \"https://target.com\" --subs\n      \n      # Follow redirects\n      gospider -s \"https://target.com\" --redirect\n      ```\n\n3. DIRECTORY AND FILE ENUMERATION (Brute-forcing):\n   a) Gobuster (Fast Directory Bruteforcer):\n      ```bash\n      # Basic directory enumeration\n      gobuster dir -u https://target.com -w /usr/share/wordlists/dirb/common.txt\n      \n      # Comprehensive with extensions\n      gobuster dir -u https://target.com -w /usr/share/seclists/Discovery/Web-Content/raft-large-directories.txt -x php,html,txt,js,bak,zip\n      \n      # With custom status codes\n      gobuster dir -u https://target.com -w wordlist.txt -s 200,204,301,302,307,401,403\n      \n      # Follow redirects\n      gobuster dir -u https://target.com -w wordlist.txt -r\n      \n      # Increase threads for speed\n      gobuster dir -u https://target.com -w wordlist.txt -t 50\n      \n      # Ignore certificate errors\n      gobuster dir -u https://target.com -w wordlist.txt -k\n      \n      # Add custom headers (auth, user-agent)\n      gobuster dir -u https://target.com -w wordlist.txt -H \"Authorization: Bearer token123\"\n      \n      # Recursive mode\n      gobuster dir -u https://target.com -w wordlist.txt --wildcard -r\n      ```\n   \n   b) Feroxbuster (Recursive Rust-based Scanner):\n      ```bash\n      # Basic scan\n      feroxbuster -u https://target.com -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt\n      \n      # Recursive with depth\n      feroxbuster -u https://target.com -w wordlist.txt -d 4\n      \n      # With extensions\n      feroxbuster -u https://target.com -w wordlist.txt -x php,html,js,txt,bak\n      \n      # Extract links from responses\n      feroxbuster -u https://target.com -w wordlist.txt --extract-links\n      \n      # High performance mode\n      feroxbuster -u https://target.com -w wordlist.txt -t 200 --rate-limit 100\n      \n      # Filter by response size\n      feroxbuster -u https://target.com -w wordlist.txt -S 1234\n      \n      # Auto-tune (adapts to server response)\n      feroxbuster -u https://target.com -w wordlist.txt --auto-tune\n      ```\n   \n   c) Dirsearch (Python Classic):\n      ```bash\n      # Basic scan\n      dirsearch -u https://target.com\n      \n      # With extensions\n      dirsearch -u https://target.com -e php,html,js,txt,zip,bak\n      \n      # Recursive\n      dirsearch -u https://target.com -r\n      \n      # Multiple URLs from file\n      dirsearch -l urls.txt\n      \n      # Custom wordlist\n      dirsearch -u https://target.com -w /path/to/wordlist.txt\n      \n      # Exclude status codes\n      dirsearch -u https://target.com -x 404,403\n      ```\n   \n   d) ffuf (Fast Fuzzer):\n      ```bash\n      # Directory fuzzing\n      ffuf -u https://target.com/FUZZ -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt\n      \n      # File fuzzing with extensions\n      ffuf -u https://target.com/FUZZ -w wordlist.txt -e .php,.html,.txt,.js,.bak\n      \n      # Recursive fuzzing\n      ffuf -u https://target.com/FUZZ -w wordlist.txt -recursion -recursion-depth 2\n      \n      # Filter by response size\n      ffuf -u https://target.com/FUZZ -w wordlist.txt -fs 4242\n      \n      # Filter by response code\n      ffuf -u https://target.com/FUZZ -w wordlist.txt -fc 404,403\n      \n      # Match regex in response\n      ffuf -u https://target.com/FUZZ -w wordlist.txt -mr \"admin\"\n      \n      # Virtual host fuzzing\n      ffuf -u https://target.com -w vhosts.txt -H \"Host: FUZZ.target.com\"\n      \n      # Multi-position fuzzing\n      ffuf -u https://target.com/FUZZ/W2 -w paths.txt:FUZZ -w files.txt:W2\n      ```\n\n4. BACKUP AND CONFIGURATION FILE DISCOVERY:\n   ```bash\n   # Common backup file patterns\n   ffuf -u https://target.com/FUZZ -w - << EOF\n   .git/\n   .git/config\n   .gitignore\n   .svn/\n   .env\n   .env.backup\n   config.php.bak\n   config.php.old\n   config.php~\n   web.config.bak\n   wp-config.php.bak\n   database.sql\n   backup.zip\n   site-backup.tar.gz\n   dump.sql\n   db_backup.sql\n   .DS_Store\n   .htaccess\n   .htpasswd\n   phpinfo.php\n   info.php\n   test.php\n   debug.php\n   console.php\n   admin.php\n   login.php.bak\n   EOF\n   \n   # Automated backup checker\n   for ext in bak old backup tmp save swp; do\n       ffuf -u https://target.com/config.php.$ext -w /dev/null\n   done\n   ```\n\n5. API ENDPOINT DISCOVERY:\n   ```bash\n   # Common API paths\n   ffuf -u https://target.com/FUZZ -w - << EOF\n   /api\n   /api/v1\n   /api/v2\n   /api/v3\n   /rest\n   /rest/v1\n   /graphql\n   /swagger\n   /swagger.json\n   /swagger-ui\n   /api-docs\n   /openapi.json\n   /v1/api-docs\n   /v2/api-docs\n   /api/swagger.json\n   /api/swagger-ui.html\n   /actuator\n   /actuator/health\n   /actuator/env\n   /health\n   /metrics\n   /docs\n   EOF\n   \n   # Kiterunner (API content discovery)\n   kr scan https://target.com -w routes-large.kite\n   \n   # Arjun (parameter discovery for APIs)\n   arjun -u https://target.com/api/users\n   ```\n\n6. JAVASCRIPT FILE ANALYSIS FOR ENDPOINTS:\n   ```bash\n   # Extract all JS files\n   echo \"https://target.com\" | hakrawler | grep -E '\\.js$' > js_files.txt\n   \n   # Download JS files\n   cat js_files.txt | while read url; do wget \"$url\"; done\n   \n   # Extract endpoints from JS (using regex)\n   grep -rEo \"['\\\"]/(api|admin|user|dashboard|config)[^'\\\"\\s]*\" *.js | sort -u\n   \n   # LinkFinder (automated endpoint extraction)\n   python3 linkfinder.py -i https://target.com/app.js -o cli\n   \n   # JSParser (comprehensive JS analysis)\n   python3 jsparser.py -u https://target.com\n   \n   # Extract API keys and secrets from JS\n   grep -rEi \"(api[_-]?key|apikey|access[_-]?token|auth[_-]?token|secret)\" *.js\n   ```\n\n7. FORM AND PARAMETER DISCOVERY:\n   ```bash\n   # ParamSpider (URL parameter collection)\n   python3 paramspider.py -d target.com -o params.txt\n   \n   # Extract unique parameters\n   cat params.txt | grep -oP '(?<=[?&])[^=&]+' | sort -u > unique_params.txt\n   \n   # Arjun (hidden parameter discovery)\n   arjun -u https://target.com/search\n   \n   # Burp Param Miner extension\n   # Install via Burp Extender, right-click request → \"Guess params\"\n   ```\n\n8. RECURSIVE AND COMPREHENSIVE DISCOVERY:\n   ```bash\n   # Multi-tool pipeline\n   #!/bin/bash\n   TARGET=\"https://target.com\"\n   \n   # Stage 1: Initial crawl\n   echo \"[*] Stage 1: Crawling...\"\n   gospider -s \"$TARGET\" -d 3 --subs -o crawl_output\n   \n   # Stage 2: Extract URLs\n   cat crawl_output/*.txt | grep -Eo 'https?://[^ ]+' | sort -u > all_urls.txt\n   \n   # Stage 3: Directory enumeration on discovered paths\n   echo \"[*] Stage 2: Directory enumeration...\"\n   feroxbuster -u \"$TARGET\" -w /usr/share/seclists/Discovery/Web-Content/raft-large-directories.txt -x php,html,js,txt --extract-links -o ferox_results.txt\n   \n   # Stage 4: Parameter discovery\n   echo \"[*] Stage 3: Parameter discovery...\"\n   python3 paramspider.py -d target.com -o params.txt\n   \n   # Stage 5: JS endpoint extraction\n   echo \"[*] Stage 4: JS analysis...\"\n   cat all_urls.txt | grep '\\.js$' | while read jsurl; do\n       python3 linkfinder.py -i \"$jsurl\" -o cli\n   done > endpoints_from_js.txt\n   \n   echo \"[*] Discovery complete! Results in all_urls.txt, ferox_results.txt, params.txt, endpoints_from_js.txt\"\n   ```\n\nWHAT TO LOOK FOR:\n- **Admin Interfaces**: /admin/, /administrator/, /manage/, /cpanel/, /dashboard/\n- **Authentication Pages**: /login, /signin, /auth, /sso\n- **API Documentation**: /api-docs, /swagger, /graphql, /openapi.json\n- **Development/Staging**: /dev/, /test/, /staging/, /qa/\n- **Backup Files**: *.bak, *.old, *.backup, *.tmp, *.swp, *~\n- **Configuration Files**: .env, config.php, web.config, application.properties\n- **Source Control**: .git/, .svn/, .hg/\n- **Database Dumps**: *.sql, dump.sql, backup.sql\n- **Error Pages**: Custom 404/500 pages that leak information\n- **File Uploads**: /uploads/, /files/, /media/, /attachments/\n- **Hidden Functionality**: Commented-out links in HTML source\n- **Monitoring Endpoints**: /health, /metrics, /status, /actuator/\n- **Debug Interfaces**: /debug/, /console/, /phpinfo.php\n- **Legacy Content**: Old versions, deprecated features\n\nSECURITY IMPLICATIONS:\n- **Exposed Admin Panels**: Direct access to management interfaces\n- **.git/ Directory**: Full source code disclosure via `git-dumper`\n- **.env Files**: Database credentials, API keys, secrets\n- **Backup Files**: Old configurations with default credentials\n- **API Documentation**: Reveals all endpoints and parameters\n- **Development Directories**: Often less secure, debug mode enabled\n- **phpinfo() Pages**: Full PHP configuration disclosure\n- **Database Dumps**: Complete data exfiltration\n- **File Upload Directories**: May allow direct access to uploaded files\n- **Comments in HTML**: Reveal internal infrastructure, IPs, hostnames\n\nCOMMON PITFALLS:\n- **WAF Blocking**: Aggressive scanning triggers IP blocks\n- **Rate Limiting**: Slow down scanning or use rotating proxies\n- **False Positives**: 200 OK responses may be custom 404 pages (wildcard DNS)\n- **JavaScript-Heavy SPAs**: Standard crawlers miss dynamically loaded content\n- **Authentication Required**: Some paths only accessible when logged in\n- **Virtual Hosting**: Different content per Host header\n- **Load Balancers**: May distribute requests to different backends\n- **Recursive Scanning Loops**: Limit recursion depth to avoid infinite loops\n- **Large Wordlists**: Balance coverage vs. scan time (start with top-1000)\n- **Client-Side Routing**: React/Angular apps use hash or history routing\n\nDOCUMENTATION REQUIREMENTS:\n- Complete site map with all discovered URLs\n- Directory structure tree showing hierarchy\n- List of interesting files and their locations\n- API endpoint inventory with methods and parameters\n- Screenshots of discovered admin/debug interfaces\n- Evidence of exposed sensitive files\n- Notes on authentication requirements per path\n- Parameter lists for all discovered endpoints\n- Recommendations for removing/securing exposed content\n\nOPTIMIZED WORDLISTS:\n- **Small (fast)**: /usr/share/seclists/Discovery/Web-Content/common.txt (~4k entries)\n- **Medium**: /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt (~30k)\n- **Large (comprehensive)**: /usr/share/seclists/Discovery/Web-Content/directory-list-2.3-medium.txt (~220k)\n- **Technology-specific**: /usr/share/seclists/Discovery/Web-Content/CMS/ (WordPress, Joomla, etc.)\n- **API-focused**: /usr/share/seclists/Discovery/Web-Content/api/ (common API paths)\n\nTOOLS REFERENCE:\n- **Burp Suite**: https://portswigger.net/burp (Industry standard spider + fuzzer)\n- **OWASP ZAP**: https://www.zaproxy.org/ (Open-source security scanner)\n- **Gobuster**: https://github.com/OJ/gobuster (Fast directory bruteforcer)\n- **Feroxbuster**: https://github.com/epi052/feroxbuster (Modern recursive scanner)\n- **ffuf**: https://github.com/ffuf/ffuf (Fast web fuzzer)\n- **Dirsearch**: https://github.com/maurosoria/dirsearch (Python directory scanner)\n- **Hakrawler**: https://github.com/hakluke/hakrawler (Fast web crawler)\n- **GoSpider**: https://github.com/jaeles-project/gospider (Fast spider with JS parsing)\n- **LinkFinder**: https://github.com/GerbenJavado/LinkFinder (Extract endpoints from JS)\n- **ParamSpider**: https://github.com/devanshbatham/ParamSpider (Parameter discovery)\n- **Arjun**: https://github.com/s0md3v/Arjun (HTTP parameter discovery)\n\nFURTHER READING:\n- OWASP WSTG-INFO-05: Review Webpage Content for Information Leakage\n- OWASP WSTG-INFO-07: Map Application Architecture\n- OWASP WSTG-CONF-05: Enumerate Infrastructure and Application Admin Interfaces\n- NIST SP 800-115: Section 7.4 - Web Application Testing",
      "tags": ["recon", "web", "crawling", "spidering", "directory-enumeration", "content-discovery", "fuzzing"]
    },
    {
      "id": "infrastructure_mapping",
      "title": "Infrastructure mapping",
      "content": "OBJECTIVE: Map the complete network infrastructure, topology, and architecture to understand organizational structure, identify key network assets, and discover potential attack paths through infrastructure relationships.\n\nACADEMIC BACKGROUND:\nInfrastructure mapping is the systematic discovery and documentation of an organization's network architecture, systems, and relationships. As defined in NIST SP 800-115 (Technical Guide to Information Security Testing and Assessment), infrastructure mapping provides the foundation for understanding attack surfaces and potential vulnerabilities.\n\nThe MITRE ATT&CK framework categorizes infrastructure reconnaissance as T1590 (Gather Victim Network Information) and T1595 (Active Scanning), emphasizing that understanding network topology is crucial for effective penetration testing.\n\nAccording to OWASP Testing Guide, infrastructure mapping should reveal:\n- Network boundaries and segmentation\n- Cloud vs on-premise infrastructure\n- Third-party service dependencies\n- Redundancy and failover mechanisms\n- Attack path identification\n\nMETHODOLOGIES:\n1. **Passive Infrastructure Discovery**: BGP analysis, DNS enumeration, certificate transparency\n2. **Active Network Mapping**: Tracerouting, port scanning, service enumeration\n3. **Cloud Infrastructure Analysis**: AWS/Azure/GCP asset discovery\n4. **Third-party Dependency Mapping**: CDN, WAF, hosting providers\n5. **Network Topology Visualization**: Understanding connectivity and relationships\n\nSTEP-BY-STEP PROCESS:\n\n1. AUTONOMOUS SYSTEM (AS) AND BGP ANALYSIS:\n   ```bash\n   # Find organization's AS number using whois\n   whois -h whois.radb.net target.com | grep -i origin\n   \n   # Alternative: Use RIPE database\n   whois -h whois.ripe.net target.com | grep -i \"origin\\|aut-num\"\n   \n   # ARIN database for North American organizations\n   whois -h whois.arin.net target.com\n   \n   # Get all IP ranges announced by an AS\n   whois -h whois.radb.net -- '-i origin AS12345' | grep -E \"^route:\"\n   \n   # BGP toolkit for comprehensive AS analysis\n   curl -s \"https://bgp.he.net/AS12345\" | grep -oP '\\d+\\.\\d+\\.\\d+\\.\\d+/\\d+'\n   \n   # Check BGP peering relationships\n   whois -h whois.radb.net -- '-i origin AS12345' | grep -i \"import\\|export\"\n   \n   # Find related AS numbers (same organization)\n   whois -h whois.radb.net -- '-i origin AS12345' | grep -oP 'AS\\d+' | sort -u\n   \n   # BGP route visualization\n   curl -s \"https://bgp.he.net/AS12345#_graph4\" | grep -A 20 \"graph\"\n   ```\n   \n   Intelligence: AS numbers reveal organizational relationships, geographic presence, and network size\n\n2. NETWORK TOPOLOGY MAPPING:\n   ```bash\n   # Standard traceroute (ICMP)\n   traceroute -I target.com\n   \n   # MTR (My Traceroute) - better than traceroute\n   mtr --report --report-wide target.com\n   \n   # Paris traceroute (avoids load balancing issues)\n   paris-traceroute target.com\n   \n   # TCP traceroute (when ICMP blocked)\n   tcptraceroute target.com 443\n   \n   # UDP traceroute\n   traceroute -U target.com\n   \n   # ICMP traceroute with specific TTL\n   traceroute -m 20 target.com\n   \n   # Multiple traceroutes from different locations (using VPN/proxies)\n   # Compare results to identify load balancing and geographic distribution\n   \n   # Pathping (Windows equivalent)\n   pathping target.com\n   ```\n   \n   Intelligence: Network hops reveal ISP relationships, geographic routing, and potential firewall locations\n\n3. CDN AND WAF DETECTION:\n   ```bash\n   # Check HTTP headers for CDN/WAF indicators\n   curl -I https://target.com | grep -iE '(cf-ray|x-amz|x-cache|server|x-sucuri|x-waf|akamai)'\n   \n   # DNS resolution to identify CDN\n   dig target.com | grep -A2 'ANSWER SECTION'\n   \n   # CNAME chain analysis\n   dig CNAME target.com\n   \n   # WAF detection with wafw00f\n   wafw00f https://target.com\n   \n   # Nmap WAF detection script\n   nmap --script=http-waf-detect target.com\n   \n   # WhatWaf (advanced WAF fingerprinting)\n   python3 whatwaf -u https://target.com\n   \n   # CDN identification tools\n   # CDNPlanet: curl \"https://www.cdnplanet.com/tools/cdnfinder/?url=target.com\"\n   # CDN Finder: python3 cdn-finder.py -d target.com\n   ```\n   \n   Intelligence: CDN providers include Cloudflare, Akamai, Fastly, CloudFront, etc.\n\n4. CLOUD INFRASTRUCTURE ANALYSIS:\n   ```bash\n   # AWS S3 bucket enumeration\n   # Common patterns: companyname.s3.amazonaws.com, s3.amazonaws.com/companyname\n   aws s3 ls s3://companyname-bucket/ --no-sign-request 2>/dev/null || echo \"Bucket not public\"\n   \n   # Azure storage enumeration\n   curl -I https://companyname.blob.core.windows.net/container/file.txt\n   \n   # GCP storage enumeration\n   curl -I https://storage.googleapis.com/companyname-bucket/file.txt\n   \n   # DigitalOcean Spaces\n   curl -I https://companyname.nyc3.digitaloceanspaces.com/file.txt\n   \n   # Cloud metadata enumeration (if accessible)\n   curl http://169.254.169.254/latest/meta-data/  # AWS\n   curl -H Metadata:true \"http://169.254.169.254/metadata/instance?api-version=2021-02-01\"  # Azure\n   ```\n\n5. THIRD-PARTY SERVICE DEPENDENCY MAPPING:\n   ```bash\n   # Email service providers\n   dig MX target.com | grep -v \";\"\n   \n   # DNS hosting providers\n   dig NS target.com | grep -v \";\"\n   \n   # SSL certificate analysis\n   openssl s_client -connect target.com:443 -servername target.com 2>/dev/null | openssl x509 -text | grep -A5 \"Subject:\" | grep -E \"(O=|CN=)\"\n   \n   # Certificate transparency logs\n   curl -s \"https://crt.sh/?q=target.com&output=json\" | jq -r '.[].name_value' | sort -u\n   \n   # Subdomain enumeration for third-party services\n   # Look for: mail.target.com, api.target.com, dev.target.com, staging.target.com\n   ```\n\n6. NETWORK BOUNDARY IDENTIFICATION:\n   ```bash\n   # Identify IP ranges using BGP data\n   whois -h whois.radb.net -- '-i origin AS12345' | grep \"route:\" | sort -u\n   \n   # Reverse DNS lookups for IP ranges\n   for ip in $(seq 1 254); do dig -x 192.168.1.$ip +short; done\n   \n   # Network scanning for boundary identification\n   nmap -sn 192.168.1.0/24  # ARP scan for local network\n   \n   # External boundary scanning (ethical considerations apply)\n   # Use passive techniques: Shodan, Censys, etc.\n   ```\n\n7. INFRASTRUCTURE VISUALIZATION:\n   ```bash\n   # Create network topology diagram\n   # Tools: draw.io, Lucidchart, or ASCII art\n   \n   # Document findings in structured format:\n   # - Public IP ranges\n   # - AS numbers and relationships\n   # - CDN/WAF providers\n   # - Cloud services identified\n   # - Third-party dependencies\n   # - Network topology hops\n   # - Geographic distribution\n   ```\n\nWHAT TO LOOK FOR:\n- **Network Boundaries**: IP ranges, AS numbers, geographic distribution\n- **Cloud Infrastructure**: AWS/Azure/GCP regions, storage buckets, cloud services\n- **CDN Providers**: Cloudflare, Akamai, Fastly, CloudFront, Imperva\n- **WAF Solutions**: ModSecurity, Cloudflare WAF, Akamai Kona, Imperva\n- **Load Balancers**: F5, Citrix, AWS ELB, Azure Load Balancer\n- **Third-party Services**: Email (Google Workspace, Office 365), DNS hosting, SSL certificates\n- **Redundancy**: Multiple data centers, failover configurations\n- **Attack Vectors**: Exposed services, misconfigured cloud storage, weak third-party dependencies\n\nSECURITY IMPLICATIONS:\n- **Cloud Misconfigurations**: Public S3 buckets, open blob storage, exposed metadata services\n- **CDN Bypass Opportunities**: Origin server direct access, misconfigured CDN rules\n- **Third-party Compromise**: Weak email security, vulnerable DNS hosting\n- **Network Segmentation Issues**: Flat networks, missing VLANs, improper firewall rules\n- **Geographic Attack Paths**: Multi-region deployments create complex attack surfaces\n- **Supply Chain Risks**: Third-party service dependencies introduce indirect vulnerabilities\n\nCOMMON PITFALLS:\n- **Traceroute Blocking**: Modern firewalls block ICMP, use TCP/UDP alternatives\n- **Load Balancing Evasion**: Single traceroute may not show full topology\n- **Cloud Dynamic IPs**: Infrastructure changes frequently in cloud environments\n- **CDN Obfuscation**: Origin servers hidden behind CDN layers\n- **Third-party Blind Spots**: Dependencies not visible in direct reconnaissance\n- **Regulatory Constraints**: Active scanning may violate terms of service\n- **Outdated Information**: BGP data and whois records may be stale\n- **Geographic Bias**: Reconnaissance from single location misses regional differences\n\nDOCUMENTATION REQUIREMENTS:\n- Complete AS and IP range inventory\n- Network topology diagrams with hop analysis\n- CDN/WAF provider identification and configuration notes\n- Cloud infrastructure mapping (regions, services, storage)\n- Third-party service dependency list\n- Geographic distribution analysis\n- Attack path identification and prioritization\n- Recommendations for infrastructure hardening\n\nTOOLS REFERENCE:\n- **BGP Analysis**: bgp.he.net, whois.radb.net, RIPE/ARIN databases\n- **Traceroute Tools**: mtr, paris-traceroute, tcptraceroute\n- **CDN/WAF Detection**: wafw00f, WhatWaf, Nmap scripts\n- **Cloud Enumeration**: awscli, az cli, gcloud, cloud_enum\n- **Network Mapping**: Nmap, Masscan, ZMap (passive)\n- **Visualization**: draw.io, Graphviz, Maltego\n\nFURTHER READING:\n- NIST SP 800-115: Technical Guide to Information Security Testing and Assessment\n- MITRE ATT&CK: T1590 Gather Victim Network Information\n- OWASP Testing Guide: Infrastructure Analysis\n- BGP Best Practices: RFC 7454, RFC 8210\n- Cloud Security Alliance: Cloud Infrastructure Security Guidance",
      "tags": ["recon", "infrastructure", "network", "topology", "bgp", "cdn", "waf", "cloud", "asn", "traceroute"]
    },
    {
      "id": "tls_ssl_assessment",
      "title": "TLS/SSL assessment",
      "content": "OBJECTIVE: Comprehensively evaluate SSL/TLS configurations, certificate validity, cipher suite strength, and protocol vulnerabilities to identify encryption weaknesses and potential man-in-the-middle attack vectors.\n\nACADEMIC BACKGROUND:\nTransport Layer Security (TLS) and its predecessor SSL are cryptographic protocols that provide secure communications over networks. According to OWASP WSTG-CRYP-01 (Testing for Weak Transport Layer Security), improper TLS configuration is one of the most common security issues affecting web applications.\n\nThe NIST SP 800-52 Rev.2 \"Guidelines for the Selection, Configuration, and Use of TLS\" mandates:\n- TLS 1.2 or higher (TLS 1.3 preferred)\n- Strong cipher suites with forward secrecy\n- Valid certificates from trusted Certificate Authorities\n- Proper certificate validation and hostname verification\n\nThe MITRE ATT&CK framework identifies improper TLS configuration under T1040 (Network Sniffing) and T1557 (Adversary-in-the-Middle), as weak cryptography enables interception of sensitive communications.\n\nCRITICAL TLS VULNERABILITIES:\n- **Heartbleed (CVE-2014-0160)**: OpenSSL memory disclosure\n- **POODLE (CVE-2014-3566)**: SSLv3 padding oracle\n- **BEAST (CVE-2011-3389)**: TLS 1.0 CBC cipher attack\n- **CRIME (CVE-2012-4929)**: TLS compression attack\n- **FREAK (CVE-2015-0204)**: Export cipher downgrade\n- **Logjam (CVE-2015-4000)**: Diffie-Hellman downgrade\n- **DROWN (CVE-2016-0800)**: SSLv2 cross-protocol attack\n- **SWEET32 (CVE-2016-2183)**: 64-bit block cipher attack\n\nSTEP-BY-STEP PROCESS:\n\n1. CERTIFICATE INSPECTION AND VALIDATION:\n   a) Basic Certificate Retrieval:\n      ```bash\n      # Retrieve certificate from server\n      openssl s_client -connect target.com:443 -servername target.com < /dev/null 2>/dev/null | openssl x509 -text -noout\n      \n      # Save certificate to file\n      echo | openssl s_client -connect target.com:443 -servername target.com 2>/dev/null | openssl x509 > target.crt\n      \n      # View certificate details\n      openssl x509 -in target.crt -text -noout\n      \n      # Check certificate expiration\n      openssl x509 -in target.crt -noout -dates\n      \n      # Extract subject and issuer\n      openssl x509 -in target.crt -noout -subject -issuer\n      \n      # Check certificate fingerprint (SHA256)\n      openssl x509 -in target.crt -noout -fingerprint -sha256\n      ```\n   \n   b) Certificate Chain Verification:\n      ```bash\n      # Verify certificate chain\n      openssl s_client -connect target.com:443 -showcerts\n      \n      # Verify against system CA bundle\n      openssl verify target.crt\n      \n      # Verify with specific CA file\n      openssl verify -CAfile ca-bundle.crt target.crt\n      \n      # Check certificate chain completeness\n      openssl s_client -connect target.com:443 -servername target.com -showcerts 2>/dev/null | grep -E '(BEGIN CERTIFICATE|END CERTIFICATE|subject=|issuer=)'\n      ```\n   \n   c) Subject Alternative Names (SAN) Analysis:\n      ```bash\n      # Extract all SANs (reveals internal domains)\n      openssl x509 -in target.crt -noout -text | grep -A1 'Subject Alternative Name'\n      \n      # Parse SANs to list\n      openssl x509 -in target.crt -noout -text | grep -oP 'DNS:\\K[^,]+'\n      \n      # Check for wildcard certificates\n      openssl x509 -in target.crt -noout -subject | grep -o '\\*\\.'\n      ```\n      \n      Intelligence gathering: Internal hostnames in SANs, infrastructure naming conventions, wildcard usage patterns, multiple domains on same certificate\n\n2. COMPREHENSIVE TLS CONFIGURATION SCANNING:\n   a) SSLScan (Fast Basic Analysis):\n      ```bash\n      # Basic SSL/TLS scan\n      sslscan target.com\n      \n      # IPv6 scan\n      sslscan --ipv6 target.com\n      \n      # Specify port\n      sslscan target.com:8443\n      \n      # XML output for parsing\n      sslscan --xml=sslscan_results.xml target.com\n      ```\n      \n      Key findings: Supported TLS versions, accepted cipher suites, certificate details, TLS compression status\n   \n   b) testssl.sh (Most Comprehensive):\n      ```bash\n      # Full comprehensive scan\n      ./testssl.sh target.com\n      \n      # Fast scan (basic checks)\n      ./testssl.sh --fast target.com\n      \n      # Check specific vulnerabilities\n      ./testssl.sh --vulnerable target.com\n      \n      # Check only protocol support\n      ./testssl.sh --protocols target.com\n      \n      # Check cipher suites\n      ./testssl.sh --ciphers target.com\n      \n      # Check certificate\n      ./testssl.sh --server-defaults target.com\n      \n      # JSON output\n      ./testssl.sh --jsonfile results.json target.com\n      \n      # HTML report\n      ./testssl.sh --htmlfile report.html target.com\n      \n      # Scan multiple hosts\n      ./testssl.sh --file hosts.txt\n      \n      # Parallel scanning (4 connections)\n      ./testssl.sh --parallel target.com\n      ```\n      \n      testssl.sh checks: All TLS vulnerabilities (Heartbleed, POODLE, BEAST, CRIME, etc.), protocol versions (SSLv2, SSLv3, TLS 1.0-1.3), cipher suite strength and order, forward secrecy support, certificate validity and trust chain, HSTS, HPKP headers, certificate transparency compliance\n   \n   c) Nmap SSL Scripts:\n      ```bash\n      # SSL enum ciphers\n      nmap --script ssl-enum-ciphers -p 443 target.com\n      \n      # Check all SSL vulnerabilities\n      nmap --script ssl-* -p 443 target.com\n      \n      # Specific vulnerability checks\n      nmap --script ssl-heartbleed -p 443 target.com\n      nmap --script ssl-poodle -p 443 target.com\n      nmap --script ssl-dh-params -p 443 target.com\n      \n      # Certificate information\n      nmap --script ssl-cert -p 443 target.com\n      \n      # Check for weak cipher suites\n      nmap --script ssl-known-key -p 443 target.com\n      ```\n   \n   d) sslyze (Python-based Analysis):\n      ```bash\n      # Comprehensive scan\n      sslyze target.com\n      \n      # Check specific vulnerability\n      sslyze --heartbleed target.com\n      \n      # Certificate info\n      sslyze --certinfo target.com\n      \n      # Check cipher suites\n      sslyze --sslv2 --sslv3 --tlsv1 --tlsv1_1 --tlsv1_2 --tlsv1_3 target.com\n      \n      # JSON output\n      sslyze --json_out=results.json target.com\n      ```\n\n3. PROTOCOL VERSION TESTING:\n   ```bash\n   # Test SSLv2 (should fail - deprecated since 2011)\n   openssl s_client -connect target.com:443 -ssl2\n   \n   # Test SSLv3 (should fail - deprecated since 2015)\n   openssl s_client -connect target.com:443 -ssl3\n   \n   # Test TLS 1.0 (should fail - deprecated since 2020)\n   openssl s_client -connect target.com:443 -tls1\n   \n   # Test TLS 1.1 (should fail - deprecated since 2020)\n   openssl s_client -connect target.com:443 -tls1_1\n   \n   # Test TLS 1.2 (should succeed - minimum requirement)\n   openssl s_client -connect target.com:443 -tls1_2\n   \n   # Test TLS 1.3 (should succeed - current standard)\n   openssl s_client -connect target.com:443 -tls1_3\n   \n   # Check protocol support summary\n   for version in ssl2 ssl3 tls1 tls1_1 tls1_2 tls1_3; do\n       echo -n \"Testing $version: \"\n       timeout 2 openssl s_client -connect target.com:443 -$version < /dev/null 2>&1 | grep -q 'Cipher' && echo \"SUPPORTED\" || echo \"Not supported\"\n   done\n   ```\n\n4. CIPHER SUITE ANALYSIS:\n   ```bash\n   # List all accepted ciphers\n   nmap --script ssl-enum-ciphers -p 443 target.com\n   \n   # Test specific cipher\n   openssl s_client -connect target.com:443 -cipher 'AES128-SHA'\n   \n   # Check for weak ciphers (NULL, EXPORT, DES, RC4, MD5)\n   ./testssl.sh --ciphers target.com | grep -iE '(null|export|des|rc4|md5|weak)'\n   \n   # Check cipher order (server vs client preference)\n   ./testssl.sh --server-preference target.com\n   \n   # Verify forward secrecy\n   ./testssl.sh --fs target.com\n   ```\n   \n   Cipher Suite Strength:\n   - **Weak**: DES, 3DES, RC4, MD5, NULL, EXPORT, ANON\n   - **Medium**: AES-CBC without forward secrecy\n   - **Strong**: AES-GCM, ChaCha20-Poly1305 with ECDHE/DHE\n   - **Modern**: TLS 1.3 cipher suites (AES-GCM, ChaCha20)\n\n5. VULNERABILITY-SPECIFIC TESTING:\n   a) Heartbleed (CVE-2014-0160):\n      ```bash\n      # Nmap check\n      nmap -p 443 --script ssl-heartbleed target.com\n      \n      # testssl.sh check\n      ./testssl.sh -H target.com\n      \n      # Manual check with python script\n      python heartbleed-poc.py target.com 443\n      ```\n   \n   b) POODLE (CVE-2014-3566):\n      ```bash\n      # Check SSLv3 support\n      nmap -p 443 --script ssl-poodle target.com\n      \n      # testssl.sh check\n      ./testssl.sh -O target.com\n      ```\n   \n   c) BEAST (CVE-2011-3389):\n      ```bash\n      # Check TLS 1.0 CBC ciphers\n      nmap -p 443 --script ssl-enum-ciphers target.com | grep -A20 'TLSv1.0' | grep CBC\n      \n      # testssl.sh check\n      ./testssl.sh -B target.com\n      ```\n   \n   d) CRIME (CVE-2012-4929):\n      ```bash\n      # Check TLS compression\n      nmap -p 443 --script ssl-enum-ciphers target.com | grep -i compression\n      \n      # testssl.sh check\n      ./testssl.sh -C target.com\n      ```\n   \n   e) FREAK (CVE-2015-0204):\n      ```bash\n      # Check for EXPORT ciphers\n      nmap -p 443 --script ssl-enum-ciphers target.com | grep -i export\n      \n      # testssl.sh check\n      ./testssl.sh -F target.com\n      ```\n   \n   f) Logjam (CVE-2015-4000):\n      ```bash\n      # Check DH parameters\n      nmap -p 443 --script ssl-dh-params target.com\n      \n      # testssl.sh check\n      ./testssl.sh -J target.com\n      ```\n   \n   g) DROWN (CVE-2016-0800):\n      ```bash\n      # Check SSLv2 support\n      ./testssl.sh -D target.com\n      ```\n   \n   h) SWEET32 (CVE-2016-2183):\n      ```bash\n      # Check for 64-bit block ciphers (3DES, DES, Blowfish)\n      ./testssl.sh --sweet32 target.com\n      ```\n\n6. CERTIFICATE TRANSPARENCY AND MONITORING:\n   ```bash\n   # Check Certificate Transparency logs\n   curl -s \"https://crt.sh/?q=%.target.com&output=json\" | jq -r '.[].name_value' | sort -u\n   \n   # Verify CT compliance\n   ./testssl.sh --ct target.com\n   \n   # Check for certificate issuance history\n   curl -s \"https://crt.sh/?q=target.com&output=json\" | jq -r '.[] | \"\\(.not_before) - \\(.issuer_name)\"'\n   ```\n\n7. HTTP SECURITY HEADERS RELATED TO TLS:\n   ```bash\n   # Check HSTS (HTTP Strict Transport Security)\n   curl -I https://target.com | grep -i strict-transport-security\n   \n   # Check HSTS with testssl.sh\n   ./testssl.sh --headers target.com | grep -i HSTS\n   \n   # Check for HSTS preload eligibility\n   curl -s https://hstspreload.org/api/v2/status?domain=target.com | jq\n   \n   # Verify HPKP (deprecated but may exist)\n   curl -I https://target.com | grep -i public-key-pins\n   ```\n\n8. CERTIFICATE REVOCATION CHECKING:\n   ```bash\n   # Check OCSP (Online Certificate Status Protocol)\n   openssl ocsp -issuer ca.crt -cert target.crt -url http://ocsp.server.com -resp_text\n   \n   # Check CRL (Certificate Revocation List)\n   openssl x509 -in target.crt -noout -text | grep -A4 'CRL Distribution'\n   \n   # Verify OCSP stapling\n   openssl s_client -connect target.com:443 -status -servername target.com < /dev/null 2>&1 | grep -A10 'OCSP'\n   ```\n\nWHAT TO LOOK FOR:\n- **Deprecated Protocols**: SSLv2, SSLv3, TLS 1.0, TLS 1.1 (all deprecated)\n- **Weak Ciphers**: DES, 3DES, RC4, MD5-based, NULL, EXPORT, ANON\n- **Missing Forward Secrecy**: Ciphers without DHE or ECDHE\n- **Self-Signed Certificates**: In production environments\n- **Expired Certificates**: Past validity period\n- **Certificate Mismatch**: Domain name doesn't match certificate CN/SAN\n- **Incomplete Chain**: Missing intermediate certificates\n- **Weak Key Length**: RSA < 2048 bits, ECDSA < 256 bits\n- **Untrusted CA**: Certificate signed by unknown/untrusted authority\n- **Missing HSTS**: No Strict-Transport-Security header\n- **Compression Enabled**: TLS compression (CRIME vulnerability)\n- **Known Vulnerabilities**: Heartbleed, POODLE, BEAST, FREAK, Logjam, DROWN\n\nSECURITY IMPLICATIONS:\n- **SSLv2/SSLv3**: Completely broken, enables DROWN and POODLE attacks\n- **TLS 1.0/1.1**: Vulnerable to BEAST, deprecated by major browsers\n- **Weak Ciphers**: Allow brute-force or cryptanalytic attacks\n- **No Forward Secrecy**: Past communications can be decrypted if private key compromised\n- **Heartbleed**: Memory disclosure, can leak private keys and session data\n- **POODLE**: Padding oracle attack, plaintext recovery\n- **Self-Signed Certs**: Enable man-in-the-middle attacks\n- **Expired Certs**: Browser warnings, user trust issues\n- **Missing HSTS**: Allows SSL stripping attacks\n- **Known Vulnerabilities**: Multiple critical CVEs affecting encryption\n\nCOMMON PITFALLS:\n- **Internal Services**: May legitimately use self-signed certificates\n- **Legacy System Support**: Some old systems require TLS 1.0 for compatibility\n- **Load Balancer Termination**: TLS terminated at load balancer, backend may be HTTP\n- **Certificate Pinning**: Can break with legitimate certificate renewals\n- **Multiple Virtual Hosts**: Different certificates per domain on same IP\n- **CDN/WAF**: May have different TLS config than origin server\n- **Port Variations**: Different TLS configs on non-standard ports (8443, 8080)\n- **False Positives**: Some scanners report issues not applicable to specific scenarios\n- **SNI Requirements**: Server Name Indication needed for virtual hosting\n\nDOCUMENTATION REQUIREMENTS:\n- **TLS Configuration Matrix**:\n  | Protocol | Status | Cipher Suites | Vulnerabilities |\n  |----------|--------|---------------|-----------------|\n  | TLS 1.3 | Enabled | AES-GCM, ChaCha20 | None |\n  | TLS 1.2 | Enabled | AES-GCM, ECDHE | None |\n  | TLS 1.1 | Disabled | N/A | BEAST |\n\n- Certificate details (issuer, expiration, SANs, key length)\n- Vulnerability scan results (Heartbleed, POODLE, etc.)\n- Cipher suite strength analysis\n- Forward secrecy support status\n- HSTS configuration and preload status\n- Evidence screenshots of configuration weaknesses\n- Comparison against NIST/Mozilla guidelines\n- Recommendations for TLS hardening\n\nCOMPLIANCE REFERENCES:\n- **PCI DSS 3.2.1**: Requires TLS 1.2+ for payment card data\n- **NIST SP 800-52 Rev.2**: Federal TLS configuration guidelines\n- **Mozilla SSL Configuration**: https://ssl-config.mozilla.org/ (Modern/Intermediate/Old profiles)\n- **FIPS 140-2**: Cryptographic module validation\n- **HIPAA**: Strong encryption for health data\n- **GDPR**: Encryption as privacy safeguard\n\nTOOLS REFERENCE:\n- **testssl.sh**: https://testssl.sh/ (Most comprehensive CLI scanner)\n- **SSLScan**: https://github.com/rbsec/sslscan (Fast basic scanner)\n- **sslyze**: https://github.com/nabla-c0d3/sslyze (Python-based analysis)\n- **Nmap SSL Scripts**: https://nmap.org/nsedoc/categories/ssl.html (Built-in to Nmap)\n- **SSL Labs**: https://www.ssllabs.com/ssltest/ (Online comprehensive testing)\n- **Certificate Transparency**: https://crt.sh/ (Certificate search)\n- **HSTS Preload**: https://hstspreload.org/ (HSTS verification)\n\nFURTHER READING:\n- OWASP WSTG-CRYP-01: Testing for Weak Transport Layer Security\n- NIST SP 800-52 Rev.2: Guidelines for TLS Implementation\n- RFC 8446: The Transport Layer Security (TLS) Protocol Version 1.3\n- RFC 6797: HTTP Strict Transport Security (HSTS)\n- Mozilla Server Side TLS: https://wiki.mozilla.org/Security/Server_Side_TLS\n- SSL/TLS Best Practices by Qualys: https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best-Practices",
      "tags": ["recon", "tls", "ssl", "certificates", "ciphers", "vulnerabilities", "heartbleed", "poodle", "hsts", "encryption"]
    },
    {
      "id": "email_reconnaissance",
      "title": "Email reconnaissance",
      "content": "OBJECTIVE: Gather email addresses, identify email infrastructure, and collect personnel information for social engineering preparation and authentication attack targeting.\n\nACADEMIC BACKGROUND:\nEmail reconnaissance is a critical component of social engineering attacks and credential-based intrusions. According to Verizon's Data Breach Investigations Report, 36% of breaches involve phishing, with email being the primary vector. The MITRE ATT&CK framework categorizes email reconnaissance as T1589 (Gather Victim Identity Information) and T1593 (Search Open Websites/Domains).\n\nOWASP Testing Guide emphasizes that email addresses and personnel information are valuable for:\n- Targeted phishing campaigns\n- Password spraying attacks\n- Social engineering preparation\n- Username enumeration\n- Credential stuffing attacks\n\nMETHODOLOGIES:\n1. **Passive Email Harvesting**: OSINT sources, public directories, social media\n2. **Active Email Enumeration**: SMTP server probing, pattern analysis\n3. **Infrastructure Analysis**: MX records, SPF/DMARC/DKIM validation\n4. **Personnel Intelligence**: LinkedIn profiling, company directories\n5. **Pattern Recognition**: Email address format analysis for prediction\n\nSTEP-BY-STEP PROCESS:\n\n1. EMAIL ADDRESS HARVESTING:\n   ```bash\n   # theHarvester (comprehensive OSINT tool)\n   theHarvester -d target.com -l 500 -b all -f results.html\n   \n   # Specify search engines/sources\n   theHarvester -d target.com -b google,bing,linkedin,github\n   \n   # JSON output for automation\n   theHarvester -d target.com -f results.json\n   \n   # Limit results per source\n   theHarvester -d target.com -l 100 -b all\n   \n   # Shodan integration for email patterns\n   theHarvester -d target.com -b shodan\n   ```\n   \n   Intelligence: theHarvester searches Google, Bing, LinkedIn, GitHub, Shodan, and other sources for email addresses\n\n2. EMAIL PATTERN RECOGNITION AND PREDICTION:\n   ```bash\n   # Common email patterns analysis\n   # firstname.lastname@domain.com\n   # firstlast@domain.com  \n   # flast@domain.com\n   # lastname@domain.com\n   # firstinitiallastname@domain.com\n   \n   # Use email permutations tool\n   python3 email-permutator.py -d target.com -n \"John Doe\"\n   \n   # Generate email list from names\n   # Input: names.txt (one name per line)\n   # Output: emails.txt\n   while read name; do\n       first=$(echo $name | awk '{print tolower($1)}')\n       last=$(echo $name | awk '{print tolower($2)}')\n       echo \"${first}.${last}@target.com\" >> emails.txt\n       echo \"${first}${last}@target.com\" >> emails.txt\n       echo \"${first:0:1}${last}@target.com\" >> emails.txt\n   done < names.txt\n   ```\n\n3. EMAIL SERVER ENUMERATION:\n   ```bash\n   # Check MX (Mail Exchange) records\n   dig target.com MX\n   \n   # Get MX server details\n   dig target.com MX +short\n   \n   # Identify email provider from MX records\n   # Google: ASPMX.L.GOOGLE.COM\n   # Microsoft: mail.protection.outlook.com\n   # Amazon SES: inbound-smtp.us-east-1.amazonaws.com\n   \n   # Reverse lookup MX servers\n   nslookup $(dig target.com MX +short | head -1 | awk '{print $2}')\n   \n   # Check for backup MX servers\n   dig target.com MX | grep -v \";\" | wc -l\n   ```\n\n4. SMTP SERVER PROBING:\n   ```bash\n   # SMTP banner grabbing (port 25)\n   nc -v target.com 25\n   \n   # Telnet connection for manual testing\n   telnet target.com 25\n   EHLO test.com\n   MAIL FROM: <test@test.com>\n   RCPT TO: <admin@target.com>\n   QUIT\n   \n   # Test for VRFY command (username enumeration)\n   # VRFY verifies if email exists\n   echo -e \"VRFY admin\\nQUIT\" | nc target.com 25\n   \n   # Test for EXPN command (mailing list expansion)\n   echo -e \"EXPN postmaster\\nQUIT\" | nc target.com 25\n   \n   # SMTP user enumeration with smtp-user-enum\n   smtp-user-enum -M VRFY -U users.txt -t target.com\n   \n   # Check for open relay (should fail)\n   swaks --to test@gmail.com --server target.com --quit-after RCPT\n   ```\n\n5. EMAIL SECURITY ASSESSMENT:\n   ```bash\n   # Check SPF (Sender Policy Framework)\n   dig target.com TXT | grep -i spf\n   \n   # Check DMARC (Domain-based Message Authentication)\n   dig _dmarc.target.com TXT\n   \n   # Check DKIM (DomainKeys Identified Mail)\n   dig default._domainkey.target.com TXT\n   \n   # Validate SPF record syntax\n   python3 -c \"import spf; print(spf.check('spf_record_here', 'sender@domain.com', '192.168.1.1'))\"\n   \n   # Test email spoofing protection\n   # Send test email and check headers\n   swaks --to your@email.com --from spoofed@target.com --server target.com\n   ```\n\n6. PERSONNEL INTELLIGENCE GATHERING:\n   ```bash\n   # LinkedIn employee enumeration\n   python3 linkedin2username.py -c \"Target Company\" -n 100\n   \n   # CrossLinked (advanced LinkedIn scraping)\n   python3 crosslinked.py -f \"company:target company\" -o linkedin_results.txt\n   \n   # Hunter.io API for email discovery\n   curl \"https://api.hunter.io/v2/domain-search?domain=target.com&api_key=YOUR_API_KEY\"\n   \n   # FullContact API (if available)\n   curl \"https://api.fullcontact.com/v3/person.enrich\" -H \"Authorization: Bearer YOUR_API_KEY\" -d '{\"email\":\"person@target.com\"}'\n   \n   # Company directory scraping\n   curl -s \"https://target.com/team\" | grep -oP 'mailto:[^\"]+' | sed 's/mailto://'\n   ```\n\n7. EMAIL ADDRESS VALIDATION AND VERIFICATION:\n   ```bash\n   # Bulk email verification\n   python3 email-verifier.py -f emails.txt\n   \n   # SMTP verification (no send, just check)\n   swaks --to target@email.com --server target.com --quit-after RCPT --suppress-data\n   \n   # Check for catch-all domain\n   swaks --to nonexistent-random-string@target.com --server target.com --quit-after RCPT\n   \n   # Mailbox detection with custom script\n   #!/bin/bash\n   EMAIL=$1\n   DOMAIN=$(echo $EMAIL | cut -d@ -f2)\n   USERNAME=$(echo $EMAIL | cut -d@ -f1)\n   \n   if swaks --to $EMAIL --server $DOMAIN --quit-after RCPT --suppress-data 2>/dev/null | grep -q \"250\"; then\n       echo \"$EMAIL - VALID\"\n   else\n       echo \"$EMAIL - INVALID\"\n   fi\n   ```\n\n8. ADVANCED EMAIL RECONNAISSANCE:\n   ```bash\n   # PGP key search (indicates security-conscious users)\n   gpg --search-keys --keyserver hkps://keys.openpgp.org user@target.com\n   \n   # HaveIBeenPwned API (breached accounts)\n   curl \"https://haveibeenpwned.com/api/v3/breachedaccount/user@target.com\" -H \"hibp-api-key: YOUR_API_KEY\"\n   \n   # Email metadata analysis from public sources\n   # GitHub commits, forum posts, etc.\n   \n   # Social media correlation\n   # Link email addresses to social profiles\n   python3 social_mapper.py -f emails.txt -t linkedin,twitter\n   ```\n\nWHAT TO LOOK FOR:\n- **Executive Email Addresses**: CEO, CTO, CIO, CISO for targeted attacks\n- **IT/Dev Team Emails**: For technical reconnaissance and insider access\n- **Email Patterns**: Consistent naming conventions for prediction\n- **Weak SPF Records**: \"v=spf1 -all\" (strict) vs \"v=spf1 +all\" (permissive)\n- **Missing DMARC**: No domain-based authentication\n- **VRFY/EXPN Enabled**: Username enumeration vulnerability\n- **Open Relay**: Can be abused for spam/phishing\n- **Catch-all Domains**: All emails accepted (harder to enumerate)\n- **LinkedIn Profiles**: Job titles, locations, connections\n- **PGP Keys**: Security-aware users may have stronger passwords\n- **Breached Accounts**: From HaveIBeenPwned for password spraying\n\nSECURITY IMPLICATIONS:\n- **Phishing Campaigns**: Targeted spear-phishing with real names/titles\n- **Credential Stuffing**: Known email patterns for password reuse attacks\n- **Social Engineering**: Personal information from LinkedIn profiles\n- **Username Enumeration**: VRFY/EXPN commands leak valid accounts\n- **Email Spoofing**: Weak SPF/DMARC allows impersonation\n- **Password Spraying**: Common passwords against known usernames\n- **Business Email Compromise**: Executive email compromise for wire fraud\n- **Supply Chain Attacks**: Third-party vendor email compromise\n\nCOMMON PITFALLS:\n- **Privacy Laws**: GDPR, CCPA limit data collection and usage\n- **Rate Limiting**: Email harvesting tools get blocked by search engines\n- **Role-based Emails**: info@, support@, admin@ may not be personal accounts\n- **Email Aliases**: Multiple addresses route to same mailbox\n- **SMTP Blocking**: Modern firewalls block port 25 from external sources\n- **API Limits**: Hunter.io, FullContact have usage restrictions\n- **False Positives**: SMTP accepts all emails (catch-all) or rejects all (strict filtering)\n- **Legal Issues**: Unauthorized email harvesting may violate terms of service\n- **Outdated Information**: Email addresses change, people leave companies\n- **VPN Requirements**: Some tools require VPN to avoid IP blocking\n\nDOCUMENTATION REQUIREMENTS:\n- Complete email address inventory with sources\n- Email pattern analysis and prediction rules\n- MX server details and email provider identification\n- SPF/DMARC/DKIM configuration assessment\n- SMTP server security evaluation (VRFY, EXPN, relay)\n- Personnel intelligence summary (names, titles, locations)\n- LinkedIn profile analysis and social engineering insights\n- Recommendations for email security hardening\n- Evidence of email spoofing vulnerabilities\n\nTOOLS REFERENCE:\n- **theHarvester**: https://github.com/laramies/theHarvester (OSINT email gathering)\n- **Hunter.io**: https://hunter.io/ (Professional email finder API)\n- **CrossLinked**: https://github.com/m8r0wn/CrossLinked (LinkedIn intelligence)\n- **smtp-user-enum**: https://pentestmonkey.net/tools/user-enumeration/smtp-user-enum (SMTP enumeration)\n- **swaks**: https://github.com/jetmore/swaks (SMTP testing tool)\n- **linkedin2username**: https://github.com/initstring/linkedin2username (LinkedIn scraping)\n- **social-mapper**: https://github.com/Greenwolf/social_mapper (Social media correlation)\n- **email-permutator**: Custom tool for email pattern generation\n\nFURTHER READING:\n- OWASP Testing Guide: Identity Management Testing\n- MITRE ATT&CK: T1589 Gather Victim Identity Information\n- Verizon DBIR: https://www.verizon.com/business/resources/reports/dbir/\n- SPF Record Syntax: https://tools.ietf.org/html/rfc7208\n- DMARC Specification: https://tools.ietf.org/html/rfc7489\n- Anti-Phishing Working Group: https://apwg.org/\n- HaveIBeenPwned: https://haveibeenpwned.com/",
      "tags": ["recon", "email", "osint", "social-engineering", "phishing", "smtp", "spf", "dmarc", "personnel"]
    },
    {
      "id": "whois_domain_analysis",
      "title": "WHOIS domain analysis",
      "content": "OBJECTIVE: Extract domain registration information including ownership, contacts, registration dates, and name servers to understand organizational structure and identify additional assets.\n\nACADEMIC BACKGROUND:\nWHOIS (\"Who is\") is a query and response protocol that provides registration information for domain names and IP addresses. According to RFC 3912, WHOIS serves as the public directory for internet resource registration data. The MITRE ATT&CK framework categorizes WHOIS analysis as T1596 (Search Open Technical Databases) under Reconnaissance.\n\nWHOIS data reveals critical intelligence for penetration testing:\n- **Domain Ownership**: Registrant organization and contact details\n- **Infrastructure Insights**: Name servers, hosting providers\n- **Temporal Analysis**: Registration and expiration dates\n- **Asset Discovery**: Related domains owned by same entity\n- **Geographic Intelligence**: Registrant country and administrative contacts\n\nThe NIST SP 800-115 Technical Guide emphasizes WHOIS as a fundamental passive reconnaissance technique for understanding target infrastructure and organizational relationships.\n\nMETHODOLOGIES:\n1. **Basic WHOIS Queries**: Standard protocol lookups\n2. **RDAP Integration**: Modern RESTful alternative to WHOIS\n3. **Historical Analysis**: Domain registration timeline\n4. **Related Domain Discovery**: Same registrant/organization assets\n5. **Privacy Service Detection**: WHOIS privacy/proxy identification\n\nSTEP-BY-STEP PROCESS:\n\n1. BASIC WHOIS QUERIES:\n   ```bash\n   # Standard WHOIS lookup\n   whois target.com\n   \n   # Specify WHOIS server for TLD-specific queries\n   whois -h whois.verisign-grs.com target.com  # .com, .net\n   whois -h whois.pir.org target.com            # .org\n   whois -h whois.nic.uk target.com             # .uk domains\n   whois -h whois.afrinic.net target.com        # African IPs\n   \n   # IPv4 address WHOIS\n   whois 192.168.1.1\n   \n   # IPv6 address WHOIS\n   whois 2001:db8::1\n   \n   # ASN WHOIS\n   whois -h whois.radb.net AS12345\n   ```\n\n2. RDAP (REGISTRATION DATA ACCESS PROTOCOL):\n   ```bash\n   # RDAP query (modern JSON-based WHOIS)\n   curl -s https://rdap.org/domain/target.com | jq\n   \n   # ARIN RDAP for American registry\n   curl -s https://rdap.arin.net/registry/ip/192.168.1.0 | jq\n   \n   # RIPE RDAP for European registry\n   curl -s https://rdap.db.ripe.net/ip/192.168.1.0 | jq\n   \n   # APNIC RDAP for Asia-Pacific\n   curl -s https://rdap.apnic.net/ip/192.168.1.0 | jq\n   \n   # LACNIC RDAP for Latin America\n   curl -s https://rdap.lacnic.net/ip/192.168.1.0 | jq\n   ```\n\n3. EXTRACTING KEY REGISTRATION INFORMATION:\n   ```bash\n   # Domain registrar\n   whois target.com | grep -iE \"(registrar|registrar name)\"\n   \n   # Name servers\n   whois target.com | grep -iE \"(name server|nserver)\"\n   \n   # Registration dates\n   whois target.com | grep -iE \"(creation|created|registration) date\"\n   whois target.com | grep -iE \"(updated|modified|changed) date\"\n   whois target.com | grep -iE \"(expir|expiration) date\"\n   \n   # Registrant information\n   whois target.com | grep -iE \"(registrant|owner)\"\n   whois target.com | grep -iA5 -i \"registrant\"\n   \n   # Administrative contact\n   whois target.com | grep -iE \"(admin|administrative)\"\n   whois target.com | grep -A5 -i \"admin contact\"\n   \n   # Technical contact\n   whois target.com | grep -iE \"(tech|technical)\"\n   whois target.com | grep -A5 -i \"tech contact\"\n   \n   # Billing contact\n   whois target.com | grep -iE \"(billing|bill)\"\n   whois target.com | grep -A5 -i \"billing contact\"\n   ```\n\n4. DOMAIN STATUS AND FLAGS:\n   ```bash\n   # Domain status codes\n   whois target.com | grep -i \"status\"\n   \n   # Common status codes:\n   # ACTIVE - Domain is active\n   # OK - Domain is active\n   # CLIENT HOLD - Domain suspended by registrar\n   # SERVER HOLD - Domain suspended by registry\n   # REDEMPTION PERIOD - Domain expired, can be restored\n   # PENDING DELETE - Domain will be deleted soon\n   # ADD PERIOD - Domain recently registered\n   \n   # DNSSEC status\n   whois target.com | grep -i dnssec\n   \n   # Domain lock status\n   whois target.com | grep -iE \"(lock|transfer)\"\n   ```\n\n5. HISTORICAL WHOIS ANALYSIS:\n   ```bash\n   # WHOIS history (if available from registrar)\n   # Some registrars provide historical data\n   \n   # Domain age calculation\n   CREATION_DATE=$(whois target.com | grep -i \"creation date\" | head -1 | cut -d: -f2- | xargs)\n   echo \"Domain registered: $CREATION_DATE\"\n   \n   # Check if domain is recently registered (< 1 year)\n   # Recently registered domains may indicate new campaigns\n   \n   # Check expiration date proximity\n   EXPIRY_DATE=$(whois target.com | grep -i \"expir\" | head -1 | cut -d: -f2- | xargs)\n   echo \"Domain expires: $EXPIRY_DATE\"\n   ```\n\n6. RELATED DOMAIN DISCOVERY:\n   ```bash\n   # Find domains with same registrant email\n   # Requires WHOIS database access or commercial tools\n   \n   # Find domains with same name servers\n   NS_LIST=$(whois target.com | grep -i \"name server\" | awk '{print $NF}' | sort -u)\n   for ns in $NS_LIST; do\n       echo \"Domains using $ns as nameserver:\"\n       # Reverse NS lookup would require zone walking or commercial tools\n   done\n   \n   # Find domains with same IP ranges\n   # Use reverse DNS and WHOIS correlation\n   \n   # Organization-based domain enumeration\n   # target.com, target.net, target.org, target.io, etc.\n   for tld in com net org io co uk; do\n       whois target.$tld 2>/dev/null | grep -q \"Domain not found\" || echo \"target.$tld exists\"\n   done\n   ```\n\n7. WHOIS PRIVACY AND PROXY DETECTION:\n   ```bash\n   # Check for privacy services\n   whois target.com | grep -iE \"(privacy|proxy|whoisguard|domainsbyproxy)\"\n   \n   # Common privacy services:\n   # WhoisGuard, Domains By Proxy, PrivacyProtect.org\n   # NameCheap Privacy, GoDaddy Privacy\n   # Porkbun Privacy, NameSilo Privacy\n   \n   # Check for proxy registration\n   whois target.com | grep -iE \"(proxy|anonymous|private)\"\n   \n   # Verify privacy service usage\n   PRIVACY_INDICATORS=\"privacy|proxy|whois.*guard|domains.*proxy|private.*registration\"\n   if whois target.com | grep -qiE \"$PRIVACY_INDICATORS\"; then\n       echo \"WHOIS privacy service detected\"\n   fi\n   ```\n\n8. BULK WHOIS ANALYSIS:\n   ```bash\n   # Multiple domain analysis\n   for domain in target.com api.target.com dev.target.com staging.target.com; do\n       echo \"=== $domain ===\"\n       whois $domain | grep -E \"(Registrar|Name Server|Creation Date|Registrant)\" | head -10\n       echo\n   done\n   \n   # Subdomain WHOIS correlation\n   # Check if subdomains have different registrants (may indicate external services)\n   ```\n\n9. ADVANCED WHOIS INTELLIGENCE:\n   ```bash\n   # Extract email addresses from WHOIS\n   whois target.com | grep -E \"@\" | grep -v \"whois\" | sort -u\n   \n   # Extract phone numbers\n   whois target.com | grep -E \"\\+[0-9]|\\([0-9]{3}\\)\" | sort -u\n   \n   # Geographic analysis\n   COUNTRY=$(whois target.com | grep -i \"country\" | head -1 | cut -d: -f2 | xargs)\n   echo \"Registrant country: $COUNTRY\"\n   \n   # Organization analysis\n   ORG=$(whois target.com | grep -i \"organization\" | head -1 | cut -d: -f2 | xargs)\n   echo \"Registrant organization: $ORG\"\n   \n   # Cross-reference with other intelligence\n   # Use registrant info for LinkedIn searches, email reconnaissance, etc.\n   ```\n\nWHAT TO LOOK FOR:\n- **Recent Registration**: Domains < 1 year old may indicate new operations\n- **Privacy Services**: WHOIS privacy/proxy indicates deliberate anonymity\n- **Bulk Registration**: Multiple domains with same registrant (asset inventory)\n- **Foreign Registration**: Domains registered in different countries than operation\n- **Contact Consistency**: Same contacts across multiple domains\n- **Name Server Patterns**: Custom NS vs. hosting provider defaults\n- **Expiration Proximity**: Domains expiring soon (acquisition opportunities)\n- **Status Anomalies**: Unusual status codes indicating issues\n- **DNSSEC Absence**: Missing DNSSEC may indicate weaker security posture\n- **Registrar Reputation**: Known registrars vs. suspicious ones\n\nSECURITY IMPLICATIONS:\n- **Domain Hijacking**: Weak registration security, expired domains\n- **Phishing Campaigns**: Recently registered similar domains\n- **Infrastructure Mapping**: Name servers reveal hosting providers\n- **Social Engineering**: Contact information for targeted attacks\n- **Supply Chain Analysis**: Related domains show organizational scope\n- **Geographic Intelligence**: Registration country vs. operation location\n- **Privacy Evasion**: WHOIS privacy services hide malicious actors\n- **Asset Discovery**: Related domains expand attack surface\n- **Business Intelligence**: Registration dates show company age/maturity\n\nCOMMON PITFALLS:\n- **WHOIS Privacy Services**: Real owner information hidden behind proxies\n- **Outdated Information**: Contact details may be years old\n- **Rate Limiting**: Bulk queries get blocked by WHOIS servers\n- **TLD Variations**: Different TLDs have different WHOIS policies (.io vs .com)\n- **ICANN Restrictions**: Some countries restrict WHOIS data access\n- **Historical Data Unavailable**: Most WHOIS servers don't provide history\n- **False Positives**: Similar names don't indicate same ownership\n- **Legal Restrictions**: Some jurisdictions limit WHOIS data collection\n- **API Dependencies**: Commercial tools needed for bulk analysis\n- **IPv6 Complexity**: IPv6 WHOIS more complex than IPv4\n\nDOCUMENTATION REQUIREMENTS:\n- Complete WHOIS record for primary domain\n- Name server analysis and hosting provider identification\n- Registration timeline (creation, updates, expiration)\n- Contact information extraction (with privacy service notes)\n- Related domain inventory\n- Geographic analysis and jurisdiction assessment\n- DNSSEC status and security posture evaluation\n- Recommendations for domain security hardening\n- Evidence of WHOIS privacy service usage\n- Comparison with industry standards and best practices\n\nTOOLS REFERENCE:\n- **whois**: Built-in Linux/Unix command (most basic)\n- **jwhois**: Enhanced WHOIS client with caching\n- **RDAP**: https://rdap.org/ (Modern JSON-based WHOIS)\n- **WhoisXML API**: https://www.whoisxmlapi.com/ (Commercial WHOIS API)\n- **DomainTools**: https://www.domaintools.com/ (Professional domain intelligence)\n- **ICANN WHOIS**: https://lookup.icann.org/ (Official ICANN lookup)\n- **WHOIS History Tools**: Various commercial services for historical data\n- **Bulk WHOIS Tools**: Scripts and commercial services for mass queries\n\nFURTHER READING:\n- RFC 3912: WHOIS Protocol Specification\n- RFC 7480-7485: RDAP Protocol Specifications\n- ICANN WHOIS Data Reminder Policy: https://www.icann.org/resources/pages/gtld-registration-data-2017-06-01-en\n- NIST SP 800-115: Technical Guide to Information Security Testing\n- MITRE ATT&CK: T1596 Search Open Technical Databases\n- OWASP Testing Guide: Information Gathering\n- WHOIS Privacy Services Analysis: https://www.eff.org/deeplinks/2016/10/whois-privacy-services",
      "tags": ["recon", "whois", "domain", "registration", "rdap", "ownership", "privacy", "infrastructure", "dns"]
    },
    {
      "id": "public_exposure_scanning",
      "title": "Public exposure scanning",
      "content": "OBJECTIVE: Identify internet-facing assets and services using global scanning engines to discover shadow IT, forgotten systems, and publicly exposed resources that should not be accessible.\n\nACADEMIC BACKGROUND:\nPublic exposure scanning leverages internet-wide search engines and databases to discover assets that are inadvertently exposed to the public internet. According to Shodan Research, there are over 500 million publicly accessible devices worldwide, many of which are misconfigured or forgotten systems.\n\nThe MITRE ATT&CK framework categorizes this as T1596 (Search Open Technical Databases) and T1590 (Gather Victim Network Information). NIST SP 800-115 emphasizes that passive reconnaissance using public databases is essential for comprehensive asset discovery without alerting the target.\n\nKey objectives include:\n- **Shadow IT Discovery**: Systems deployed without IT approval\n- **Forgotten Assets**: Legacy systems left exposed\n- **Misconfigurations**: Services accessible without proper authentication\n- **IoT/OT Exposure**: Internet-connected devices and industrial systems\n- **Vulnerable Services**: Outdated software with known exploits\n\nMETHODOLOGIES:\n1. **Global Search Engines**: Shodan, Censys, ZoomEye, BinaryEdge\n2. **Certificate Transparency**: SSL certificate analysis for domain discovery\n3. **Passive DNS**: Historical DNS resolution data\n4. **Vulnerability Correlation**: Matching exposed services to known CVEs\n5. **Technology Fingerprinting**: Identifying service versions and configurations\n\nSTEP-BY-STEP PROCESS:\n\n1. SHODAN RECONNAISSANCE:\n   ```bash\n   # Install Shodan CLI\n   pip3 install shodan\n   shodan init YOUR_API_KEY\n   \n   # Search by organization name\n   shodan search \"org:\\\"Target Company\\\"\"\n   \n   # Search by domain\n   shodan search \"hostname:target.com\"\n   \n   # Search by IP range (requires paid API)\n   shodan search \"net:192.168.1.0/24\"\n   \n   # Find specific exposed services\n   shodan search \"target.com port:3389\"  # RDP\n   shodan search \"target.com port:22\"    # SSH\n   shodan search \"target.com port:27017\" # MongoDB\n   shodan search \"target.com port:6379\"  # Redis\n   shodan search \"target.com port:9200\"  # Elasticsearch\n   \n   # Search for web technologies\n   shodan search \"target.com http.title:\\\"admin\\\"\"\n   shodan search \"target.com http.component:\\\"phpMyAdmin\\\"\"\n   shodan search \"target.com http.component:\\\"WordPress\\\"\"\n   \n   # Download and parse results\n   shodan download --limit 1000 target_scan \"org:\\\"Target Company\\\"\"\n   shodan parse --fields ip_str,port,hostnames,product,version target_scan.json.gz\n   \n   # Convert to host list for further scanning\n   shodan parse --fields ip_str target_scan.json.gz | sort -u > target_hosts.txt\n   ```\n\n2. CENSYS SCANNING:\n   ```bash\n   # Install Censys CLI\n   pip3 install censys\n   censys config\n   \n   # Search via command line\n   censys search \"target.com and services.service_name: HTTP\"\n   \n   # Search for specific services\n   censys search \"target.com and services.service_name: SSH\"\n   censys search \"target.com and services.service_name: RDP\"\n   censys search \"target.com and services.port: 27017\"  # MongoDB\n   \n   # Advanced queries\n   censys search \"target.com and labels: \\\"self-signed\\\" \"  # Self-signed certificates\n   censys search \"target.com and services.tls.version: \\\"1.0\\\"\"  # Weak TLS\n   \n   # Export results\n   censys search \"target.com\" --format json > censys_results.json\n   \n   # Extract IPs for further analysis\n   cat censys_results.json | jq -r '.[] | .ip' | sort -u > censys_ips.txt\n   ```\n\n3. ZOOMEYE RECONNAISSANCE:\n   ```bash\n   # Install ZoomEye CLI\n   pip3 install zoomeye\n   zoomeye init\n   \n   # Search by domain\n   zoomeye search \"domain:target.com\"\n   \n   # Search by IP\n   zoomeye search \"ip:192.168.1.1\"\n   \n   # Search for specific services\n   zoomeye search \"port:3389\"\n   zoomeye search \"app:\\\"IIS\\\"\"\n   \n   # Search for banners\n   zoomeye search \"banner:\\\"Apache Tomcat\\\"\"\n   \n   # Export results\n   zoomeye search \"domain:target.com\" --save\n   ```\n\n4. BINARYEDGE SCANNING:\n   ```bash\n   # API-based queries\n   curl \"https://api.binaryedge.io/v2/query/search?query=target.com&page=1\" \\\n        -H \"X-Key: YOUR_API_KEY\"\n   \n   # Search for specific services\n   curl \"https://api.binaryedge.io/v2/query/search?query=services:ssh AND target.com\" \\\n        -H \"X-Key: YOUR_API_KEY\"\n   \n   # Search for vulnerabilities\n   curl \"https://api.binaryedge.io/v2/query/search?query=vulnerabilities:cve-2021-44228\" \\\n        -H \"X-Key: YOUR_API_KEY\"\n   ```\n\n5. FOFA (CHINESE SEARCH ENGINE):\n   ```bash\n   # Web interface queries\n   # domain=\"target.com\"\n   # cert=\"target.com\"\n   # body=\"target.com\"\n   # header=\"target.com\"\n   \n   # API queries (requires account)\n   curl \"https://fofa.info/api/v1/search?qbase64=BASE64_ENCODED_QUERY\" \\\n        -H \"Authorization: YOUR_API_KEY\"\n   ```\n\n6. CERTIFICATE TRANSPARENCY LOGS:\n   ```bash\n   # Search certificate transparency logs\n   curl -s \"https://crt.sh/?q=%.target.com&output=json\" | jq -r '.[].name_value' | sort -u\n   \n   # Find subdomains via certificates\n   curl -s \"https://crt.sh/?q=target.com&output=json\" | jq -r '.[] | select(.name_value | contains(\"target.com\")) | .name_value' | sort -u\n   \n   # Check for wildcard certificates\n   curl -s \"https://crt.sh/?q=target.com&output=json\" | jq -r '.[] | select(.name_value | startswith(\"*\")) | .name_value' | sort -u\n   ```\n\n7. VULNERABILITY CORRELATION WITH NUCLEI:\n   ```bash\n   # Install Nuclei\n   go install -v github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest\n   \n   # Scan discovered hosts for known vulnerabilities\n   nuclei -l target_hosts.txt -t cves/ -t exposures/ -t vulnerabilities/\n   \n   # Technology fingerprinting\n   nuclei -l target_hosts.txt -t technologies/\n   \n   # Misconfiguration scanning\n   nuclei -l target_hosts.txt -t misconfigurations/\n   \n   # Specific vulnerability checks\n   nuclei -l target_hosts.txt -t cves/2021/CVE-2021-44228.yaml  # Log4Shell\n   nuclei -l target_hosts.txt -t cves/2021/CVE-2021-34527.yaml  # PrintNightmare\n   \n   # Custom severity filtering\n   nuclei -l target_hosts.txt -t cves/ -severity critical,high\n   \n   # JSON output for reporting\n   nuclei -l target_hosts.txt -t exposures/ -json -o nuclei_exposures.json\n   ```\n\n8. COMMON EXPOSURE PATTERNS:\n   ```bash\n   # Exposed databases\n   shodan search \"port:27017 mongodb\"  # MongoDB\n   shodan search \"port:6379 redis\"     # Redis\n   shodan search \"port:9200 elasticsearch\"  # Elasticsearch\n   shodan search \"port:5432 postgresql\"     # PostgreSQL\n   \n   # Exposed admin interfaces\n   shodan search \"http.title:\\\"phpMyAdmin\\\"\"\n   shodan search \"http.title:\\\"Adminer\\\"\"\n   shodan search \"http.title:\\\"phpRedisAdmin\\\"\"\n   shodan search \"http.component:\\\"cPanel\\\"\"\n   shodan search \"http.component:\\\"Plesk\\\"\"\n   \n   # Exposed development servers\n   shodan search \"http.title:\\\"Jenkins\\\"\"\n   shodan search \"http.title:\\\"GitLab\\\"\"\n   shodan search \"http.title:\\\"phpinfo()\\\"\"\n   shodan search \"http.title:\\\"Test Page\\\"\"\n   \n   # IoT and embedded devices\n   shodan search \"port:80 \\\"Server: GoAhead\\\"\"\n   shodan search \"port:80 \\\"Server: Boa\\\"\"\n   shodan search \"port:554 rtsp\"  # IP cameras\n   \n   # Remote access services\n   shodan search \"port:3389 rdp\"  # RDP\n   shodan search \"port:5900 vnc\"  # VNC\n   shodan search \"port:22 ssh \\\"OpenSSH\\\"\"\n   \n   # API endpoints\n   shodan search \"http.component:\\\"Swagger UI\\\"\"\n   shodan search \"http.component:\\\"GraphQL\\\"\"\n   shodan search \"port:8080 \\\"tomcat\\\"\"\n   ```\n\n9. ADVANCED QUERY TECHNIQUES:\n   ```bash\n   # Complex Shodan queries\n   shodan search \"org:\\\"Target Company\\\" port:443 ssl.cert.subject.cn:target.com\"\n   shodan search \"hostname:*.target.com -hostname:www.target.com\"\n   shodan search \"net:192.168.0.0/16 country:US\"\n   \n   # Censys advanced queries\n   censys search \"target.com and services.tls.certificates.leaf_data.subject.common_name: *.target.com\"\n   censys search \"target.com and services.http.response.headers.server: \\\"nginx\\\"\"\n   \n   # Time-based searches (recent exposures)\n   shodan search \"target.com after:2024-01-01\"\n   \n   # Geographic filtering\n   shodan search \"target.com country:US city:\\\"San Francisco\\\"\"\n   ```\n\n10. AUTOMATED EXPOSURE HUNTING:\n    ```bash\n    #!/bin/bash\n    TARGET=\"target.com\"\n    OUTPUT_DIR=\"exposure_scan_$(date +%Y%m%d)\"\n    mkdir -p \"$OUTPUT_DIR\"\n    \n    echo \"[*] Starting comprehensive exposure scan for $TARGET\"\n    \n    # Shodan scanning\n    echo \"[*] Shodan reconnaissance...\"\n    shodan search \"hostname:$TARGET\" --fields ip_str,port,hostnames > \"$OUTPUT_DIR/shodan_hosts.txt\"\n    \n    # Censys scanning\n    echo \"[*] Censys reconnaissance...\"\n    censys search \"$TARGET\" --format csv > \"$OUTPUT_DIR/censys_results.csv\"\n    \n    # Certificate transparency\n    echo \"[*] Certificate transparency analysis...\"\n    curl -s \"https://crt.sh/?q=%.$TARGET&output=json\" | jq -r '.[].name_value' | sort -u > \"$OUTPUT_DIR/cert_domains.txt\"\n    \n    # Combine all discovered IPs\n    cat \"$OUTPUT_DIR/shodan_hosts.txt\" \"$OUTPUT_DIR/censys_results.csv\" | grep -oE '\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b' | sort -u > \"$OUTPUT_DIR/all_ips.txt\"\n    \n    # Nuclei vulnerability scanning\n    echo \"[*] Vulnerability correlation...\"\n    nuclei -l \"$OUTPUT_DIR/all_ips.txt\" -t exposures/ -t vulnerabilities/ -json -o \"$OUTPUT_DIR/nuclei_findings.json\"\n    \n    echo \"[*] Scan complete. Results in $OUTPUT_DIR/\"\n    ```\n\nWHAT TO LOOK FOR:\n- **Exposed Databases**: MongoDB, Redis, Elasticsearch without authentication\n- **Admin Panels**: phpMyAdmin, cPanel, Webmin accessible from internet\n- **Development Servers**: Jenkins, GitLab, test environments exposed\n- **Remote Access**: RDP, VNC, SSH with weak/default credentials\n- **IoT Devices**: IP cameras, smart devices, industrial control systems\n- **API Endpoints**: Swagger UI, GraphQL, REST APIs without authentication\n- **File Shares**: SMB, NFS, FTP servers exposed to internet\n- **Mail Servers**: SMTP, IMAP, POP3 with open relay\n- **Monitoring Systems**: Nagios, Zabbix, Cacti publicly accessible\n- **Backup Files**: Database dumps, configuration files exposed\n- **Default Credentials**: Services running with factory defaults\n- **Outdated Software**: Vulnerable versions of common applications\n- **Cloud Storage**: S3 buckets, blob storage with public access\n- **CDN Misconfigurations**: Origin servers exposed alongside CDN\n\nSECURITY IMPLICATIONS:\n- **Data Exfiltration**: Exposed databases contain sensitive information\n- **Initial Access**: Open remote access services provide entry points\n- **Lateral Movement**: Exposed internal services enable network traversal\n- **Credential Theft**: Default credentials on exposed services\n- **Ransomware Entry**: Exposed RDP/VNC common infection vectors\n- **Supply Chain Compromise**: Exposed development/build systems\n- **Privacy Violations**: Exposed personal data in databases\n- **Regulatory Non-Compliance**: Exposed systems violate data protection laws\n- **Reputation Damage**: Exposed sensitive systems erode trust\n- **Financial Loss**: Exposed systems lead to business disruption\n\nCOMMON PITFALLS:\n- **Intentional Exposure**: Some services are meant to be public (APIs, CDNs)\n- **Stale Data**: Search engine data may be days/weeks old\n- **Rate Limiting**: Free API tiers limit query volume\n- **False Positives**: Honeypots and research systems appear vulnerable\n- **Legal Restrictions**: Some scanning violates terms of service\n- **Geographic Bias**: Search engines may miss regionally hosted assets\n- **API Costs**: Comprehensive scanning requires paid API access\n- **Data Overload**: Too many results make analysis difficult\n- **Context Missing**: Exposed service may be in DMZ by design\n- **Time Sensitivity**: Exposures may be remediated between scans\n\nDOCUMENTATION REQUIREMENTS:\n- Complete inventory of discovered assets and services\n- Service classification (exposed databases, admin panels, etc.)\n- Vulnerability correlation with known CVEs\n- Geographic distribution of exposed assets\n- Severity prioritization of findings\n- Evidence screenshots of exposed interfaces\n- Recommendations for remediation and hardening\n- Timeline analysis (when exposures were first discovered)\n- Comparison with industry benchmarks\n- Executive summary for management communication\n\nTOOLS REFERENCE:\n- **Shodan**: https://www.shodan.io/ (Primary internet-wide search engine)\n- **Censys**: https://search.censys.io/ (Academic search engine with API)\n- **ZoomEye**: https://www.zoomeye.org/ (Chinese search engine)\n- **BinaryEdge**: https://binaryedge.io/ (Security-focused search)\n- **FOFA**: https://fofa.info/ (Chinese cybersecurity search engine)\n- **Nuclei**: https://github.com/projectdiscovery/nuclei (Vulnerability scanner)\n- **Masscan**: https://github.com/robertdavidgraham/masscan (Fast internet scanner)\n- **ZMap**: https://zmap.io/ (Academic internet scanner)\n- **Censys-CLI**: https://github.com/censys/censys-cli (Command-line interface)\n- **Shodan-CLI**: https://cli.shodan.io/ (Official Shodan CLI)\n\nFURTHER READING:\n- Shodan Research Reports: https://research.shodan.io/\n- MITRE ATT&CK: T1596 Search Open Technical Databases\n- NIST SP 800-115: Technical Guide to Information Security Testing\n- OWASP Testing Guide: Configuration and Deployment Management Testing\n- \"The Internet-Wide Scan Data from Censys\": https://censys.io/\n- \"Worldwide Exposure of Industrial Control Systems\": https://www.shodan.io/report/89Lh9L2f\n- \"The Shadow Server Foundation\": https://www.shadowserver.org/\n- \"Rapid7 Project Sonar\": https://www.rapid7.com/research/project-sonar/",
      "tags": ["recon", "exposure", "shodan", "censys", "vulnerabilities", "shadow-it", "public-assets", "internet-scanning"]
    },
    {
      "id": "javascript_analysis",
      "title": "JavaScript analysis",
      "content": "OBJECTIVE: Extract and analyze JavaScript files to discover hidden API endpoints, exposed secrets, client-side logic vulnerabilities, and sensitive information hardcoded in frontend code.\n\nACADEMIC BACKGROUND:\nJavaScript analysis is a critical component of modern web application reconnaissance. According to OWASP WSTG-INFO-05 (Review Webpage Content for Information Leakage), client-side code often contains sensitive information that developers assume will remain hidden. The MITRE ATT&CK framework categorizes this as T1593 (Search Open Websites/Domains) under Reconnaissance.\n\nModern web applications rely heavily on JavaScript for functionality, often exposing:\n- **API Endpoints**: REST, GraphQL, and internal service URLs\n- **Authentication Tokens**: JWTs, API keys, OAuth secrets\n- **Configuration Data**: Database connections, service URLs\n- **Business Logic**: Client-side validation rules and workflows\n- **Third-party Integrations**: Analytics keys, payment processor IDs\n- **Development Artifacts**: Source maps, debug endpoints\n\nMETHODOLOGIES:\n1. **JavaScript File Discovery**: Systematic collection of all JS files\n2. **Endpoint Extraction**: Regex and AST-based URL discovery\n3. **Secret Hunting**: Pattern matching for credentials and keys\n4. **Source Code Analysis**: Deobfuscation and source map analysis\n5. **Client-side Logic Review**: Understanding application behavior\n\nSTEP-BY-STEP PROCESS:\n\n1. JAVASCRIPT FILE DISCOVERY AND COLLECTION:\n   ```bash\n   # Extract JS files with Hakrawler\n   echo \"https://target.com\" | hakrawler -js | grep -E '\\.js($|\\?)' > js_files.txt\n   \n   # Using getJS (comprehensive JS extractor)\n   getJS --url https://target.com --complete --output jsfiles.txt\n   \n   # Alternative: LinkFinder discovery\n   python3 LinkFinder/linkfinder.py -i https://target.com -o cli | grep '\\.js' > js_files.txt\n   \n   # Download all discovered JS files\n   mkdir -p js_analysis && cd js_analysis\n   wget -i ../js_files.txt --no-check-certificate -q\n   \n   # Handle dynamic JS loading (check network tab in browser)\n   # Look for: XMLHttpRequest, fetch(), axios calls\n   \n   # Extract JS from HTML pages\n   curl -s https://target.com | grep -oP 'src=\"[^\"]*\\.js[^\"]*\"' | sed 's/src=\"//' | sed 's/\"$//' > inline_js.txt\n   ```\n\n2. ENDPOINT EXTRACTION FROM JAVASCRIPT:\n   ```bash\n   # LinkFinder (advanced regex-based extraction)\n   python3 LinkFinder/linkfinder.py -i https://target.com/static/app.js -o cli\n   \n   # Bulk analysis of all JS files\n   for jsfile in *.js; do\n       echo \"=== $jsfile ===\"\n       python3 LinkFinder/linkfinder.py -i \"$jsfile\" -o cli\n       echo\n   done > endpoints_from_js.txt\n   \n   # Manual regex extraction for common patterns\n   grep -rEo \"https?://[^'\\\"\\s]+\" *.js | sort -u > manual_endpoints.txt\n   grep -rEo \"(/api|/v[0-9]+|/rest|/graphql|/admin|/internal)[^'\\\"\\s]*\" *.js | sort -u > api_endpoints.txt\n   \n   # Extract internal domain references\n   grep -rEo \"(target\\.com|internal\\.target\\.com|api\\.target\\.com)[^'\\\"\\s]*\" *.js | sort -u > internal_urls.txt\n   \n   # Find WebSocket endpoints\n   grep -rEo \"wss?://[^'\\\"\\s]+\" *.js | sort -u > websocket_endpoints.txt\n   \n   # Extract GraphQL endpoints\n   grep -rEi \"(graphql|/graphql)\" *.js | sort -u > graphql_endpoints.txt\n   ```\n\n3. SECRET AND CREDENTIAL HUNTING:\n   ```bash\n   # API keys and tokens (comprehensive patterns)\n   grep -rEi \"(api[_-]?key|apikey|api_key|apiKey)\" *.js\n   grep -rEi \"(access[_-]?token|auth[_-]?token|bearer[_-]?token)\" *.js\n   grep -rEi \"(client[_-]?secret|client[_-]?id|app[_-]?secret)\" *.js\n   \n   # AWS credentials\n   grep -rE \"AKIA[0-9A-Z]{16}\" *.js\n   grep -rEi \"(aws_access_key|aws_secret_key)\" *.js\n   \n   # Google Cloud/Service Account keys\n   grep -rEi \"(AIza[0-9A-Za-z-_]{35}|GOOGLE_API_KEY)\" *.js\n   \n   # Azure keys\n   grep -rEi \"(azure.*key|AZURE_.*_KEY)\" *.js\n   \n   # JWT tokens (may be test/example tokens)\n   grep -rEo \"eyJ[A-Za-z0-9-_]+\\.[A-Za-z0-9-_]+\\.[A-Za-z0-9-_]*\" *.js\n   \n   # OAuth secrets\n   grep -rEi \"(oauth.*secret|oauth.*key)\" *.js\n   \n   # Database connection strings\n   grep -rEi \"(mongodb|mysql|postgresql|redis)://[^'\\\"\\s]*\" *.js\n   \n   # Private keys and certificates\n   grep -rE \"BEGIN.*PRIVATE KEY\" *.js\n   grep -rE \"BEGIN.*CERTIFICATE\" *.js\n   \n   # Passwords and credentials\n   grep -rEi \"(password|passwd|pwd)\\s*[:=]\\s*['\\\"][^'\\\"\\s]{6,}\" *.js\n   grep -rEi \"(username|user|login)\\s*[:=]\\s*['\\\"][^'\\\"\\s]+\" *.js\n   \n   # Encryption keys and salts\n   grep -rEi \"(secret[_-]?key|encryption[_-]?key|salt)\" *.js\n   ```\n\n4. SOURCE MAP ANALYSIS:\n   ```bash\n   # Find source map references\n   grep -rE '\\.js\\.map' *.js > source_maps.txt\n   \n   # Download source maps\n   cat source_maps.txt | while read line; do\n       map_url=$(echo \"$line\" | grep -oP 'sourceMappingURL=\\K[^\\s]+')\n       if [[ $map_url == http* ]]; then\n           wget \"$map_url\" -q\n       fi\n   done\n   \n   # Analyze source maps with sourcemapper\n   for mapfile in *.map; do\n       python3 sourcemapper.py -u \"$mapfile\" -o \"deobfuscated_$(basename \"$mapfile\" .map)\"\n   done\n   \n   # Extract original source code paths\n   find . -name \"*.map\" -exec sh -c 'echo \"=== $1 ===\"; cat \"$1\" | jq -r \".sources[]\"' _ {} \\;\n   \n   # Look for sensitive information in source maps\n   grep -rEi \"(password|secret|key|token)\" deobfuscated_*/\n   ```\n\n5. WEBPACK AND BUILD ANALYSIS:\n   ```bash\n   # Identify webpack bundles\n   grep -l \"webpackJsonp\" *.js\n   grep -l \"__webpack_require__\" *.js\n   \n   # Extract webpack module information\n   grep -oP '/\\*!.*?\\*/' bundle.js | head -20\n   \n   # Find chunk files\n   ls -la | grep -E \"chunk|bundle\" | grep -v \".map\"\n   \n   # Analyze webpack configuration leaks\n   grep -rEi \"(webpack|build|config)\" *.js | grep -v \"webpackJsonp\"\n   \n   # Extract environment variables\n   grep -rE \"process\\.env\\.\" *.js\n   ```\n\n6. CLIENT-SIDE LOGIC ANALYSIS:\n   ```bash\n   # Find authentication logic\n   grep -rEi \"(login|auth|authenticate|session)\" *.js -A5 -B5\n   \n   # Identify API authentication methods\n   grep -rEi \"(Authorization|Bearer|X-API-Key|X-Auth-Token)\" *.js -A2 -B2\n   \n   # Find CORS configuration\n   grep -rEi \"Access-Control-Allow\" *.js\n   \n   # Extract form validation rules\n   grep -rEi \"(validate|validation|required)\" *.js -A10\n   \n   # Find error handling and debug information\n   grep -rEi \"(console\\.log|console\\.error|alert)\" *.js\n   \n   # Identify third-party integrations\n   grep -rEi \"(google|facebook|twitter|analytics|stripe|paypal)\" *.js\n   ```\n\n7. ADVANCED JAVASCRIPT ANALYSIS:\n   ```bash\n   # Deobfuscation with JSDetox\n   jsdetox obfuscated.js --output deobfuscated.js\n   \n   # AST analysis with Acorn/ESPrima\n   node -e \"\n   const acorn = require('acorn');\n   const fs = require('fs');\n   const code = fs.readFileSync('app.js', 'utf8');\n   const ast = acorn.parse(code, {ecmaVersion: 2020});\n   console.log(JSON.stringify(ast, null, 2));\n   \" > ast_analysis.json\n   \n   # Extract all string literals (potential endpoints/secrets)\n   grep -rEo \"'[^']*'|\\\"[^\\\"]*\\\"\" *.js | sort -u | head -100\n   \n   # Find base64 encoded content\n   grep -rEo \"[A-Za-z0-9+/]{20,}={0,2}\" *.js | head -20\n   \n   # Decode potential base64 secrets\n   echo \"U29tZVNlY3JldEtleQ==\" | base64 -d 2>/dev/null || echo \"Not valid base64\"\n   ```\n\n8. AUTOMATED JAVASCRIPT RECONNAISSANCE:\n   ```bash\n   #!/bin/bash\n   TARGET_URL=\"https://target.com\"\n   OUTPUT_DIR=\"js_recon_$(date +%Y%m%d)\"\n   mkdir -p \"$OUTPUT_DIR\"\n   \n   echo \"[*] Starting JavaScript reconnaissance for $TARGET_URL\"\n   \n   # Discover JS files\n   echo \"[*] Discovering JavaScript files...\"\n   getJS --url \"$TARGET_URL\" --complete --output \"$OUTPUT_DIR/js_files.txt\"\n   \n   # Download JS files\n   echo \"[*] Downloading JavaScript files...\"\n   mkdir -p \"$OUTPUT_DIR/js_downloads\"\n   wget -i \"$OUTPUT_DIR/js_files.txt\" -P \"$OUTPUT_DIR/js_downloads\" --no-check-certificate -q\n   \n   cd \"$OUTPUT_DIR/js_downloads\"\n   \n   # Extract endpoints\n   echo \"[*] Extracting endpoints...\"\n   for jsfile in *.js; do\n       python3 ../../../LinkFinder/linkfinder.py -i \"$jsfile\" -o cli >> \"../endpoints.txt\"\n   done\n   \n   # Hunt for secrets\n   echo \"[*] Hunting for secrets...\"\n   grep -rEi \"(api[_-]?key|access[_-]?token|client[_-]?secret)\" . > \"../secrets.txt\"\n   \n   # Analyze source maps\n   echo \"[*] Analyzing source maps...\"\n   grep -rE '\\.js\\.map' . > \"../source_maps.txt\"\n   \n   # Check for sensitive patterns\n   echo \"[*] Checking for sensitive patterns...\"\n   grep -rEi \"(password|secret|private)\" . > \"../sensitive_patterns.txt\"\n   \n   echo \"[*] JavaScript reconnaissance complete. Results in $OUTPUT_DIR/\"\n   ```\n\nWHAT TO LOOK FOR:\n- **API Endpoints**: Undocumented REST, GraphQL, WebSocket endpoints\n- **Authentication Tokens**: JWTs, API keys, OAuth tokens\n- **Cloud Credentials**: AWS, Azure, GCP access keys\n- **Database Connections**: MongoDB, Redis, SQL connection strings\n- **Private Keys**: RSA/ECDSA private keys in client code\n- **OAuth Secrets**: Client secrets, app IDs for social login\n- **Environment Variables**: process.env leaks in client code\n- **Internal URLs**: Development, staging, admin panel URLs\n- **Third-party Keys**: Analytics, payment, CDN API keys\n- **Source Maps**: Original source code with sensitive comments\n- **Debug Endpoints**: /debug/, /api/debug/, /admin endpoints\n- **Configuration Objects**: App config with sensitive settings\n- **Error Messages**: Stack traces revealing internal structure\n- **Base64 Content**: Encoded secrets or configuration data\n- **Webpack Chunks**: Split code bundles with sensitive information\n\nSECURITY IMPLICATIONS:\n- **API Abuse**: Undocumented endpoints may lack rate limiting or authentication\n- **Credential Theft**: Exposed keys enable unauthorized access to services\n- **Data Exfiltration**: Database connection strings allow direct data access\n- **Authentication Bypass**: Client-side validation can be manipulated\n- **Supply Chain Attacks**: Compromised third-party integrations\n- **Session Hijacking**: Exposed session tokens and cookies\n- **Business Logic Flaws**: Client-side logic reveals application workflows\n- **Information Disclosure**: Source maps expose internal code structure\n- **Cryptographic Weaknesses**: Hardcoded encryption keys\n- **Privacy Violations**: Exposed personal data in client-side storage\n- **Compliance Issues**: Exposed sensitive data violates regulations\n\nCOMMON PITFALLS:\n- **Minification**: Obfuscated code requires deobfuscation tools\n- **Dynamic Loading**: AJAX-loaded JS may not be discovered statically\n- **False Positives**: Test/example credentials in development code\n- **Intentionally Public**: Analytics keys are meant to be public\n- **CDN Content**: Third-party JS may contain false positives\n- **Source Maps Missing**: Production builds often remove source maps\n- **Base64 Confusion**: Not all base64 is sensitive (images, fonts)\n- **Environment Variables**: process.env may be replaced at build time\n- **CSP Violations**: Content Security Policy may block analysis\n- **CORS Restrictions**: Cross-origin requests may be blocked\n- **Rate Limiting**: Aggressive scanning may trigger protection\n\nDOCUMENTATION REQUIREMENTS:\n- Complete inventory of discovered JavaScript files\n- Extracted endpoints categorized by type (API, internal, third-party)\n- Identified secrets and credentials with risk assessment\n- Source map analysis results and sensitive code findings\n- Client-side logic vulnerabilities and bypass opportunities\n- Third-party integrations and potential supply chain risks\n- Recommendations for code hardening and secret removal\n- Evidence screenshots of exposed sensitive information\n- Comparison with secure coding practices\n- Executive summary for development team remediation\n\nTOOLS REFERENCE:\n- **LinkFinder**: https://github.com/GerbenJavado/LinkFinder (Endpoint extraction)\n- **getJS**: https://github.com/003random/getJS (JavaScript file discovery)\n- **SecretFinder**: https://github.com/m4ll0k/SecretFinder (Secret detection)\n- **JSParser**: https://github.com/nahamsec/JSParser (Advanced JS analysis)\n- **sourcemapper**: https://github.com/denandz/sourcemapper (Source map analysis)\n- **JSDetox**: https://github.com/cebrusfs/jsdetox (JavaScript deobfuscation)\n- **Acorn**: https://github.com/acornjs/acorn (JavaScript AST parser)\n- **ESPrima**: https://github.com/jquery/esprima (JavaScript parser)\n- **Nuclei**: https://github.com/projectdiscovery/nuclei (Template-based scanning)\n- **JSAnalysis**: https://github.com/notdodo/JSAnalysis (Comprehensive JS analyzer)\n\nFURTHER READING:\n- OWASP WSTG-INFO-05: Review Webpage Content for Information Leakage\n- MITRE ATT&CK: T1593 Search Open Websites/Domains\n- \"Hunting for Secrets in JavaScript\" - PortSwigger Research\n- \"JavaScript Supply Chain Attacks\" - Snyk Blog\n- \"Source Map Security\" - Mozilla Developer Network\n- \"Client-Side Security Best Practices\" - OWASP Cheat Sheet\n- \"JavaScript Deobfuscation Techniques\" - Black Hat Presentations\n- \"API Discovery through JavaScript Analysis\" - Bug Bounty Reports",
      "tags": ["recon", "javascript", "api-endpoints", "secrets", "source-maps", "client-side", "deobfuscation", "credentials"]
    },
    {
      "id": "parameter_discovery",
      "title": "Parameter discovery",
      "content": "OBJECTIVE: Identify all input parameters including GET/POST parameters, API parameters, and hidden form fields to establish complete attack surface for injection testing and fuzzing.\n\nACADEMIC BACKGROUND:\nParameter discovery is fundamental to comprehensive web application testing. According to OWASP WSTG-INFO-07 (Map Application Architecture), identifying all input parameters is essential for complete attack surface mapping. The MITRE ATT&CK framework categorizes parameter analysis as T1590 (Gather Victim Network Information) under Reconnaissance.\n\nWeb applications accept input through various mechanisms:\n- **URL Parameters**: GET request query strings\n- **Form Fields**: POST data and hidden inputs\n- **API Parameters**: REST, GraphQL, SOAP parameters\n- **Headers**: Custom HTTP headers\n- **Cookies**: Session and preference data\n- **File Uploads**: Multipart form data\n- **AJAX Requests**: JSON and XML payloads\n\nMETHODOLOGIES:\n1. **Passive Parameter Collection**: Archive analysis and proxy logs\n2. **Active Parameter Discovery**: Fuzzing and brute-forcing\n3. **API Parameter Enumeration**: Schema analysis and introspection\n4. **Form Analysis**: HTML parsing and input extraction\n5. **Header/Cookie Analysis**: Request/response header inspection\n\nSTEP-BY-STEP PROCESS:\n\n1. URL PARAMETER EXTRACTION FROM ARCHIVES:\n   ```bash\n   # ParamSpider (comprehensive parameter collection from wayback/otx)\n   python3 paramspider.py -d target.com -o params.txt\n   \n   # Extract unique parameter names\n   cat params.txt | grep -oP '(?<=[?&])[^=&]+' | sort -u > unique_params.txt\n   \n   # GAU (Get All URLs - comprehensive archive collection)\n   echo \"target.com\" | gau | grep \"\\?\" > urls_with_params.txt\n   \n   # Extract parameters from GAU results\n   cat urls_with_params.txt | grep -oP '(?<=[?&])[^=&]+' | sort | uniq -c | sort -nr > param_frequency.txt\n   \n   # WaybackURLs (Internet Archive)\n   waybackurls target.com | grep \"\\?\" > wayback_params.txt\n   \n   # Combine all parameter sources\n   cat params.txt wayback_params.txt | grep -oP '(?<=[?&])[^=&]+' | sort -u > all_params.txt\n   ```\n\n2. HIDDEN PARAMETER DISCOVERY THROUGH FUZZING:\n   ```bash\n   # Arjun (advanced HTTP parameter discovery)\n   arjun -u \"https://target.com/api/users\" -m GET\n   arjun -u \"https://target.com/search\" -m POST -b \"username=admin&password=pass\"\n   \n   # Bulk parameter discovery\n   cat endpoints.txt | while read url; do\n       arjun -u \"$url\" -oT arjun_results.txt\n   done\n   \n   # x8 (hidden parameter fuzzer)\n   x8 -u \"https://target.com/api/users\" -w /usr/share/seclists/Discovery/Web-Content/burp-parameter-names.txt\n   \n   # Custom parameter wordlist creation\n   cat /usr/share/seclists/Discovery/Web-Content/burp-parameter-names.txt \\\n       /usr/share/seclists/Discovery/Web-Content/api/api-endpoints.txt \\\n       | sort -u > custom_params.txt\n   \n   # ffuf parameter fuzzing\n   ffuf -u \"https://target.com/api/FUZZ\" -w custom_params.txt -mc 200,301,302,403\n   ```\n\n3. API PARAMETER ENUMERATION:\n   ```bash\n   # GraphQL introspection (if enabled)\n   curl -X POST https://target.com/graphql \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"query\":\"{__schema{types{name,fields{name}}}}\"}' | jq\n   \n   # GraphQL field enumeration\n   python3 graphqlmap.py -u https://target.com/graphql -v\n   \n   # REST API documentation discovery\n   curl -s https://target.com/swagger.json | jq '.paths | keys[]'\n   curl -s https://target.com/api-docs | grep -oP '\"/[^\"]*\"' | sort -u\n   \n   # OpenAPI/Swagger parameter extraction\n   curl -s https://target.com/swagger.json | jq '.paths[][].parameters[]?.name' | sort -u\n   \n   # SOAP WSDL parameter analysis\n   curl -s https://target.com/service.wsdl | grep -oP '<message name=\"[^\"]*\"' | sort -u\n   \n   # JSON API schema analysis\n   curl -s https://target.com/api/schema | jq '.properties | keys[]'\n   ```\n\n4. FORM PARAMETER IDENTIFICATION:\n   ```bash\n   # Extract all form inputs from HTML\n   curl -s https://target.com | grep -Eo '<(input|select|textarea)[^>]*>' | grep -Eo 'name=\"[^\"]*\"' | sort -u\n   \n   # Find hidden form fields\n   curl -s https://target.com | grep -Eo '<input[^>]*type=\"hidden\"[^>]*>' | grep -Eo 'name=\"[^\"]*\"'\n   \n   # Extract form actions and methods\n   curl -s https://target.com | grep -Eo '<form[^>]*>' | grep -Eo '(action|method)=\"[^\"]*\"'\n   \n   # Parse forms with htmlq\n   curl -s https://target.com | htmlq 'form input' --attribute name\n   \n   # Find file upload forms\n   curl -s https://target.com | grep -A5 -B5 'type=\"file\"'\n   \n   # Extract CSRF tokens and anti-forgery fields\n   curl -s https://target.com | grep -Ei '(csrf|_token|authenticity_token)' | grep -Eo 'name=\"[^\"]*\"'\n   ```\n\n5. HEADER AND COOKIE PARAMETER ANALYSIS:\n   ```bash\n   # Extract custom headers from responses\n   curl -I https://target.com | grep -E '^X-' | sort\n   \n   # Cookie parameter analysis\n   curl -I https://target.com | grep -E '^Set-Cookie' | cut -d: -f2 | cut -d= -f1\n   \n   # Test header-based parameters\n   curl -H \"X-API-Key: test\" https://target.com/api\n   curl -H \"X-Forwarded-For: 127.0.0.1\" https://target.com\n   \n   # JWT token parameter extraction\n   curl -H \"Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\" https://target.com/api\n   ```\n\n6. JAVASCRIPT PARAMETER DISCOVERY:\n   ```bash\n   # Extract parameters from JavaScript\n   grep -rEo \"\\?[^'\\\"\\s]*\" js_files/ | grep -oP '(?<=[?&])[^=&]+' | sort -u\n   \n   # Find AJAX parameter usage\n   grep -rEi \"(fetch|axios|xmlhttprequest)\" js_files/ -A5 | grep -Eo \"\\?[^'\\\"\\s]*\"\n   \n   # Extract API call parameters\n   grep -rEo \"/api/[^'\\\"\\s]*\\?[^'\\\"\\s]*\" js_files/ | sort -u\n   \n   # Find parameter encoding/decoding\n   grep -rEi \"(encodeURIComponent|decodeURIComponent|btoa|atob)\" js_files/\n   ```\n\n7. ADVANCED PARAMETER FUZZING:\n   ```bash\n   # Burp Intruder parameter discovery\n   # 1. Send request to Intruder\n   # 2. Set payload position on parameter name\n   # 3. Use parameter wordlist\n   # 4. Filter responses for different behavior\n   \n   # wfuzz parameter discovery\n   wfuzz -u \"https://target.com/api?FUZZ=test\" -w params.txt --hc 404\n   \n   # dirsearch parameter mode\n   dirsearch -u \"https://target.com/page?FUZZ=value\" -w params.txt\n   \n   # Custom parameter injection testing\n   for param in $(cat params.txt); do\n       echo \"Testing parameter: $param\"\n       curl -s \"https://target.com/api?$param=test\" | grep -q \"error\\|invalid\" || echo \"Parameter $param accepted\"\n   done\n   ```\n\n8. PARAMETER CLASSIFICATION AND ANALYSIS:\n   ```bash\n   # Classify parameters by potential vulnerability\n   # SQL injection candidates\n   grep -E \"(id|page|sort|order|filter)\" params.txt > sql_candidates.txt\n   \n   # XSS candidates\n   grep -E \"(search|query|keyword|term)\" params.txt > xss_candidates.txt\n   \n   # File inclusion candidates\n   grep -E \"(file|path|include|require|template)\" params.txt > lfi_candidates.txt\n   \n   # Command injection candidates\n   grep -E \"(cmd|exec|command|run)\" params.txt > cmd_candidates.txt\n   \n   # Open redirect candidates\n   grep -E \"(url|redirect|return|next|callback)\" params.txt > redirect_candidates.txt\n   \n   # SSTI candidates\n   grep -E \"(template|render|format|view)\" params.txt > ssti_candidates.txt\n   \n   # IDOR candidates\n   grep -E \"(user|account|profile|id)\" params.txt > idor_candidates.txt\n   ```\n\n9. AUTOMATED PARAMETER DISCOVERY WORKFLOW:\n   ```bash\n   #!/bin/bash\n   TARGET=\"https://target.com\"\n   OUTPUT_DIR=\"param_discovery_$(date +%Y%m%d)\"\n   mkdir -p \"$OUTPUT_DIR\"\n   \n   echo \"[*] Starting comprehensive parameter discovery for $TARGET\"\n   \n   # Passive collection\n   echo \"[*] Passive parameter collection...\"\n   python3 paramspider.py -d \"${TARGET#https://}\" -o \"$OUTPUT_DIR/paramspider.txt\"\n   echo \"target.com\" | gau | grep \"\\?\" > \"$OUTPUT_DIR/gau_params.txt\"\n   waybackurls \"${TARGET#https://}\" | grep \"\\?\" > \"$OUTPUT_DIR/wayback_params.txt\"\n   \n   # Extract unique parameters\n   cat \"$OUTPUT_DIR\"/*.txt | grep -oP '(?<=[?&])[^=&]+' | sort -u > \"$OUTPUT_DIR/all_params.txt\"\n   \n   # Active discovery\n   echo \"[*] Active parameter discovery...\"\n   cat endpoints.txt | head -10 | while read url; do\n       arjun -u \"$url\" -oT \"$OUTPUT_DIR/arjun_results.txt\" 2>/dev/null\n   done\n   \n   # Form analysis\n   echo \"[*] Form parameter analysis...\"\n   curl -s \"$TARGET\" | grep -Eo '<input[^>]*name=\"[^\"]*\"' | grep -Eo 'name=\"[^\"]*\"' | sort -u > \"$OUTPUT_DIR/form_params.txt\"\n   \n   # JavaScript analysis\n   echo \"[*] JavaScript parameter extraction...\"\n   grep -rEo \"\\?[^'\\\"\\s]*\" js_files/ 2>/dev/null | grep -oP '(?<=[?&])[^=&]+' | sort -u >> \"$OUTPUT_DIR/all_params.txt\"\n   \n   # Classification\n   echo \"[*] Parameter classification...\"\n   grep -E \"(id|page|sort|order)\" \"$OUTPUT_DIR/all_params.txt\" > \"$OUTPUT_DIR/sql_candidates.txt\"\n   grep -E \"(search|query|term)\" \"$OUTPUT_DIR/all_params.txt\" > \"$OUTPUT_DIR/xss_candidates.txt\"\n   \n   echo \"[*] Parameter discovery complete. Results in $OUTPUT_DIR/\"\n   echo \"[*] Total unique parameters found: $(wc -l < \"$OUTPUT_DIR/all_params.txt\")\"\n   ```\n\nWHAT TO LOOK FOR:\n- **SQL Injection Candidates**: id, page, sort, order, filter parameters\n- **XSS Candidates**: search, query, keyword, term parameters\n- **File Inclusion**: file, path, include, require, template parameters\n- **Command Injection**: cmd, exec, command, run parameters\n- **Open Redirect**: url, redirect, return, next, callback parameters\n- **Server-Side Template Injection**: template, render, format, view parameters\n- **IDOR Candidates**: user, account, profile, id parameters\n- **File Upload**: file, upload, attachment parameters\n- **Authentication Bypass**: admin, debug, test parameters\n- **API Versioning**: v1, v2, version, api parameters\n- **Debug Parameters**: debug, verbose, trace parameters\n- **Hidden Functionality**: undocumented parameters in archives\n- **Mass Assignment**: parameters that accept object notation\n- **Parameter Pollution**: duplicate parameter handling\n\nSECURITY IMPLICATIONS:\n- **Injection Attacks**: SQL, NoSQL, command injection through parameters\n- **Cross-Site Scripting**: Reflected/stored XSS via input parameters\n- **Directory Traversal**: File inclusion through path parameters\n- **Open Redirects**: Parameter-based URL redirection attacks\n- **Server-Side Request Forgery**: URL parameters allowing internal requests\n- **Template Injection**: SSTI through template parameters\n- **Insecure Direct Object References**: ID parameters exposing other users' data\n- **Mass Assignment**: Object parameters allowing privilege escalation\n- **Parameter Tampering**: Logic bypass through modified parameters\n- **Information Disclosure**: Debug parameters leaking sensitive data\n- **Rate Limit Bypass**: Hidden parameters bypassing protections\n\nCOMMON PITFALLS:\n- **Authentication Requirements**: Some parameters only available when logged in\n- **AJAX-Only Parameters**: Parameters used only in JavaScript requests\n- **Rate Limiting**: Aggressive fuzzing triggers IP blocks\n- **Parameter Encoding**: URL encoding may hide parameter discovery\n- **Context-Dependent**: Same parameter name may behave differently per endpoint\n- **False Positives**: Parameters that accept any input but don't process it\n- **API Keys Required**: Some endpoints require authentication tokens\n- **CSRF Tokens**: Anti-forgery tokens may prevent parameter testing\n- **WAF Interference**: Web application firewalls may block fuzzing\n- **Parameter Pollution**: Multiple same-named parameters may confuse analysis\n- **State-Dependent**: Parameters may only work in specific application states\n\nDOCUMENTATION REQUIREMENTS:\n- Complete parameter inventory with sources (passive/active)\n- Parameter classification by vulnerability type\n- Endpoint-to-parameter mapping\n- Authentication requirements per parameter set\n- Parameter behavior analysis (accepted values, responses)\n- Hidden parameter discovery results\n- API schema documentation analysis\n- Form parameter extraction results\n- Recommendations for parameter hardening\n- Test cases for parameter-based vulnerabilities\n- Executive summary of attack surface expansion\n\nTOOLS REFERENCE:\n- **Arjun**: https://github.com/s0md3v/Arjun (HTTP parameter discovery)\n- **ParamSpider**: https://github.com/devanshbatham/ParamSpider (Parameter collection from archives)\n- **x8**: https://github.com/Sh1Yo/x8 (Hidden parameter fuzzer)\n- **GAU**: https://github.com/lc/gau (URL archive collection)\n- **WaybackURLs**: https://github.com/tomnomnom/waybackurls (Archive URL extraction)\n- **wfuzz**: https://github.com/xmendez/wfuzz (Web application fuzzer)\n- **dirsearch**: https://github.com/maurosoria/dirsearch (Directory scanner with parameter support)\n- **Burp Intruder**: Built-in parameter fuzzing capabilities\n- **Postman**: API parameter discovery and testing\n- **GraphQL Voyager**: GraphQL schema visualization\n\nFURTHER READING:\n- OWASP WSTG-INFO-07: Map Application Architecture\n- MITRE ATT&CK: T1590 Gather Victim Network Information\n- \"Parameter Discovery Techniques\" - PortSwigger Research\n- \"Hidden Parameter Attacks\" - Black Hat Presentations\n- \"API Parameter Analysis\" - OWASP API Security Top 10\n- \"GraphQL Security Testing\" - GraphQL Foundation\n- \"Parameter Tampering\" - OWASP Cheat Sheet Series\n- \"Mass Assignment Vulnerabilities\" - CWE-915\n- \"Server-Side Request Forgery\" - OWASP Testing Guide",
      "tags": ["recon", "parameters", "fuzzing", "api", "forms", "injection", "input-discovery", "attack-surface"]
    },
    {
      "id": "screenshot_capture",
      "title": "Screenshot capture",
      "content": "OBJECTIVE: Create visual documentation of all discovered web assets for evidence collection, visual comparison, and identification of interesting pages requiring further investigation.\n\nACADEMIC BACKGROUND:\nScreenshot capture provides visual reconnaissance of web applications, enabling rapid identification of interesting targets and documentation of attack surfaces. According to OWASP WSTG-INFO-01 (Conduct Search Engine Discovery and Reconnaissance Activities), visual reconnaissance complements technical discovery by revealing application interfaces and potential vulnerabilities.\n\nVisual reconnaissance serves multiple purposes:\n- **Interface Identification**: Admin panels, login pages, error pages\n- **Technology Fingerprinting**: CMS, framework, and version identification\n- **Content Discovery**: Hidden or unusual pages requiring investigation\n- **Change Detection**: Visual comparison for security monitoring\n- **Evidence Collection**: Documenting exposed assets and vulnerabilities\n- **Reporting**: Visual proof for penetration testing reports\n\nMETHODOLOGIES:\n1. **Bulk Screenshot Capture**: Automated tools for large-scale reconnaissance\n2. **Responsive Testing**: Multiple viewport sizes for mobile/desktop analysis\n3. **Authenticated Capture**: Screenshots requiring login credentials\n4. **Dynamic Content Handling**: JavaScript rendering and interaction\n5. **Change Detection**: Visual diffing for monitoring\n\nSTEP-BY-STEP PROCESS:\n\n1. AUTOMATED SCREENSHOT CAPTURE TOOLS:\n   ```bash\n   # EyeWitness (comprehensive reconnaissance with reporting)\n   python3 EyeWitness.py -f urls.txt --web --timeout 30 -d eyewitness_report\n   \n   # Gowitness (fast Go-based screenshot tool)\n   gowitness scan file -f urls.txt --threads 4 --timeout 30 --resolution 1920x1080\n   \n   # Convert gowitness results to HTML report\n   gowitness report generate\n   \n   # Aquatone (pipeline-friendly with passive reconnaissance)\n   cat urls.txt | aquatone -out aquatone_results --threads 4\n   \n   # Webscreenshot (simple Python tool)\n   webscreenshot -i urls.txt -o screenshots/ --timeout 30\n   \n   # HTTPScreenshot (Nmap script integration)\n   nmap -p 80,443 --script http-screenshot target.com/24 -oA nmap_screenshots\n   \n   # Masscan integration for large-scale scanning\n   masscan -p80,443,8080,8443 target.com/24 --rate=1000 | awk '{print $6\":\"$4}' | gowitness scan file -f -\n   ```\n\n2. RESPONSIVE DESIGN AND MULTI-VIEWPORT CAPTURE:\n   ```bash\n   # Desktop resolution\n   gowitness scan single --url https://target.com --resolution 1920x1080 --destination desktop/\n   \n   # Tablet resolution\n   gowitness scan single --url https://target.com --resolution 768x1024 --destination tablet/\n   \n   # Mobile resolution\n   gowitness scan single --url https://target.com --resolution 375x667 --destination mobile/\n   \n   # Custom resolutions for specific devices\n   gowitness scan single --url https://target.com --resolution 414x896  # iPhone XR\n   gowitness scan single --url https://target.com --resolution 360x640   # Android\n   \n   # EyeWitness responsive capture\n   python3 EyeWitness.py -f urls.txt --web --resolution 1920x1080,768x1024,375x667 -d responsive_screenshots\n   \n   # Compare responsive behavior\n   # Look for different content/functionality across viewports\n   ```\n\n3. AUTHENTICATED AND SESSION-BASED SCREENSHOTS:\n   ```bash\n   # EyeWitness with cookies\n   python3 EyeWitness.py -f urls.txt --web --cookie \"session=abc123; auth_token=xyz789\" -d auth_screenshots\n   \n   # Gowitness with authentication\n   gowitness scan single --url https://target.com/admin --cookies \"session=abc123\" --destination admin_panel/\n   \n   # Aquatone with custom headers\n   cat urls.txt | aquatone -H \"Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9...\" -H \"Cookie: session=abc123\"\n   \n   # Selenium-based authenticated capture\n   python3 authenticated_screenshot.py --url https://target.com --username admin --password pass123\n   \n   # Burp Suite integration\n   # 1. Configure browser with Burp proxy\n   # 2. Login through Burp\n   # 3. Use screenshot tool with Burp's cookies\n   ```\n\n4. DYNAMIC CONTENT AND JAVASCRIPT RENDERING:\n   ```bash\n   # Chrome headless screenshots (handles JavaScript)\n   chromium --headless --disable-gpu --screenshot=target.png --window-size=1920,1080 https://target.com\n   \n   # Firefox headless\n   firefox --headless --screenshot target.png https://target.com\n   \n   # Puppeteer script for complex interactions\n   cat > capture.js << 'EOF'\n   const puppeteer = require('puppeteer');\n   \n   (async () => {\n     const browser = await puppeteer.launch();\n     const page = await browser.newPage();\n     await page.setViewport({width: 1920, height: 1080});\n     await page.goto('https://target.com');\n     \n     // Wait for dynamic content\n     await page.waitForSelector('.dynamic-content');\n     \n     // Interact with page\n     await page.click('#load-more');\n     await page.waitForTimeout(2000);\n     \n     await page.screenshot({path: 'dynamic_screenshot.png', fullPage: true});\n     await browser.close();\n   })();\n   EOF\n   node capture.js\n   \n   # Selenium with Python\n   python3 selenium_screenshot.py --url https://target.com --wait-for .dynamic-element\n   ```\n\n5. BULK SCREENSHOT WORKFLOW:\n   ```bash\n   #!/bin/bash\n   URL_FILE=\"discovered_urls.txt\"\n   OUTPUT_DIR=\"screenshots_$(date +%Y%m%d)\"\n   mkdir -p \"$OUTPUT_DIR\"/{desktop,tablet,mobile,authenticated}\n   \n   echo \"[*] Starting bulk screenshot capture...\"\n   echo \"[*] Found $(wc -l < \"$URL_FILE\") URLs to capture\"\n   \n   # Desktop screenshots\n   echo \"[*] Capturing desktop screenshots...\"\n   gowitness scan file -f \"$URL_FILE\" --threads 4 --timeout 30 --resolution 1920x1080 --destination \"$OUTPUT_DIR/desktop\"\n   \n   # Tablet screenshots\n   echo \"[*] Capturing tablet screenshots...\"\n   gowitness scan file -f \"$URL_FILE\" --threads 4 --timeout 30 --resolution 768x1024 --destination \"$OUTPUT_DIR/tablet\"\n   \n   # Mobile screenshots\n   echo \"[*] Capturing mobile screenshots...\"\n   gowitness scan file -f \"$URL_FILE\" --threads 4 --timeout 30 --resolution 375x667 --destination \"$OUTPUT_DIR/mobile\"\n   \n   # Generate HTML report\n   echo \"[*] Generating HTML report...\"\n   gowitness report generate --destination \"$OUTPUT_DIR\"\n   \n   # Create index of interesting screenshots\n   echo \"[*] Analyzing screenshots for interesting content...\"\n   find \"$OUTPUT_DIR\" -name \"*.png\" -exec identify -verbose {} \\; | grep -E \"(login|admin|error|403|404|500)\" > \"$OUTPUT_DIR/interesting_screenshots.txt\"\n   \n   echo \"[*] Screenshot capture complete. Results in $OUTPUT_DIR/\"\n   echo \"[*] Check interesting_screenshots.txt for potentially vulnerable pages\"\n   ```\n\n6. CHANGE DETECTION AND MONITORING:\n   ```bash\n   # Baseline screenshots\n   gowitness scan file -f urls.txt --destination baseline_$(date +%Y%m%d)\n   \n   # Periodic monitoring\n   #!/bin/bash\n   BASELINE_DIR=\"baseline_20240101\"\n   CURRENT_DIR=\"current_$(date +%Y%m%d)\"\n   \n   gowitness scan file -f urls.txt --destination \"$CURRENT_DIR\"\n   \n   # Compare with baseline (requires imagemagick)\n   for url in $(cat urls.txt); do\n       baseline_img=\"$BASELINE_DIR/$(echo $url | sed 's|https*://||' | tr '/' '_' | tr ':' '_').png\"\n       current_img=\"$CURRENT_DIR/$(echo $url | sed 's|https*://||' | tr '/' '_' | tr ':' '_').png\"\n       \n       if [ -f \"$baseline_img\" ] && [ -f \"$current_img\" ]; then\n           difference=$(compare -metric AE \"$baseline_img\" \"$current_img\" /dev/null 2>&1)\n           if [ \"$difference\" -gt 1000 ]; then\n               echo \"Significant change detected for $url\"\n               # Send alert or take action\n           fi\n       fi\n   done\n   ```\n\n7. ADVANCED SCREENSHOT ANALYSIS:\n   ```bash\n   # OCR text extraction from screenshots\n   for img in screenshots/*.png; do\n       tesseract \"$img\" \"$(basename \"$img\" .png)\" -l eng\n       echo \"=== OCR Results for $(basename \"$img\") ===\"\n       cat \"$(basename \"$img\" .png).txt\"\n   done\n   \n   # Image classification (requires machine learning)\n   # python3 classify_screenshots.py --directory screenshots/ --model security_pages.model\n   \n   # Color analysis for visual patterns\n   convert screenshot.png -colors 5 -unique-colors txt: | grep -v \"#\"\n   \n   # Detect login forms automatically\n   python3 detect_login_forms.py --image screenshot.png\n   \n   # Extract dominant colors (may indicate branding/themes)\n   convert screenshot.png -scale 1x1 -format \"%[pixel:p{0,0}]\" info:\n   ```\n\n8. SPECIALIZED CAPTURE SCENARIOS:\n   ```bash\n   # Error page capture\n   curl -s \"https://target.com/nonexistent\" | head -1 > /dev/null && \\\n   gowitness scan single --url \"https://target.com/404-test\" --destination error_pages/\n   \n   # Admin panel discovery and capture\n   for path in admin administrator manage cpannel wp-admin; do\n       gowitness scan single --url \"https://target.com/$path\" --destination admin_discovery/\n   done\n   \n   # API documentation capture\n   for path in api docs swagger openapi; do\n       gowitness scan single --url \"https://target.com/$path\" --destination api_docs/\n   done\n   \n   # Development environment capture\n   for subdomain in dev staging test qa beta; do\n       gowitness scan single --url \"https://$subdomain.target.com\" --destination dev_envs/\n   done\n   ```\n\nWHAT TO LOOK FOR:\n- **Login Interfaces**: Admin panels, user portals, authentication pages\n- **Error Pages**: Custom 404/500 pages revealing technology stack\n- **Admin Panels**: /admin, /administrator, /manage, /cpanel\n- **Development Environments**: /dev, /staging, /test, staging.target.com\n- **API Documentation**: /api, /swagger, /docs, /openapi\n- **File Upload Interfaces**: Forms accepting file uploads\n- **Database Interfaces**: phpMyAdmin, Adminer, database management tools\n- **Monitoring Dashboards**: Nagios, Grafana, Kibana, exposed metrics\n- **Version Information**: Software versions in page footers or headers\n- **Default Installations**: Default pages of CMS, frameworks, applications\n- **Unusual Applications**: Legacy systems, custom software, IoT interfaces\n- **Security Headers Missing**: Pages without HTTPS or security indicators\n- **Responsive Design Issues**: Broken layouts on different viewports\n- **Content Changes**: Significant visual differences indicating updates\n\nSECURITY IMPLICATIONS:\n- **Exposed Admin Interfaces**: Direct access to management functions\n- **Default Credentials**: Factory-default login pages\n- **Development Exposure**: Staging environments with production data\n- **Information Disclosure**: Version information aiding exploit research\n- **Legacy Systems**: Outdated software with known vulnerabilities\n- **Misconfigurations**: Exposed databases, file managers, debug interfaces\n- **Supply Chain Risks**: Third-party applications with vulnerabilities\n- **Shadow IT**: Undocumented systems outside security controls\n- **Data Exposure**: Database interfaces with sensitive information\n- **Authentication Bypass**: Weak or missing authentication on admin pages\n\nCOMMON PITFALLS:\n- **JavaScript-Heavy Sites**: Dynamic content not captured without proper rendering\n- **Authentication Required**: Many interesting pages require login\n- **Rate Limiting**: Bulk capture triggers anti-bot protections\n- **Session Expiry**: Authenticated screenshots become invalid\n- **Viewport Differences**: Mobile versions may have different functionality\n- **AJAX Content**: Asynchronous loading not captured in static screenshots\n- **CDN Interference**: Content delivery networks may serve different content\n- **Browser Differences**: Chrome/Firefox may render pages differently\n- **SSL Certificate Issues**: Invalid certificates prevent screenshot capture\n- **Resource Loading**: Images, CSS, JS failures affect page appearance\n- **Timing Issues**: Pages requiring time to fully load\n\nDOCUMENTATION REQUIREMENTS:\n- Complete screenshot gallery organized by category\n- Responsive design analysis across viewports\n- Authentication state documentation\n- Interesting page identification and prioritization\n- Change detection results and alerts\n- OCR text extraction from screenshots\n- Technology identification from visual cues\n- Recommendations for securing exposed interfaces\n- Evidence collection for penetration testing reports\n- Timeline of visual changes for monitoring\n\nTOOLS REFERENCE:\n- **EyeWitness**: https://github.com/FortyNorthSecurity/EyeWitness (Comprehensive web screenshot tool)\n- **Gowitness**: https://github.com/sensepost/gowitness (Fast Go-based screenshot utility)\n- **Aquatone**: https://github.com/michenriksen/aquatone (Pipeline-friendly reconnaissance)\n- **Webscreenshot**: https://github.com/maaaaz/webscreenshot (Simple Python screenshot tool)\n- **Puppeteer**: https://pptr.dev/ (Headless Chrome automation)\n- **Selenium**: https://www.selenium.dev/ (Browser automation framework)\n- **ImageMagick**: https://imagemagick.org/ (Image comparison and analysis)\n- **Tesseract**: https://github.com/tesseract-ocr/tesseract (OCR text extraction)\n- **Chrome Headless**: Built-in Chrome screenshot capabilities\n- **Firefox Headless**: Built-in Firefox screenshot capabilities\n\nFURTHER READING:\n- OWASP WSTG-INFO-01: Conduct Search Engine Discovery\n- \"Visual Reconnaissance Techniques\" - Black Hat Presentations\n- \"Automated Screenshot Analysis\" - DEF CON Talks\n- \"Web Application Visual Fingerprinting\" - Security Research\n- \"Change Detection in Web Applications\" - Academic Papers\n- \"Headless Browser Security Testing\" - OWASP Resources\n- \"Screenshot-based Vulnerability Discovery\" - Bug Bounty Reports\n- \"Visual Security Assessment\" - Penetration Testing Guides",
      "tags": ["recon", "screenshots", "visual-recon", "evidence-collection", "responsive-design", "change-detection", "documentation"]
    },
    {
      "id": "social_media_reconnaissance",
      "title": "Social media reconnaissance",
      "content": "OBJECTIVE: Gather intelligence from social media platforms about target organization and personnel for social engineering preparation.\n\nACADEMIC BACKGROUND:\nSocial media reconnaissance leverages public information from social platforms to build comprehensive profiles of organizations and individuals. According to NIST SP 800-115 (Technical Guide to Information Security Testing and Assessment), social engineering preparation is a critical component of penetration testing methodology.\n\nThe MITRE ATT&CK framework categorizes social media reconnaissance as T1593 (Search Open Websites/Domains) and T1590 (Gather Victim Identity Information), emphasizing that social media provides rich intelligence for social engineering attacks.\n\nSocial media intelligence gathering serves multiple purposes:\n- **Personnel Identification**: Employee names, roles, and contact information\n- **Organizational Structure**: Company hierarchy and reporting relationships\n- **Technology Insights**: Tools, frameworks, and infrastructure mentions\n- **Security Awareness**: Employee posting habits and security consciousness\n- **Social Engineering Vectors**: Personal information for pretexting attacks\n- **Supply Chain Mapping**: Partner and vendor relationships\n\nMETHODOLOGIES:\n1. **Professional Networks**: LinkedIn, Xing, professional profiles\n2. **Social Platforms**: Twitter/X, Facebook, Instagram, personal information\n3. **Code Repositories**: GitHub, GitLab, Bitbucket, code and commit analysis\n4. **Forum Communities**: Reddit, Stack Overflow, industry forums\n5. **Passive Monitoring**: Social media listening and alerting\n\nSTEP-BY-STEP PROCESS:\n\n1. LINKEDIN RECONNAISSANCE:\n   ```bash\n   # CrossLinked (advanced LinkedIn scraping)\n   python3 CrossLinked.py -f '{first}.{last}@target.com' \"Target Company\"\n   \n   # linkedin2username (username enumeration)\n   python3 linkedin2username.py -c \"Target Company\" -n 100 -d target.com\n   \n   # LinkedIn company search\n   # Search for: site:linkedin.com \"target company\" employees\n   # Use Google dorks for employee discovery\n   \n   # Hunter.io integration for email validation\n   curl \"https://api.hunter.io/v2/domain-search?domain=target.com&api_key=YOUR_API_KEY\" | jq '.data.emails[] | select(.sources[].domain == \"linkedin.com\")'\n   \n   # Extract employee titles and departments\n   # Look for: CEO, CTO, CISO, IT staff, developers, administrators\n   \n   # Company page analysis\n   curl -s \"https://www.linkedin.com/company/target-company\" | grep -oP '(?<=data-entity-urn=\"urn:li:fs_miniCompany:)\\d+' > company_id.txt\n   ```\n\n2. TWITTER/X INTELLIGENCE GATHERING:\n   ```bash\n   # Twint (Twitter scraping without API limits)\n   twint -s \"target.com OR @targetcompany OR #targetcompany\" --email --phone -o twitter_intel.txt\n   \n   # Search for employee handles\n   twint -s \"site:linkedin.com/in/ AND target.com\" --user-full -o employee_twitter.txt\n   \n   # Monitor company tweets\n   twint -s \"from:targetcompany\" --since \"2024-01-01\" -o company_tweets.txt\n   \n   # Find technology mentions\n   twint -s \"target.com AND (aws OR azure OR gcp OR kubernetes OR docker)\" -o tech_mentions.txt\n   \n   # Employee personal tweets\n   for employee in $(cat employee_handles.txt); do\n       twint -s \"from:$employee\" --limit 100 -o \"${employee}_tweets.txt\"\n   done\n   \n   # Twitter advanced search operators\n   # from:user since:2024-01-01 until:2024-12-31 target.com\n   # target.com filter:replies (replies mentioning company)\n   # target.com min_faves:10 (popular mentions)\n   ```\n\n3. GITHUB/GITLAB RECONNAISSANCE:\n   ```bash\n   # GitHub organization enumeration\n   curl -s \"https://api.github.com/orgs/targetcompany/repos?per_page=100\" | jq -r '.[] | .name + \" - \" + .description'\n   \n   # Find employee GitHub accounts\n   curl -s \"https://api.github.com/search/users?q=target.com\" | jq -r '.items[].login'\n   \n   # Repository analysis\n   for repo in $(curl -s \"https://api.github.com/orgs/targetcompany/repos\" | jq -r '.[].name'); do\n       echo \"=== $repo ===\"\n       curl -s \"https://api.github.com/repos/targetcompany/$repo\" | jq '.language, .updated_at, .forks_count, .stargazers_count'\n   done\n   \n   # GitDorker (GitHub dorking for secrets)\n   python3 GitDorker.py -tf github_tokens.txt -q target.com -d dorks/ -o github_findings.txt\n   \n   # Common GitHub dorks\n   # filename:.env target.com\n   # filename:config.json password\n   # filename:.git/config target.com\n   # extension:sql target.com\n   \n   # GitLab reconnaissance\n   curl -s \"https://gitlab.com/api/v4/groups?search=target\" | jq\n   \n   # Commit analysis for sensitive information\n   curl -s \"https://api.github.com/repos/targetcompany/repo/commits\" | jq -r '.[].commit.message' | grep -iE \"(password|secret|key|token)\"\n   ```\n\n4. FACEBOOK AND INSTAGRAM INTELLIGENCE:\n   ```bash\n   # Facebook page analysis\n   curl -s \"https://www.facebook.com/pg/targetcompany/posts/\" | grep -oP '(?<=data-ft=\")\\{[^}]*\\}' | head -5\n   \n   # Facebook Graph API (requires access token)\n   curl \"https://graph.facebook.com/v18.0/search?q=target%20company&type=page&access_token=YOUR_TOKEN\"\n   \n   # Instagram business account discovery\n   # Search for company hashtags and handles\n   \n   # Employee personal profiles\n   # Cross-reference LinkedIn with Instagram\n   \n   # Photo metadata analysis\n   # Download profile pictures and extract EXIF data\n   exiftool profile_picture.jpg\n   ```\n\n5. REDDIT AND FORUM RECONNAISSANCE:\n   ```bash\n   # Reddit search\n   curl -s \"https://www.reddit.com/search.json?q=target.com&sort=new&limit=100\" | jq '.data.children[].data | {title, selftext, author, subreddit}'\n   \n   # Company subreddit discovery\n   curl -s \"https://www.reddit.com/search.json?q=subreddit:targetcompany\" | jq\n   \n   # Employee Reddit accounts\n   # Search for company email domains in Reddit\n   \n   # Stack Overflow company mentions\n   curl -s \"https://api.stackexchange.com/2.3/search?order=desc&sort=activity&intitle=target.com&site=stackoverflow\" | jq '.items[] | {title, link, owner}'\n   \n   # Technology forum analysis\n   # Hacker News, Reddit r/netsec, r/cybersecurity\n   ```\n\n6. DISCORD AND SLACK COMMUNITY ANALYSIS:\n   ```bash\n   # Discord server discovery\n   # Search for company Discord invites\n   # target.com discord, target.gg, discord.target.com\n   \n   # Slack community detection\n   # target.slack.com, slack.target.com\n   \n   # Community analysis for insider information\n   # Technology discussions, internal tool mentions\n   # Employee complaints, security discussions\n   ```\n\n7. SOCIAL MEDIA MONITORING AND ALERTING:\n   ```bash\n   # Social media listening tools\n   # Brandwatch, Hootsuite, Sprout Social\n   \n   # Custom monitoring script\n   #!/bin/bash\n   COMPANY=\"target.com\"\n   KEYWORDS=\"password breach hack security vulnerability\"\n   \n   # Twitter monitoring\n   twint -s \"$COMPANY AND ($KEYWORDS)\" --since \"$(date -d '7 days ago' +%Y-%m-%d)\" -o alerts.txt\n   \n   # Reddit monitoring\n   curl -s \"https://www.reddit.com/search.json?q=$COMPANY+$KEYWORDS&sort=new&limit=50\" | jq '.data.children[].data.title' >> alerts.txt\n   \n   # GitHub security issues\n   curl -s \"https://api.github.com/search/issues?q=repo:targetcompany/*+is:issue+label:security\" | jq '.items[].title' >> alerts.txt\n   \n   # Send alerts if findings exist\n   if [ -s alerts.txt ]; then\n       echo \"Security alerts found for $COMPANY\" | mail -s \"Security Alert\" security@yourcompany.com\n   fi\n   ```\n\n8. EMPLOYEE SOCIAL ENGINEERING PROFILE BUILDING:\n   ```bash\n   # Comprehensive employee profiling\n   for employee in $(cat employee_list.txt); do\n       echo \"=== Profiling: $employee ===\"\n       \n       # LinkedIn data\n       # Title, experience, education, connections\n       \n       # Twitter analysis\n       twint -s \"from:$employee\" --stats | head -20\n       \n       # GitHub contributions\n       curl -s \"https://api.github.com/users/$employee\" | jq '.public_repos, .followers, .following'\n       \n       # Personal interests (from social media)\n       # Hobbies, political views, personal information\n       \n       # Social engineering vectors\n       # Family information, pets, locations, interests\n   done > employee_profiles.txt\n   ```\n\n9. ORGANIZATIONAL INTELLIGENCE GATHERING:\n   ```bash\n   # Company structure analysis\n   # Executive team, departments, locations\n   \n   # Technology stack mentions\n   grep -rEi \"(aws|azure|gcp|kubernetes|docker|react|angular|vue)\" social_media_data/\n   \n   # Security awareness assessment\n   # Password policies, MFA usage, security training mentions\n   \n   # Recent changes\n   # Layoffs, new hires, office moves, product launches\n   \n   # Partner and vendor relationships\n   # Integration announcements, partnership posts\n   ```\n\nWHAT TO LOOK FOR:\n- **Executive Information**: CEO, CTO, CISO names and contact details\n- **IT/Dev Team**: Technical staff for targeted technical attacks\n- **Technology Stack**: AWS, Azure, frameworks, tools in use\n- **Security Awareness**: Password policies, security training mentions\n- **Personal Information**: Family, hobbies, locations for pretexting\n- **Recent Changes**: Layoffs, new hires indicating instability\n- **Code Leaks**: Exposed credentials, API keys in repositories\n- **Forum Complaints**: Employee dissatisfaction, system issues\n- **Social Engineering Vectors**: Pet names, birthdays, personal details\n- **Community Involvement**: Open source contributions, conference attendance\n- **Professional Networks**: Industry connections and partnerships\n- **Security Incidents**: Breach mentions, security discussions\n- **Internal Tools**: Custom software, internal system names\n- **Office Culture**: Remote work policies, security measures\n\nSECURITY IMPLICATIONS:\n- **Targeted Phishing**: Personal information enables spear-phishing\n- **Pretexting Attacks**: Personal details for social engineering\n- **Credential Stuffing**: Company email patterns for password attacks\n- **Supply Chain Attacks**: Partner information for indirect compromise\n- **Insider Threats**: Disgruntled employee identification\n- **Technology Targeting**: Known stack for exploit research\n- **Physical Security**: Office locations, access patterns\n- **Business Email Compromise**: Executive impersonation\n- **Code Review**: Exposed source code with vulnerabilities\n- **Secret Disclosure**: API keys, tokens in public repositories\n- **Privacy Violations**: Personal data exposure risks\n- **Regulatory Compliance**: Data protection law violations\n\nCOMMON PITFALLS:\n- **Outdated Information**: Social media data becomes stale quickly\n- **Privacy Settings**: Limited profile visibility restricts data collection\n- **Terms of Service**: Scraping may violate platform policies\n- **Legal Restrictions**: Privacy laws limit data usage and storage\n- **False Information**: Social media content may be inaccurate\n- **Cultural Differences**: International privacy norms vary\n- **Rate Limiting**: API restrictions limit automated collection\n- **Account Suspension**: Aggressive scraping triggers bans\n- **Data Volume**: Overwhelming amount of social media data\n- **Context Missing**: Posts lack full context or may be sarcastic\n- **Verification Challenges**: Difficulty confirming information accuracy\n\nDOCUMENTATION REQUIREMENTS:\n- Employee directory with roles and contact information\n- Technology stack and tool mentions compilation\n- Social engineering profile summaries\n- Security awareness assessment findings\n- Code repository analysis results\n- Social media monitoring alerts and trends\n- Organizational structure mapping\n- Recommendations for social engineering defenses\n- Evidence of exposed sensitive information\n- Timeline of significant findings and changes\n\nTOOLS REFERENCE:\n- **CrossLinked**: https://github.com/m8r0wn/CrossLinked (LinkedIn intelligence)\n- **Twint**: https://github.com/twintproject/twint (Twitter scraping)\n- **GitDorker**: https://github.com/obheda12/GitDorker (GitHub dorking)\n- **linkedin2username**: https://github.com/initstring/linkedin2username (LinkedIn enumeration)\n- **Hunter.io**: https://hunter.io/ (Email discovery API)\n- **Social Mapper**: https://github.com/Greenwolf/social_mapper (Social media correlation)\n- **OSINT Framework**: https://osintframework.com/ (OSINT resource collection)\n- **Maltego**: https://www.maltego.com/ (Link analysis and visualization)\n- **SpiderFoot**: https://www.spiderfoot.net/ (Automated OSINT)\n- **Recon-ng**: https://github.com/lanmaster53/recon-ng (Web reconnaissance framework)\n\nFURTHER READING:\n- \"Social Engineering: The Art of Human Hacking\" - Christopher Hadnagy\n- \"The Art of Deception\" - Kevin Mitnick\n- \"Ghost in the Wires\" - Kevin Mitnick\n- OWASP Social Engineering Cheat Sheet\n- NIST SP 800-115: Technical Guide to Information Security Testing\n- MITRE ATT&CK: T1593 Search Open Websites/Domains\n- \"Social Media Intelligence\" - Security Intelligence Reports\n- \"OSINT Techniques for Penetration Testing\" - Black Hat Briefings\n- \"Advanced Social Engineering\" - DEF CON Presentations\n- \"Privacy and Data Protection Laws\" - GDPR, CCPA Guidelines",
      "tags": ["recon", "social-media", "osint", "social-engineering", "personnel", "linkedin", "twitter", "github", "forums"]
    }
  ]
}