{
  "id": "genai-driven-reconnaissance",
  "title": "GenAI-Driven Reconnaissance & OSINT",
  "description": "Master AI-powered reconnaissance techniques including automated OSINT, AI-generated Google dorks, intelligent subdomain discovery, and LLM-assisted target profiling using modern tools and methodologies.",
  "type": "tutorial",
  "steps": [
    {
      "id": "ai-recon-methodology",
      "title": "AI-Enhanced Reconnaissance Methodology",
      "content": "OBJECTIVE: Understand how AI transforms traditional reconnaissance by automating OSINT collection, pattern recognition, and intelligence synthesis.\n\nTRADITIONAL VS AI-ENHANCED RECON:\n\n| Phase | Traditional | AI-Enhanced |\n|-------|------------|-------------|\n| Target identification | Manual search | AI-assisted profiling |\n| Subdomain discovery | Tool enumeration | ML-based prediction |\n| Google dorking | Manual crafting | AI-generated queries |\n| OSINT analysis | Time-intensive | Automated synthesis |\n| Report generation | Manual compilation | AI summarization |\n\nAI RECON WORKFLOW:\n```\n1. Target Definition\n   ├── AI-assisted scope identification\n   └── Attack surface brainstorming\n   \n2. Passive Reconnaissance  \n   ├── AI-generated Google dorks\n   ├── Automated OSINT collection\n   ├── Social media analysis\n   └── Technology fingerprinting\n   \n3. Semi-Passive Reconnaissance\n   ├── Subdomain enumeration with AI prediction\n   ├── DNS intelligence gathering\n   └── Certificate transparency analysis\n   \n4. Analysis & Synthesis\n   ├── AI-powered data correlation\n   ├── Pattern recognition\n   └── Intelligence report generation\n```\n\nKEY AI CAPABILITIES FOR RECON:\n\n1. **Natural Language Understanding**\n   - Parse unstructured data sources\n   - Extract entities (names, emails, IPs)\n   - Understand context and relationships\n\n2. **Pattern Recognition**\n   - Identify naming conventions (subdomains)\n   - Detect technology stacks from signatures\n   - Recognize organizational patterns\n\n3. **Content Generation**\n   - Create targeted search queries\n   - Generate phishing reconnaissance profiles\n   - Produce comprehensive reports\n\n4. **Data Synthesis**\n   - Correlate findings across sources\n   - Identify attack surface priorities\n   - Generate actionable intelligence\n\nETHICAL CONSIDERATIONS:\n- Only perform recon on authorized targets\n- Respect robots.txt and rate limits\n- Don't social engineer employees\n- Document all sources for legal compliance",
      "tags": ["methodology", "osint", "reconnaissance", "workflow"],
      "related_tools": ["nmap", "theHarvester"]
    },
    {
      "id": "ai-google-dorking",
      "title": "AI-Generated Google Dorks & Advanced Search",
      "content": "OBJECTIVE: Use LLMs to generate sophisticated Google dorks and advanced search queries for discovering exposed assets, sensitive files, and misconfigurations.\n\nBASIC AI DORK GENERATION:\n\n```bash\n# Using Shell GPT to generate dorks\nsgpt \"Generate 15 Google dorks to find exposed .env files containing API keys for company: ACME Corp\"\n\nsgpt \"Create Google dorks to discover exposed Swagger/OpenAPI documentation\"\n\nsgpt \"Generate search queries to find S3 buckets belonging to example.com domain\"\n```\n\nADVANCED DORK TEMPLATES:\n\n**Exposed Configuration Files:**\n```\nsite:target.com filetype:env\nsite:target.com filetype:yml password OR api_key\nsite:target.com inurl:config filetype:json\nsite:target.com filetype:xml inurl:config\n```\n\n**AI Prompt for Comprehensive Dorks:**\n```\nPrompt: Generate Google dorks for target.com to find:\n1. Exposed configuration files (.env, .yml, .json, .xml)\n2. Directory listings\n3. Backup files (.bak, .old, .backup)\n4. Database dumps (.sql, .db)\n5. Exposed admin panels\n6. API documentation\n7. Error messages revealing stack traces\n8. Exposed credentials in GitHub/GitLab\n```\n\n**AI-Enhanced GitHub Dorking:**\n```bash\nsgpt \"Create GitHub search queries to find leaked credentials for organization: AcmeCorp\"\n\n# Example outputs:\n# org:AcmeCorp password filename:.env\n# org:AcmeCorp api_key filename:config\n# org:AcmeCorp secret filename:.yml\n# \"acmecorp.com\" password\n```\n\nAUTOMATED DORK EXECUTION:\n\n```python\n# dork_executor.py\nimport requests\nimport time\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ndef generate_dorks(target_domain, focus_area):\n    \"\"\"Generate targeted Google dorks using AI.\"\"\"\n    prompt = f\"\"\"\n    Generate 10 specific Google dorks for {target_domain} focused on {focus_area}.\n    Return only the dork queries, one per line.\n    Include site: operator and relevant file types or inurl patterns.\n    \"\"\"\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.7\n    )\n    \n    dorks = response.choices[0].message.content.strip().split('\\n')\n    return [d.strip() for d in dorks if d.strip()]\n\n# Usage\ntarget = \"example.com\"\nfor area in [\"exposed configs\", \"admin panels\", \"backup files\"]:\n    print(f\"\\n=== Dorks for {area} ===\")\n    dorks = generate_dorks(target, area)\n    for dork in dorks:\n        print(f\"  {dork}\")\n```\n\nSHODAN/CENSYS QUERY GENERATION:\n```bash\n# Generate Shodan queries\nsgpt \"Create 10 Shodan queries to find exposed services for organization: ACME Corp with domain acme.com\"\n\n# Example outputs:\nssl.cert.subject.CN:\"*.acme.com\"\norg:\"ACME Corp\" port:22,3389,5900\nhttp.title:\"ACME\" port:80,443,8080,8443\nssl:\"acme.com\" has_vuln:true\n```\n\nBEST PRACTICES:\n- Validate dorks manually before bulk execution\n- Use rate limiting to avoid blocks\n- Document all queries for reproducibility\n- Cross-reference findings with other tools",
      "tags": ["google-dorks", "search", "osint", "ai-generation"],
      "related_tools": ["theHarvester", "recon-ng"]
    },
    {
      "id": "ai-subdomain-discovery",
      "title": "AI-Powered Subdomain Enumeration",
      "content": "OBJECTIVE: Combine traditional subdomain enumeration tools with AI-predicted subdomains based on naming patterns and organizational intelligence.\n\nTRADITIONAL TOOLS BASELINE:\n\n```bash\n# Subfinder - Fast passive subdomain discovery\nsubfinder -d target.com -o subdomains.txt\n\n# Amass - Comprehensive enumeration\namass enum -passive -d target.com -o amass_subs.txt\n\n# Assetfinder\nassetfinder --subs-only target.com > assetfinder_subs.txt\n\n# Certificate Transparency\ncurl -s \"https://crt.sh/?q=%.target.com&output=json\" | \\\n  jq -r '.[].name_value' | sort -u > crt_subs.txt\n\n# Combine results\ncat *_subs.txt subdomains.txt crt_subs.txt | sort -u > all_subs.txt\n```\n\nAI-PREDICTED SUBDOMAIN GENERATION:\n\n```bash\n# Analyze discovered subdomains and predict more\nsgpt \"Given these discovered subdomains for acme.com:\\napi.acme.com\\ndev.acme.com\\nstaging.acme.com\\napp.acme.com\\n\\nPredict 30 additional likely subdomains based on common patterns:\"\n```\n\nPYTHON SCRIPT FOR AI SUBDOMAIN PREDICTION:\n\n```python\n# ai_subdomain_predictor.py\nimport openai\nimport subprocess\n\ndef get_discovered_subdomains(domain):\n    \"\"\"Run subfinder to get initial subdomains.\"\"\"\n    result = subprocess.run(\n        ['subfinder', '-d', domain, '-silent'],\n        capture_output=True, text=True\n    )\n    return result.stdout.strip().split('\\n')\n\ndef predict_subdomains(domain, discovered):\n    \"\"\"Use AI to predict additional subdomains.\"\"\"\n    \n    prompt = f\"\"\"Analyze these discovered subdomains for {domain}:\n{chr(10).join(discovered[:20])}\n\nBased on these patterns and common naming conventions, predict 50 additional \nsubdomains that likely exist. Consider:\n1. Development environments (dev, staging, test, qa, uat)\n2. Regional variations (us, eu, asia, uk)\n3. Service-specific (api, mail, vpn, ftp, sftp)\n4. Internal tools (jenkins, gitlab, jira, confluence)\n5. Versioning patterns (v1, v2, api-v2)\n\nReturn only subdomain names (without the main domain), one per line.\"\"\"\n    \n    response = openai.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    predictions = response.choices[0].message.content.strip().split('\\n')\n    return [f\"{p.strip()}.{domain}\" for p in predictions if p.strip()]\n\ndef verify_subdomains(subdomains):\n    \"\"\"Check which predicted subdomains actually resolve.\"\"\"\n    valid = []\n    for sub in subdomains:\n        result = subprocess.run(\n            ['dig', '+short', sub],\n            capture_output=True, text=True\n        )\n        if result.stdout.strip():\n            valid.append(sub)\n            print(f\"[+] Found: {sub}\")\n    return valid\n\n# Usage\ndomain = \"target.com\"\ndiscovered = get_discovered_subdomains(domain)\nprint(f\"[*] Found {len(discovered)} subdomains via subfinder\")\n\npredicted = predict_subdomains(domain, discovered)\nprint(f\"[*] AI predicted {len(predicted)} additional subdomains\")\n\nprint(\"\\n[*] Verifying predicted subdomains...\")\nvalid = verify_subdomains(predicted)\nprint(f\"\\n[+] Verified {len(valid)} new subdomains!\")\n```\n\nDNS BRUTEFORCE WITH AI WORDLIST:\n\n```bash\n# Generate custom wordlist\nsgpt \"Generate 200 subdomain prefixes for a tech company. Include:\\n- Dev/staging environments\\n- API versions\\n- Internal tools\\n- Geographic regions\\nOutput one word per line.\" > ai_wordlist.txt\n\n# Use with dns bruteforce\nffuf -u \"http://FUZZ.target.com\" -w ai_wordlist.txt \\\n  -mc 200,301,302,403 -o ffuf_results.json\n\n# Or with gobuster\ngobuster dns -d target.com -w ai_wordlist.txt -o gobuster_dns.txt\n```\n\nCOMBINED WORKFLOW:\n```bash\n#!/bin/bash\n# comprehensive_subdomain.sh\nDOMAIN=$1\n\necho \"[1/4] Running passive enumeration...\"\nsubfinder -d $DOMAIN -silent > passive.txt\n\necho \"[2/4] Checking certificate transparency...\"\ncurl -s \"https://crt.sh/?q=%.$DOMAIN&output=json\" | \\\n  jq -r '.[].name_value' 2>/dev/null | sort -u >> passive.txt\n\necho \"[3/4] AI predicting additional subdomains...\"\nsgpt \"Predict 50 subdomains for $DOMAIN based on tech company patterns\" > ai_predicted.txt\n\necho \"[4/4] Verifying all subdomains...\"\ncat passive.txt ai_predicted.txt | sort -u | while read sub; do\n  if dig +short $sub | grep -q '.'; then\n    echo \"$sub\" >> verified.txt\n  fi\ndone\n\necho \"Found $(wc -l < verified.txt) verified subdomains\"\n```",
      "tags": ["subdomain", "enumeration", "dns", "ai-prediction"],
      "related_tools": ["subfinder", "amass", "gobuster"]
    },
    {
      "id": "ai-osint-automation",
      "title": "Automated OSINT Collection with AI Analysis",
      "content": "OBJECTIVE: Automate OSINT collection from multiple sources and use AI to analyze, correlate, and synthesize intelligence.\n\nOSINT DATA SOURCES:\n\n| Category | Sources | AI Application |\n|----------|---------|----------------|\n| Corporate | LinkedIn, Company websites | Employee profiling |\n| Technical | GitHub, StackOverflow | Technology stack ID |\n| DNS/IP | WHOIS, DNS records | Infrastructure mapping |\n| Social | Twitter, Forums | Sentiment analysis |\n| Documents | SEC filings, PDFs | Entity extraction |\n| Leaks | Breach databases | Credential intelligence |\n\nTHEHARVESTER WITH AI ANALYSIS:\n\n```bash\n# Run theHarvester\ntheHarvester -d target.com -l 500 -b all -f harvester_results\n\n# Analyze results with AI\nsgpt \"Analyze this theHarvester output and identify:\\n1. Key personnel and their roles\\n2. Email patterns\\n3. Technology indicators\\n4. Potential attack vectors\\n\\n$(cat harvester_results.json | head -200)\"\n```\n\nCOMPREHENSIVE OSINT SCRIPT:\n\n```python\n# ai_osint_collector.py\nimport subprocess\nimport json\nimport requests\nfrom openai import OpenAI\nfrom dataclasses import dataclass\nfrom typing import List, Dict\n\nclient = OpenAI()\n\n@dataclass\nclass OSINTResult:\n    source: str\n    data_type: str\n    findings: List[str]\n\nclass AIAssistedOSINT:\n    def __init__(self, target_domain: str):\n        self.target = target_domain\n        self.results: List[OSINTResult] = []\n    \n    def run_theharvester(self) -> Dict:\n        \"\"\"Run theHarvester for email and subdomain collection.\"\"\"\n        print(f\"[*] Running theHarvester on {self.target}\")\n        result = subprocess.run(\n            ['theHarvester', '-d', self.target, '-l', '100', '-b', 'bing,duckduckgo'],\n            capture_output=True, text=True\n        )\n        return {'raw_output': result.stdout}\n    \n    def query_crtsh(self) -> List[str]:\n        \"\"\"Get certificate transparency data.\"\"\"\n        print(f\"[*] Querying crt.sh for {self.target}\")\n        resp = requests.get(\n            f\"https://crt.sh/?q=%.{self.target}&output=json\",\n            timeout=30\n        )\n        if resp.status_code == 200:\n            certs = resp.json()\n            return list(set(c['name_value'] for c in certs))\n        return []\n    \n    def query_whois(self) -> str:\n        \"\"\"Get WHOIS information.\"\"\"\n        result = subprocess.run(\n            ['whois', self.target],\n            capture_output=True, text=True\n        )\n        return result.stdout\n    \n    def ai_analyze(self, data: Dict) -> str:\n        \"\"\"Use AI to analyze and correlate OSINT findings.\"\"\"\n        \n        prompt = f\"\"\"Analyze this OSINT data for {self.target} and provide:\n\n1. EXECUTIVE SUMMARY (2-3 sentences)\n2. ATTACK SURFACE FINDINGS:\n   - Subdomains discovered\n   - Email addresses and patterns\n   - Technology stack indicators\n3. KEY PERSONNEL identified\n4. POTENTIAL VULNERABILITIES:\n   - Exposed services\n   - Misconfigurations\n   - Information disclosure\n5. RECOMMENDED NEXT STEPS for penetration testing\n\nOSINT Data:\n{json.dumps(data, indent=2)[:4000]}\n\"\"\"\n        \n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        return response.choices[0].message.content\n    \n    def run_full_recon(self) -> str:\n        \"\"\"Execute complete OSINT workflow with AI analysis.\"\"\"\n        \n        # Collect data from multiple sources\n        harvester_data = self.run_theharvester()\n        cert_domains = self.query_crtsh()\n        whois_data = self.query_whois()\n        \n        # Combine all findings\n        combined = {\n            'target': self.target,\n            'theharvester': harvester_data,\n            'certificate_transparency': cert_domains[:50],\n            'whois': whois_data[:1000]\n        }\n        \n        # AI analysis and correlation\n        analysis = self.ai_analyze(combined)\n        \n        return analysis\n\n# Usage\nosint = AIAssistedOSINT('target.com')\nreport = osint.run_full_recon()\nprint(report)\n\n# Save report\nwith open('osint_report.md', 'w') as f:\n    f.write(report)\n```\n\nSOCIAL MEDIA OSINT:\n```bash\n# Generate LinkedIn search queries\nsgpt \"Create 10 LinkedIn search queries to find employees at ACME Corp who work in:\\n- IT Security\\n- DevOps\\n- System Administration\"\n\n# Analyze public social profiles\nsgpt \"Given this Twitter bio: 'Senior DevOps Engineer @ACMECorp | K8s enthusiast | AWS certified'\\nWhat technology attack vectors might be relevant?\"\n```\n\nDOCUMENT ANALYSIS:\n```bash\n# Extract metadata from PDFs\nexiftool document.pdf > metadata.txt\n\n# AI analysis of document metadata\nsgpt \"Analyze this PDF metadata and identify:\\n1. Author information\\n2. Software used\\n3. Internal paths revealed\\n4. Potential intelligence value\\n\\n$(cat metadata.txt)\"\n```",
      "tags": ["osint", "automation", "analysis", "theharvester"],
      "related_tools": ["theHarvester", "recon-ng", "spiderfoot"]
    },
    {
      "id": "technology-fingerprinting",
      "title": "AI-Enhanced Technology Stack Detection",
      "content": "OBJECTIVE: Use AI to analyze web responses, headers, and patterns to accurately identify technology stacks, frameworks, and potential vulnerabilities.\n\nTRADITIONAL FINGERPRINTING:\n\n```bash\n# Wappalyzer CLI\nwappalyzer https://target.com -P\n\n# WhatWeb\nwhatweb -a 3 https://target.com\n\n# Nuclei technology detection\nnuclei -u https://target.com -t technologies/\n\n# HTTP headers analysis\ncurl -I https://target.com\n```\n\nAI-ENHANCED ANALYSIS:\n\n```python\n# ai_tech_fingerprint.py\nimport requests\nimport re\nfrom openai import OpenAI\nimport subprocess\n\nclient = OpenAI()\n\ndef collect_fingerprints(url: str) -> dict:\n    \"\"\"Collect data for technology fingerprinting.\"\"\"\n    \n    data = {'url': url}\n    \n    # HTTP Headers\n    resp = requests.get(url, allow_redirects=True)\n    data['headers'] = dict(resp.headers)\n    data['status_code'] = resp.status_code\n    \n    # HTML analysis\n    html = resp.text\n    data['html_sample'] = html[:5000]\n    \n    # Extract JavaScript files\n    js_files = re.findall(r'src=[\"\\']([^\"\\'>]+\\.js)[\"\\']', html)\n    data['js_files'] = js_files[:20]\n    \n    # Extract CSS files\n    css_files = re.findall(r'href=[\"\\']([^\"\\'>]+\\.css)[\"\\']', html)\n    data['css_files'] = css_files[:10]\n    \n    # Meta tags\n    meta_tags = re.findall(r'<meta[^>]+>', html)\n    data['meta_tags'] = meta_tags[:15]\n    \n    # Cookies\n    data['cookies'] = [f\"{k}={v}\" for k, v in resp.cookies.items()]\n    \n    return data\n\ndef ai_fingerprint_analysis(data: dict) -> str:\n    \"\"\"Use AI to analyze fingerprints and identify technologies.\"\"\"\n    \n    prompt = f\"\"\"Analyze this web fingerprint data and identify the complete technology stack:\n\nURL: {data['url']}\n\nHTTP Headers:\n{json.dumps(data['headers'], indent=2)}\n\nJavaScript Files:\n{chr(10).join(data['js_files'])}\n\nCSS Files:\n{chr(10).join(data['css_files'])}\n\nMeta Tags:\n{chr(10).join(data['meta_tags'])}\n\nCookies:\n{chr(10).join(data['cookies'])}\n\nHTML Sample (first 2000 chars):\n{data['html_sample'][:2000]}\n\nProvide:\n1. IDENTIFIED TECHNOLOGIES (with confidence level):\n   - Web Server\n   - Programming Language/Framework\n   - CMS/Platform\n   - JavaScript Libraries\n   - CSS Frameworks\n   - CDN/Hosting\n   - Analytics/Tracking\n   - Security Tools\n\n2. VERSION INFORMATION (if detectable)\n\n3. POTENTIAL VULNERABILITIES:\n   - Known CVEs for identified versions\n   - Misconfigurations detected\n   - Information disclosure\n\n4. RECOMMENDED SCANS:\n   - Specific Nuclei templates\n   - Targeted exploits to try\n\"\"\"\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return response.choices[0].message.content\n\n# Usage\nimport json\n\nurl = \"https://target.com\"\nfingerprints = collect_fingerprints(url)\nanalysis = ai_fingerprint_analysis(fingerprints)\nprint(analysis)\n```\n\nSHELL-BASED WORKFLOW:\n\n```bash\n#!/bin/bash\n# ai_tech_stack.sh\nTARGET=$1\n\n# Collect data\necho \"[*] Collecting HTTP headers...\"\ncurl -sI \"$TARGET\" > headers.txt\n\necho \"[*] Running WhatWeb...\"\nwhatweb -a 3 \"$TARGET\" 2>/dev/null > whatweb.txt\n\necho \"[*] Extracting JavaScript files...\"\ncurl -s \"$TARGET\" | grep -oP 'src=[\"\\']\\K[^\"\\'>]+\\.js' > js_files.txt\n\necho \"[*] Running AI analysis...\"\nsgpt \"Analyze this technology fingerprint:\\n\\nHeaders:\\n$(cat headers.txt)\\n\\nWhatWeb:\\n$(cat whatweb.txt)\\n\\nJS Files:\\n$(cat js_files.txt)\\n\\nIdentify technologies, versions, and potential vulnerabilities.\"\n```\n\nAPI ENDPOINT DISCOVERY:\n\n```bash\n# Use AI to predict API endpoints based on technology\nsgpt \"Given this is a React/Node.js application, predict 20 likely API endpoints including:\\n- Authentication\\n- User management\\n- Data retrieval\\n- Admin functions\"\n\n# Verify predicted endpoints\nfor endpoint in $(cat predicted_endpoints.txt); do\n  status=$(curl -s -o /dev/null -w \"%{http_code}\" \"$TARGET$endpoint\")\n  echo \"$endpoint: $status\"\ndone\n```",
      "tags": ["fingerprinting", "technology-stack", "enumeration", "analysis"],
      "related_tools": ["wappalyzer", "whatweb", "nuclei"]
    },
    {
      "id": "recon-report-generation",
      "title": "AI-Generated Reconnaissance Reports",
      "content": "OBJECTIVE: Automatically compile reconnaissance findings into professional, actionable reports using AI synthesis and analysis.\n\nREPORT STRUCTURE:\n```markdown\n# Reconnaissance Report: [Target]\n\n## Executive Summary\n- Scope overview\n- Key findings summary\n- Risk assessment\n\n## Target Profile\n- Organization details\n- Domain information\n- Network ranges\n\n## Attack Surface\n- Subdomains discovered\n- Open ports/services\n- Web technologies\n- Exposed applications\n\n## OSINT Findings\n- Employee information\n- Email patterns\n- Social media presence\n- Document leakage\n\n## Vulnerabilities Identified\n- Critical findings\n- High-risk exposures\n- Misconfigurations\n\n## Recommendations\n- Priority actions\n- Further testing needed\n- Remediation suggestions\n\n## Appendices\n- Raw data\n- Tool outputs\n- Methodology notes\n```\n\nAI REPORT GENERATOR:\n\n```python\n# ai_recon_report.py\nimport json\nfrom datetime import datetime\nfrom openai import OpenAI\nfrom pathlib import Path\n\nclient = OpenAI()\n\nclass ReconReportGenerator:\n    def __init__(self, target: str, output_dir: str = \"./reports\"):\n        self.target = target\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True)\n        self.findings = {}\n    \n    def add_findings(self, category: str, data: any):\n        \"\"\"Add findings to the report.\"\"\"\n        self.findings[category] = data\n    \n    def generate_report(self) -> str:\n        \"\"\"Generate comprehensive report using AI.\"\"\"\n        \n        prompt = f\"\"\"Generate a professional penetration testing reconnaissance report for: {self.target}\n\nDate: {datetime.now().strftime('%Y-%m-%d')}\n\nCollected Data:\n{json.dumps(self.findings, indent=2, default=str)[:8000]}\n\nCreate a detailed report with:\n\n1. EXECUTIVE SUMMARY (3-4 paragraphs)\n   - Engagement overview\n   - Critical findings\n   - Overall risk assessment\n\n2. TARGET PROFILE\n   - Organization information\n   - Technical infrastructure\n   - Online presence\n\n3. ATTACK SURFACE ANALYSIS\n   - Subdomains and their purposes\n   - Exposed services and ports\n   - Web applications identified\n   - Technology stack summary\n\n4. OSINT INTELLIGENCE\n   - Employee information discovered\n   - Email patterns and addresses\n   - Social media findings\n   - Document/data leakage\n\n5. VULNERABILITY ASSESSMENT\n   - Critical (immediate action required)\n   - High (address within 7 days)\n   - Medium (address within 30 days)\n   - Low (address at convenience)\n   - Informational\n\n6. ATTACK VECTORS\n   - Most promising entry points\n   - Social engineering opportunities\n   - Technical exploitation paths\n\n7. RECOMMENDATIONS\n   - Prioritized remediation steps\n   - Security improvements\n   - Further testing needed\n\nFormat as clean Markdown with proper headings and tables.\"\"\"\n        \n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=4000\n        )\n        \n        return response.choices[0].message.content\n    \n    def save_report(self, format: str = \"markdown\"):\n        \"\"\"Save report to file.\"\"\"\n        report = self.generate_report()\n        \n        filename = f\"{self.target.replace('.', '_')}_recon_{datetime.now().strftime('%Y%m%d')}\"\n        \n        if format == \"markdown\":\n            path = self.output_dir / f\"{filename}.md\"\n            path.write_text(report)\n        \n        print(f\"[+] Report saved to: {path}\")\n        return path\n\n# Usage example\nreport = ReconReportGenerator(\"target.com\")\n\n# Add findings from various tools\nreport.add_findings(\"subdomains\", [\"api.target.com\", \"dev.target.com\", \"staging.target.com\"])\nreport.add_findings(\"emails\", [\"admin@target.com\", \"support@target.com\"])\nreport.add_findings(\"technologies\", {\"server\": \"nginx/1.18\", \"framework\": \"React\", \"backend\": \"Node.js\"})\nreport.add_findings(\"open_ports\", {\"22\": \"SSH\", \"80\": \"HTTP\", \"443\": \"HTTPS\", \"8080\": \"Tomcat\"})\n\n# Generate and save\nreport.save_report()\n```\n\nQUICK SHELL REPORT:\n\n```bash\n#!/bin/bash\n# quick_recon_report.sh\nTARGET=$1\n\n# Collect all findings\necho \"Generating reconnaissance report for $TARGET...\"\n\n# Combine all recon outputs\ncat << EOF | sgpt \"Generate a detailed penetration testing reconnaissance report from this data. Format as Markdown.\"\n\nTARGET: $TARGET\nDATE: $(date)\n\n=== SUBDOMAINS ===\n$(cat subdomains.txt 2>/dev/null || echo \"No subdomain data\")\n\n=== NMAP SCAN ===\n$(cat nmap_scan.txt 2>/dev/null || echo \"No Nmap data\")\n\n=== TECHNOLOGY STACK ===\n$(cat tech_stack.txt 2>/dev/null || echo \"No tech data\")\n\n=== OSINT FINDINGS ===\n$(cat osint_results.txt 2>/dev/null || echo \"No OSINT data\")\n\nEOF\n```",
      "tags": ["reporting", "documentation", "synthesis", "automation"],
      "related_tools": ["nmap", "theHarvester"]
    }
  ]
}