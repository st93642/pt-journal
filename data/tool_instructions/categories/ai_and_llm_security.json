[
  {
    "id": "garak",
    "name": "Garak",
    "summary": "Garak is an LLM vulnerability scanner that probes for prompt injection, jailbreaks, data exfiltration, and other AI security issues. It provides systematic testing of AI model safety and robustness.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip with Python 3.10+",
        "steps": [
          {
            "detail": "sudo apt update",
            "copyable": true
          },
          {
            "detail": "sudo apt install -y python3-pip python3-venv",
            "copyable": true
          },
          {
            "detail": "python3 -m pip install --user garak",
            "copyable": true
          },
          {
            "detail": "garak --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Pre-installed or install via pip",
        "steps": [
          {
            "detail": "sudo apt install -y python3-pip",
            "copyable": true
          },
          {
            "detail": "python3 -m pip install --user garak",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Official containerized deployment",
        "steps": [
          {
            "detail": "docker pull ghcr.io/leondz/garak:latest",
            "copyable": true
          },
          {
            "detail": "docker run --rm ghcr.io/leondz/garak:latest --help",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Basic prompt injection test",
        "command": "garak --model_type huggingface --model_name microsoft/DialoGPT-medium --probes promptinject",
        "notes": []
      },
      {
        "description": "Test multiple generators",
        "command": "garak --generators openai gpt-3.5-turbo,huggingface microsoft/DialoGPT-medium --probes all",
        "notes": []
      },
      {
        "description": "Custom probe execution",
        "command": "garak --model_type openai --model_name gpt-4 --probes dan_11.0",
        "notes": []
      },
      {
        "description": "Generate HTML report",
        "command": "garak --model_type openai --model_name gpt-3.5-turbo --probes promptinject --report_formats html",
        "notes": []
      }
    ],
    "common_flags": [
      {
        "flag": "--model_type",
        "description": "Type of model to test (openai, huggingface, etc.)"
      },
      {
        "flag": "--model_name",
        "description": "Specific model identifier"
      },
      {
        "flag": "--probes",
        "description": "Security probes to run (promptinject, dan_11.0, etc.)"
      },
      {
        "flag": "--generators",
        "description": "Model generators to test"
      },
      {
        "flag": "--report_formats",
        "description": "Output formats (html, json)"
      }
    ],
    "operational_tips": [
      "Configure API keys for cloud models (OpenAI, Anthropic) in environment variables",
      "Use --parallel option for faster scanning of multiple models",
      "Start with basic probes before running comprehensive test suites",
      "Review HTML reports for detailed vulnerability analysis"
    ],
    "step_sequences": [
      {
        "title": "AI Model Security Assessment",
        "steps": [
          {
            "title": "Configure API access",
            "details": "Set up API keys for target models",
            "command": "export OPENAI_API_KEY=your_key_here"
          },
          {
            "title": "Run prompt injection tests",
            "details": "Test for prompt injection vulnerabilities",
            "command": "garak --model_type openai --model_name gpt-4 --probes promptinject"
          },
          {
            "title": "Execute jailbreak probes",
            "details": "Test resistance to jailbreaking attempts",
            "command": "garak --model_type openai --model_name gpt-4 --probes dan_11.0"
          },
          {
            "title": "Generate security report",
            "details": "Create comprehensive assessment report",
            "command": "garak --model_type openai --model_name gpt-4 --probes all --report_formats html"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "Comprehensive AI Security Testing Pipeline",
        "stages": [
          {
            "label": "Model enumeration",
            "description": "Identify all AI models and APIs in scope",
            "command": "# Manual discovery of AI endpoints"
          },
          {
            "label": "Garak baseline scan",
            "description": "Run standard security probes",
            "command": "garak --model_type openai --model_name gpt-4 --probes promptinject,dan_11.0,encoding"
          },
          {
            "label": "Custom probe development",
            "description": "Create organization-specific test cases",
            "command": "# Develop custom garak probes"
          },
          {
            "label": "Report generation",
            "description": "Compile findings and recommendations",
            "command": "garak --report_formats html,json"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "PASS: probe_name",
        "meaning": "Model successfully resisted the security probe",
        "severity": "info"
      },
      {
        "indicator": "FAIL: probe_name",
        "meaning": "Model vulnerable to the tested attack vector",
        "severity": "warning"
      },
      {
        "indicator": "SCORE: 0.85",
        "meaning": "Vulnerability score (higher = more vulnerable)",
        "severity": "info"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom probe development",
        "command": "garak --model_type openai --model_name gpt-4 --probes custom_probe.py",
        "scenario": "Create and test custom security probes for specific AI applications",
        "notes": [
          "Extend garak with custom probe classes",
          "Test organization-specific attack patterns"
        ]
      }
    ],
    "comparison_table": null,
    "resources": [
      {
        "label": "Garak Documentation",
        "url": "https://github.com/leondz/garak",
        "description": "Official documentation and probe reference"
      },
      {
        "label": "AI Security Testing Guide",
        "url": "https://owasp.org/www-project-ai-security-and-privacy-guide/",
        "description": "OWASP AI security testing methodology"
      }
    ]
  },
  {
    "id": "llm-guard",
    "name": "LLM Guard",
    "summary": "LLM Guard is a comprehensive security toolkit for protecting Large Language Models against prompt injections, toxic content, and data exfiltration attacks.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip with Python dependencies",
        "steps": [
          {
            "detail": "sudo apt update",
            "copyable": true
          },
          {
            "detail": "sudo apt install -y python3-pip python3-dev",
            "copyable": true
          },
          {
            "detail": "pip3 install llm-guard",
            "copyable": true
          },
          {
            "detail": "llm-guard --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Containerized deployment with all dependencies",
        "steps": [
          {
            "detail": "docker pull quay.io/lavalliere/llm-guard",
            "copyable": true
          },
          {
            "detail": "docker run --rm quay.io/lavalliere/llm-guard --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "macOS",
        "summary": "Install via Homebrew or pip",
        "steps": [
          {
            "detail": "brew install python3",
            "copyable": true
          },
          {
            "detail": "pip3 install llm-guard",
            "copyable": true
          },
          {
            "detail": "llm-guard --help",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Basic input sanitization",
        "command": "echo \"malicious prompt\" | llm-guard sanitize",
        "notes": []
      },
      {
        "description": "Scan for prompt injections",
        "command": "llm-guard scan --input-file prompts.txt --scanner prompt_injection",
        "notes": []
      },
      {
        "description": "Test content safety",
        "command": "llm-guard scan --text \"unsafe content\" --scanner toxicity",
        "notes": []
      },
      {
        "description": "Batch processing",
        "command": "llm-guard scan --input-file batch.txt --output-file results.json",
        "notes": []
      }
    ],
    "common_flags": [
      {
        "flag": "--scanner",
        "description": "Security scanner to use (prompt_injection, toxicity, etc.)"
      },
      {
        "flag": "--input-file",
        "description": "File containing text to scan"
      },
      {
        "flag": "--output-file",
        "description": "File to save scan results"
      },
      {
        "flag": "--text",
        "description": "Direct text input for scanning"
      }
    ],
    "operational_tips": [
      "Use multiple scanners for comprehensive protection",
      "Implement LLM Guard as middleware in AI pipelines",
      "Regularly update scanner models for latest threats",
      "Monitor false positive rates and adjust thresholds"
    ],
    "step_sequences": [
      {
        "title": "AI Input Validation Pipeline",
        "steps": [
          {
            "title": "Install and configure",
            "details": "Set up LLM Guard with required models",
            "command": "pip3 install llm-guard"
          },
          {
            "title": "Test prompt injection detection",
            "details": "Validate injection detection capabilities",
            "command": "llm-guard scan --text \"ignore previous instructions\" --scanner prompt_injection"
          },
          {
            "title": "Implement in application",
            "details": "Integrate as preprocessing step",
            "command": "# Add to AI application pipeline"
          },
          {
            "title": "Monitor and tune",
            "details": "Track performance and adjust parameters",
            "command": "# Review logs and metrics"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "Defensive AI Pipeline with LLM Guard",
        "stages": [
          {
            "label": "Input validation",
            "description": "Sanitize and validate all user inputs",
            "command": "llm-guard scan --input-file user_inputs.txt --scanner prompt_injection,toxicity"
          },
          {
            "label": "Content filtering",
            "description": "Block harmful or inappropriate content",
            "command": "llm-guard sanitize --input-file filtered_inputs.txt"
          },
          {
            "label": "Response validation",
            "description": "Check AI outputs for safety",
            "command": "llm-guard scan --input-file ai_responses.txt --scanner output_safety"
          },
          {
            "label": "Logging and monitoring",
            "description": "Track security events and incidents",
            "command": "# Implement comprehensive logging"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "SAFE",
        "meaning": "Content passed all security checks",
        "severity": "info"
      },
      {
        "indicator": "UNSAFE: prompt_injection",
        "meaning": "Prompt injection detected in input",
        "severity": "warning"
      },
      {
        "indicator": "SCORE: 0.92",
        "meaning": "Confidence score for detected vulnerability",
        "severity": "info"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom scanner development",
        "command": "llm-guard scan --custom-scanner my_scanner.py --input-file test_data.txt",
        "scenario": "Develop organization-specific security scanners",
        "notes": [
          "Extend LLM Guard with custom detection logic",
          "Train models on organization-specific threat patterns"
        ]
      }
    ],
    "comparison_table": null,
    "resources": [
      {
        "label": "LLM Guard Documentation",
        "url": "https://github.com/lavalliere/llm-guard",
        "description": "Official documentation and usage examples"
      }
    ]
  },
  {
    "id": "ml-pipeline-audit",
    "name": "ML Pipeline Audit Toolkit",
    "summary": "Comprehensive toolkit for auditing Machine Learning pipelines against data poisoning, adversarial examples, and model exfiltration attacks.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip with ML dependencies",
        "steps": [
          {
            "detail": "sudo apt update",
            "copyable": true
          },
          {
            "detail": "sudo apt install -y python3-pip python3-dev",
            "copyable": true
          },
          {
            "detail": "pip3 install ml-pipeline-audit scikit-learn torch",
            "copyable": true
          },
          {
            "detail": "ml-audit --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Container with GPU support for ML workloads",
        "steps": [
          {
            "detail": "docker pull mlsec/ml-pipeline-audit",
            "copyable": true
          },
          {
            "detail": "docker run --rm mlsec/ml-pipeline-audit --help",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Conda Environment",
        "summary": "Isolated ML environment with conda",
        "steps": [
          {
            "detail": "conda create -n ml-audit python=3.9",
            "copyable": true
          },
          {
            "detail": "conda activate ml-audit",
            "copyable": true
          },
          {
            "detail": "pip install ml-pipeline-audit scikit-learn torch",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Data poisoning detection",
        "command": "ml-audit detect-poisoning --dataset training_data.csv --model classifier.pkl",
        "notes": []
      },
      {
        "description": "Generate adversarial examples",
        "command": "ml-audit generate-adversarial --model target_model.h5 --input sample.jpg",
        "notes": []
      },
      {
        "description": "Model vulnerability assessment",
        "command": "ml-audit assess-vulnerabilities --model model.pkl --test-data test.csv",
        "notes": []
      },
      {
        "description": "Membership inference test",
        "command": "ml-audit test-membership --model model.pkl --candidate-data candidates.csv",
        "notes": []
      }
    ],
    "common_flags": [
      {
        "flag": "--dataset",
        "description": "Path to training/validation dataset"
      },
      {
        "flag": "--model",
        "description": "Path to trained model file"
      },
      {
        "flag": "--test-data",
        "description": "Test data for vulnerability assessment"
      },
      {
        "flag": "--output-dir",
        "description": "Directory for saving results and reports"
      }
    ],
    "operational_tips": [
      "Use GPU acceleration for large model testing",
      "Validate results with multiple attack types",
      "Implement findings in CI/CD security gates",
      "Regular auditing as part of ML lifecycle"
    ],
    "step_sequences": [
      {
        "title": "ML Pipeline Security Audit",
        "steps": [
          {
            "title": "Data integrity check",
            "details": "Test for data poisoning and tampering",
            "command": "ml-audit detect-poisoning --dataset data.csv --model model.pkl"
          },
          {
            "title": "Adversarial robustness test",
            "details": "Generate and test adversarial examples",
            "command": "ml-audit generate-adversarial --model model.pkl --input test_samples/"
          },
          {
            "title": "Privacy assessment",
            "details": "Check for data exfiltration vulnerabilities",
            "command": "ml-audit test-membership --model model.pkl --candidate-data candidates.csv"
          },
          {
            "title": "Generate audit report",
            "details": "Compile findings and recommendations",
            "command": "ml-audit generate-report --output-dir audit_results/"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "ML Security Assessment Framework",
        "stages": [
          {
            "label": "Data validation",
            "description": "Audit training data integrity",
            "command": "ml-audit detect-poisoning --dataset training.csv"
          },
          {
            "label": "Model testing",
            "description": "Test model against known attacks",
            "command": "ml-audit assess-vulnerabilities --model model.pkl"
          },
          {
            "label": "Adversarial evaluation",
            "description": "Generate and test adversarial inputs",
            "command": "ml-audit generate-adversarial --model model.pkl"
          },
          {
            "label": "Risk assessment",
            "description": "Calculate overall security risk score",
            "command": "ml-audit calculate-risk --audit-results results/"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "POISONING DETECTED: 85% confidence",
        "meaning": "Data poisoning identified with confidence score",
        "severity": "warning"
      },
      {
        "indicator": "ADVERSARIAL SUCCESS: 92%",
        "meaning": "Percentage of successful adversarial attacks",
        "severity": "warning"
      },
      {
        "indicator": "MEMBERSHIP LEAK: HIGH",
        "meaning": "Significant risk of membership inference",
        "severity": "critical"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom attack simulation",
        "command": "ml-audit simulate-attack --custom-attack my_attack.py --model target.pkl",
        "scenario": "Develop and test custom attack vectors",
        "notes": [
          "Create custom attack modules for specific ML architectures",
          "Test novel threat models and scenarios"
        ]
      }
    ],
    "comparison_table": null,
    "resources": [
      {
        "label": "ML Pipeline Audit Documentation",
        "url": "https://github.com/mlsec/ml-pipeline-audit",
        "description": "Official toolkit documentation and examples"
      },
      {
        "label": "Adversarial ML Research",
        "url": "https://arxiv.org/abs/2002.08619",
        "description": "Research paper on adversarial attacks"
      }
    ]
  },
  {
    "id": "llmguard",
    "name": "LLMGuard",
    "summary": "LLMGuard is a security library for LLM input and output sanitization. It provides defenses against prompt injection, jailbreaks, and ensures LLM outputs comply with safety policies.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via pip",
        "steps": [
          {
            "detail": "sudo apt-get update && sudo apt-get install -y python3-pip",
            "copyable": true
          },
          {
            "detail": "pip3 install llmguard",
            "copyable": true
          },
          {
            "detail": "python3 -c \"import llmguard; print(llmguard.__version__)\"",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Install from pip",
        "steps": [
          {
            "detail": "pip3 install llmguard",
            "copyable": true
          },
          {
            "detail": "python3 -c \"import llmguard; print(llmguard.__version__)\"",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Use Docker image",
        "steps": [
          {
            "detail": "docker pull llmguard:latest",
            "copyable": true
          },
          {
            "detail": "docker run -it llmguard:latest python3 -m llmguard --help",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Validate user input",
        "command": "python3 -c \"from llmguard import LLMGuard; guard = LLMGuard(); print(guard.validate_input(\\\"test\\\"))\"",
        "notes": [
          "Check user input for safety"
        ]
      },
      {
        "description": "Sanitize LLM output",
        "command": "python3 -c \"from llmguard import LLMGuard; guard = LLMGuard(); print(guard.sanitize_output(\\\"response\\\"))\"",
        "notes": [
          "Ensure model output is safe"
        ]
      }
    ],
    "step_sequences": [
      {
        "title": "LLM Input/Output Security",
        "steps": [
          {
            "title": "Initialize LLMGuard",
            "details": "Set up security layer",
            "command": "from llmguard import LLMGuard\nguard = LLMGuard()"
          },
          {
            "title": "Validate input",
            "details": "Check for prompt injection",
            "command": "result = guard.validate_input(user_input)"
          },
          {
            "title": "Process with LLM",
            "details": "Send safe input to model",
            "command": "response = model.generate(result)"
          },
          {
            "title": "Sanitize output",
            "details": "Clean model response",
            "command": "safe_output = guard.sanitize_output(response)"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "LLM Application Security Workflow",
        "stages": [
          {
            "label": "Setup guards",
            "description": "Initialize LLMGuard",
            "command": "guard = LLMGuard(policies=['no-sql-injection', 'no-prompt-injection'])"
          },
          {
            "label": "Process input",
            "description": "Validate and sanitize user input",
            "command": "safe_input = guard.validate_input(user_input)"
          },
          {
            "label": "Query LLM",
            "description": "Send to language model",
            "command": "response = llm.generate(safe_input)"
          },
          {
            "label": "Secure output",
            "description": "Sanitize before returning",
            "command": "return guard.sanitize_output(response)"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "INJECTION DETECTED",
        "meaning": "Prompt injection pattern found in input",
        "severity": "high"
      },
      {
        "indicator": "JAILBREAK ATTEMPT",
        "meaning": "Attempt to bypass safety mechanisms",
        "severity": "high"
      },
      {
        "indicator": "POLICY VIOLATION",
        "meaning": "Output violates defined safety policy",
        "severity": "medium"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom policy definition",
        "command": "guard = LLMGuard(custom_policies=['ban-harmful-keywords', 'rate-limit-requests'])",
        "scenario": "Implement custom security policies",
        "notes": [
          "Define organization-specific rules",
          "Policy scoring and weighting"
        ]
      }
    ],
    "resources": [
      {
        "label": "LLMGuard Documentation",
        "url": "https://github.com/protectai/llmguard",
        "description": "Official LLMGuard repository and documentation"
      },
      {
        "label": "LLM Security Best Practices",
        "url": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
        "description": "OWASP Top 10 for LLM Applications"
      }
    ]
  },
  {
    "id": "lakera-guard",
    "name": "Lakera Guard",
    "summary": "Lakera Guard is an API-based security service for detecting and preventing LLM attacks including prompt injection, jailbreaks, and PII leakage. Provides real-time threat detection.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install SDK via pip",
        "steps": [
          {
            "detail": "sudo apt-get update && sudo apt-get install -y python3-pip",
            "copyable": true
          },
          {
            "detail": "pip3 install lakera-guard",
            "copyable": true
          },
          {
            "detail": "python3 -c \"import lakera; print(lakera.__version__)\"",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Install from pip",
        "steps": [
          {
            "detail": "pip3 install lakera-guard",
            "copyable": true
          },
          {
            "detail": "export LAKERA_API_KEY='your-api-key'",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Docker deployment",
        "steps": [
          {
            "detail": "docker pull lakera/guard:latest",
            "copyable": true
          },
          {
            "detail": "docker run -e LAKERA_API_KEY=your-api-key lakera/guard:latest",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Check input for attacks",
        "command": "python3 -c \"from lakera import Guard; client = Guard(api_key='key'); result = client.detect.prompt_injection('test')\"",
        "notes": [
          "Real-time attack detection"
        ]
      },
      {
        "description": "Detect PII in text",
        "command": "python3 -c \"from lakera import Guard; client = Guard(); result = client.detect.pii('text with email@example.com')\"",
        "notes": [
          "Identify sensitive information"
        ]
      }
    ],
    "step_sequences": [
      {
        "title": "Lakera Guard Integration",
        "steps": [
          {
            "title": "Set up API key",
            "details": "Configure Lakera credentials",
            "command": "export LAKERA_API_KEY='your-api-key'"
          },
          {
            "title": "Initialize client",
            "details": "Create Guard instance",
            "command": "from lakera import Guard\nclient = Guard()"
          },
          {
            "title": "Check input",
            "details": "Scan for prompt injections",
            "command": "result = client.detect.prompt_injection(user_input)"
          },
          {
            "title": "Take action",
            "details": "Block or allow based on risk",
            "command": "if result.is_attack: block_request()"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "LLM Threat Detection Workflow",
        "stages": [
          {
            "label": "Initialize",
            "description": "Set up Lakera Guard client",
            "command": "client = Guard(api_key=os.getenv('LAKERA_API_KEY'))"
          },
          {
            "label": "Receive request",
            "description": "Get user input",
            "command": "user_input = request.json()['text']"
          },
          {
            "label": "Scan for threats",
            "description": "Check for multiple attack types",
            "command": "threats = client.detect.multiple_threats(user_input)"
          },
          {
            "label": "Block if needed",
            "description": "Prevent malicious requests",
            "command": "if threats.risk_level == 'HIGH': return error_response()"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "Prompt injection: HIGH confidence",
        "meaning": "Strong indicator of injection attack",
        "severity": "critical"
      },
      {
        "indicator": "PII detected: SSN, email",
        "meaning": "Sensitive personal information found",
        "severity": "high"
      },
      {
        "indicator": "Jailbreak pattern: Known technique",
        "meaning": "Recognized jailbreak attempt",
        "severity": "high"
      }
    ],
    "advanced_usage": [
      {
        "title": "Custom threat model",
        "command": "client = Guard(custom_models=['harmful-content', 'data-theft'])",
        "scenario": "Deploy custom threat detection models",
        "notes": [
          "Organization-specific threat models",
          "Fine-tuned detection algorithms"
        ]
      }
    ],
    "resources": [
      {
        "label": "Lakera Guard Documentation",
        "url": "https://docs.lakera.ai/",
        "description": "Official Lakera Guard API documentation"
      },
      {
        "label": "LLM Security Research",
        "url": "https://www.lakera.ai/blog",
        "description": "Latest LLM security research and findings"
      }
    ]
  },
  {
    "id": "promptfoo",
    "name": "Promptfoo",
    "summary": "Promptfoo is an LLM testing framework for evaluating and validating LLM prompts, outputs, and model behaviors. Helps identify prompt weaknesses, jailbreaks, and ensure consistent performance.",
    "installation_guides": [
      {
        "platform": "Debian/Ubuntu",
        "summary": "Install via npm",
        "steps": [
          {
            "detail": "sudo apt-get update && sudo apt-get install -y nodejs npm",
            "copyable": true
          },
          {
            "detail": "npm install -g promptfoo",
            "copyable": true
          },
          {
            "detail": "promptfoo --version",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Kali Linux",
        "summary": "Install from npm",
        "steps": [
          {
            "detail": "npm install -g promptfoo",
            "copyable": true
          },
          {
            "detail": "promptfoo --version",
            "copyable": true
          }
        ]
      },
      {
        "platform": "Docker / Alternative",
        "summary": "Run with Docker",
        "steps": [
          {
            "detail": "docker pull promptfoo:latest",
            "copyable": true
          },
          {
            "detail": "docker run -it promptfoo:latest promptfoo --help",
            "copyable": true
          }
        ]
      }
    ],
    "quick_examples": [
      {
        "description": "Initialize test suite",
        "command": "promptfoo init",
        "notes": [
          "Create promptfooconfig.yaml"
        ]
      },
      {
        "description": "Run tests",
        "command": "promptfoo eval",
        "notes": [
          "Execute all configured tests"
        ]
      },
      {
        "description": "View results",
        "command": "promptfoo view",
        "notes": [
          "Open test results in browser"
        ]
      }
    ],
    "step_sequences": [
      {
        "title": "LLM Prompt Evaluation",
        "steps": [
          {
            "title": "Set up test suite",
            "details": "Initialize Promptfoo project",
            "command": "promptfoo init"
          },
          {
            "title": "Define test cases",
            "details": "Create test scenarios",
            "command": "# Edit promptfooconfig.yaml with test cases"
          },
          {
            "title": "Configure prompts",
            "details": "Add prompts to test",
            "command": "# Define prompt variants for evaluation"
          },
          {
            "title": "Execute tests",
            "details": "Run evaluation suite",
            "command": "promptfoo eval"
          }
        ]
      }
    ],
    "workflow_guides": [
      {
        "name": "Prompt Testing Workflow",
        "stages": [
          {
            "label": "Initialize",
            "description": "Set up test environment",
            "command": "promptfoo init --model openai:gpt-4"
          },
          {
            "label": "Define tests",
            "description": "Create test cases",
            "command": "cat > promptfooconfig.yaml << 'EOF'\nprompts: ['prompts/*.txt']\ntest: 'tests.txt'\nEOF"
          },
          {
            "label": "Run evaluation",
            "description": "Execute test suite",
            "command": "promptfoo eval"
          },
          {
            "label": "Analyze results",
            "description": "Review test results",
            "command": "promptfoo view"
          }
        ]
      }
    ],
    "output_notes": [
      {
        "indicator": "PASS: Prompt behaves as expected",
        "meaning": "Test case passed successfully",
        "severity": "info"
      },
      {
        "indicator": "FAIL: Unexpected output format",
        "meaning": "Output doesn't match expected format",
        "severity": "high"
      },
      {
        "indicator": "JAILBREAK: Prompt bypassed restrictions",
        "meaning": "Prompt injection or jailbreak successful",
        "severity": "critical"
      }
    ],
    "advanced_usage": [
      {
        "title": "Multi-model comparison",
        "command": "promptfoo eval --models 'openai:gpt-4,openai:gpt-3.5-turbo,anthropic:claude-3'",
        "scenario": "Compare prompt performance across models",
        "notes": [
          "Test prompt compatibility",
          "Identify model-specific vulnerabilities"
        ]
      }
    ],
    "resources": [
      {
        "label": "Promptfoo Documentation",
        "url": "https://www.promptfoo.dev/",
        "description": "Official Promptfoo documentation and guides"
      },
      {
        "label": "LLM Prompt Engineering",
        "url": "https://github.com/promptfoo/promptfoo",
        "description": "Prompt engineering best practices and examples"
      }
    ]
  }
]